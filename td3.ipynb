{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "td3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVa+qajzs+k9mshYj3JewI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chevron9/iannwtf/blob/master/td3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMoz1Eudy6V6",
        "outputId": "613864f7-c3be-4706-f315-0a9f0bfcdd3b"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "import gym\n",
        "env = gym.make(\"BipedalWalker-v3\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 15.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 13.3MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 9.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 9.1MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKd7teSc0eDg",
        "outputId": "5130c659-7d70-4cd9-bfb0-99c480d8e45d"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os \n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ-2qPxH0fmk"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plots the scores of the course of training\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVOJPRV60hL2"
      },
      "source": [
        "def timespan_format(timespan):\n",
        "    timespan = round(timespan)\n",
        "    h_full = timespan / (60*60)\n",
        "    h = timespan // (60*60)\n",
        "    m = (timespan % (60*60)) // 60\n",
        "    s = timespan % 60\n",
        "    time = f\"{h:02}:{m:02}:{s:02}\"\n",
        "    if h_full > 24:\n",
        "        d = timespan // (60*60*24)\n",
        "        h = (timespan % (60*60*24)) // (60*60)\n",
        "        time = f\"{d:02}::{h:02}:{m:02}:{s:02}\"\n",
        "    return time\n",
        "\n",
        "timespan_format(303601)\n",
        "test = False\n",
        "if test:\n",
        "    assert timespan_format(60) == \"00:01:00\"\n",
        "    assert timespan_format(60*60) == \"01:00:00\"\n",
        "    assert timespan_format(60*60+1) == \"01:00:01\"\n",
        "    assert timespan_format(60*60+60) == \"01:01:00\"\n",
        "    assert timespan_format(24*60*60+1) == \"01::00:00:01\"\n",
        "    assert timespan_format(303601) == \"03::12:20:01\" # 3 days 12 hours 20 minutes 1 second\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyl2Mlf__2Kj"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "#all the networks of the model\n",
        "\n",
        "\n",
        "#critic network that takes the state and the action and puts out the value of the\n",
        "#the action in the given state\n",
        "class CriticNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512,\n",
        "            name='critic', chkpt_dir='tmp/'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        #Dimensions of the dense layers\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "\n",
        "\n",
        "        #says where the weights are saved\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_td3.h5')\n",
        "\n",
        "        \n",
        "        #Optimization: using LeakyReLU as per https://arxiv.org/pdf/1709.06560.pdf\n",
        "\n",
        "        #dense layers with kernel and bias initializer(from an other implementation)\n",
        "        #and relu activation\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.nn.leaky_relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.nn.leaky_relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "        #denselayer with 1 neuron that gives the estimated q value of the\n",
        "        #state-action pair\n",
        "        f3 = 0.003\n",
        "        self.q = Dense(1, activation=None, kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f3, f3),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, action, training = True):\n",
        "        #feeds the network state and action pairs\n",
        "        action_value = self.dense_layer1(tf.concat([state, action], axis=1))\n",
        "\n",
        "        action_value = self.dense_layer2(action_value)\n",
        "\n",
        "        q = self.q(action_value)\n",
        "\n",
        "        #gives back an estimation of a q value\n",
        "        return q\n",
        "\n",
        "\n",
        "#actor network that takes the state and outputs a probability\n",
        "#distribution over all possible actions\n",
        "class ActorNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512, n_actions=4, name='actor',\n",
        "            chkpt_dir='tmp/'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_td3.h5')\n",
        "\n",
        "        #first dense layer\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        #second dense layer\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "\n",
        "        #output layer with tanh activation to get an output vector of length actionspace\n",
        "        #with values between -1 and 1 to fit to the action boundaries\n",
        "        f3 = 0.003\n",
        "        self.mu = Dense(self.n_actions, activation='tanh', kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3), \n",
        "                        bias_initializer= tf.keras.initializers.RandomUniform(-f3, f3))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, training = True):\n",
        "\n",
        "        actions = self.dense_layer1(state)\n",
        "        actions = self.dense_layer2(actions)\n",
        "\n",
        "        #gives back the actions the agent should take (deterministic policy)\n",
        "        actions = self.mu(actions)\n",
        "\n",
        "\n",
        "        return actions\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyDoCKTR0pBQ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replaybuffer that stores all informations about the agents transitions\n",
        "# in np arrays\n",
        "class ReplayBuffer:\n",
        "    #initializes the memories with zeros with sizes depending on a big maximal size,\n",
        "    #the input_dimensions(output of the environment) or the numbers of possible\n",
        "    #actions\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.max_size = max_size\n",
        "        self.current_position = 0\n",
        "        self.state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.new_state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.action_memory = np.zeros((self.max_size, n_actions))\n",
        "        self.reward_memory = np.zeros(self.max_size)\n",
        "        self.terminal_memory = np.zeros(self.max_size, dtype=np.bool)\n",
        "\n",
        "    #stores new transitions in memory\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.current_position % self.max_size\n",
        "\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.current_position+= 1\n",
        "\n",
        "    #gives back a random batch of of transition samples\n",
        "    def sample_buffer(self, batch_size):\n",
        "        #makes sure to have the correct current size of the memory\n",
        "        max_mem = min(self.current_position, self.max_size)\n",
        "\n",
        "        #selects a random batch of indexes in the memory size\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        #retrieves the batch from memory\n",
        "        states = self.state_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, dones\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "111NtTmA1KJO"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# the agent class where the all the important parameters and systems of the\n",
        "# model are managed\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, input_dims, alpha=0.001, beta=0.002, env=None,\n",
        "            gamma=0.99, n_actions=4, max_size=1000000, tau=0.001,\n",
        "            dense1=512, dense2=512, batch_size=128, noise=0.3, module_dir = \"\"):\n",
        "\n",
        "        #initializing network-parameters\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.n_actions = n_actions\n",
        "        self.noise = noise\n",
        "        \n",
        "        self.delay = 0 # this governs if we wanna update the actor this step or not\n",
        "        self.delay_threshold = 2 #we want to update the actor every x steps\n",
        "\n",
        "        #retrieves the maximum and minimum of the actionvalues\n",
        "        self.max_action = env.action_space.high[0]\n",
        "        self.min_action = env.action_space.low[0]\n",
        "\n",
        "        #initializes the Replaybuffer which stores what the agents does\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "        \n",
        "        # initializing the Networks with given parameters\n",
        "        # target_actor and target_critic are just initialized as the actor and\n",
        "        # critic networks\n",
        "\n",
        "        chkpt_dir = module_dir+\"/tmp\"\n",
        "        self.actor = ActorNetwork(n_actions=n_actions, name='actor', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_actor = ActorNetwork(n_actions=n_actions, name='target_actor',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "\n",
        "\n",
        "        # The critics might be better handled in a list, but this might slow down tensorflow performance\n",
        "        self.critic1 = CriticNetwork(name='critic1',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_critic1 = CriticNetwork(name='target_critic1', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "        self.critic2 = CriticNetwork(name='critic2',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_critic2 = CriticNetwork(name='target_critic2', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "\n",
        "        #compile the networks with learning rates\n",
        "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "        \n",
        "        self.critic1.compile(optimizer=Adam(learning_rate=beta))\n",
        "        self.target_critic1.compile(optimizer=Adam(learning_rate=beta))\n",
        "\n",
        "        self.critic2.compile(optimizer=Adam(learning_rate=beta))\n",
        "        self.target_critic2.compile(optimizer=Adam(learning_rate=beta))\n",
        "\n",
        "        self.update_network_parameters(tau=1, delay = False) #Hard copy, since this is the initialization\n",
        "\n",
        "    #updates the target networks\n",
        "    #soft copies the target and actor network dependent on tau\n",
        "    def update_network_parameters(self, tau=None, delay = False):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        if not delay:\n",
        "            weights = []\n",
        "            targets = self.target_actor.weights\n",
        "            for i, weight in enumerate(self.actor.weights):\n",
        "                weights.append(weight * tau + targets[i]*(1-tau))\n",
        "            self.target_actor.set_weights(weights)\n",
        "\n",
        "\n",
        "        # critics copying to target critics\n",
        "        weights = []\n",
        "        targets = self.target_critic1.weights\n",
        "        for i, weight in enumerate(self.critic1.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_critic1.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic2.weights\n",
        "        for i, weight in enumerate(self.critic2.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_critic2.set_weights(weights)\n",
        "\n",
        "    #stores the state, action, reward transition\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    #saves models in files\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic1.save_weights(self.critic1.checkpoint_file)\n",
        "        self.target_critic1.save_weights(self.target_critic1.checkpoint_file)\n",
        "        self.critic2.save_weights(self.critic2.checkpoint_file)\n",
        "        self.target_critic2.save_weights(self.target_critic2.checkpoint_file)\n",
        "\n",
        "    #loads models from files\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic1.load_weights(self.critic1.checkpoint_file)\n",
        "        self.target_critic1.load_weights(self.target_critic1.checkpoint_file)\n",
        "        self.critic2.load_weights(self.critic2.checkpoint_file)\n",
        "        self.target_critic2.load_weights(self.target_critic2.checkpoint_file)\n",
        "\n",
        "    #choose_action with help of the actor network, adds noise if it's for training\n",
        "    \n",
        "    def choose_action(self, observation, evaluate=False):\n",
        "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "        actions = self.actor(state)\n",
        "        \n",
        "        # inject eploration noise\n",
        "        if not evaluate:\n",
        "\n",
        "            actions += tf.random.normal(shape=[self.n_actions],\n",
        "                    mean=0.0, stddev=self.noise)\n",
        "\n",
        "        #makes sure that action boundaries are met\n",
        "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
        "\n",
        "        return actions[0]\n",
        "\n",
        "    #learn function of the networks\n",
        "    def learn(self):\n",
        "        # control problem: How to solve this problem? (or: picking policy) -> actor\n",
        "        # leads to\n",
        "        # prediction problem: Are my actiosn actually getting me closer to accomplishing my goal? (or: value function of policy) -> critic\n",
        "\n",
        "        #starts to learn when there are enough samples to fill a batch\n",
        "        if self.memory.current_position < self.batch_size:\n",
        "            return\n",
        "\n",
        "        #gets batch form memory\n",
        "        state, action, reward, new_state, done = \\\n",
        "                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        #convert np arrays to tensors to feed them to the networks\n",
        "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
        "\n",
        "        #update critic network\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "            #call target actor to simulate which action to take\n",
        "            target_actions = self.target_actor(states_)\n",
        "\n",
        "            # TD3: Add noise regularization to policy\n",
        "            # This is achieved by adding \"clipped noise\"\n",
        "            # Noise is clipped to ensure the noisy actions aren't too far from policy.\n",
        "            target_actions = target_actions + tf.clip_by_value(np.random.normal(scale=0.2), -0.5, 0.5)\n",
        "\n",
        "            # Clip again to make sure we aren't violating action space\n",
        "            target_actions = tf.clip_by_value(target_actions, self.min_action, self.max_action)\n",
        "\n",
        "\n",
        "            #target critic evaluates the value of the actions in the given states\n",
        "            critic1_value_ = tf.squeeze(self.target_critic1(states_, target_actions), 1)\n",
        "\n",
        "            #critic network evaluate the actual states and actions the model took\n",
        "            critic1_value = tf.squeeze(self.critic1(states, actions), 1)\n",
        "\n",
        "\n",
        "            critic2_value_ = tf.squeeze(self.target_critic2(states_, target_actions), 1)\n",
        "            critic2_value = tf.squeeze(self.critic2(states, actions), 1)\n",
        "\n",
        "            #target says what value of the action in a certain state should\n",
        "            #be like\n",
        "            target = rewards + self.gamma*tf.math.minimum(critic1_value_,critic2_value_)*(1-done)\n",
        "\n",
        "            #takes the MSE of the target and the actual critic value as the loss\n",
        "            critic1_loss = keras.losses.MSE(target, critic1_value)\n",
        "            critic2_loss = keras.losses.MSE(target, critic2_value)\n",
        "\n",
        "\n",
        "        #gets the gradients of the loss in respect to the parameters of the network\n",
        "        critic1_network_gradient = tape.gradient(critic1_loss,\n",
        "                                            self.critic1.trainable_variables)\n",
        "\n",
        "        critic2_network_gradient = tape.gradient(critic2_loss,\n",
        "                                            self.critic2.trainable_variables)\n",
        "\n",
        "        #aplies the gradients to the critic network\n",
        "        self.critic1.optimizer.apply_gradients(zip(\n",
        "            critic1_network_gradient, self.critic1.trainable_variables))\n",
        "\n",
        "        self.critic2.optimizer.apply_gradients(zip(\n",
        "            critic2_network_gradient, self.critic2.trainable_variables))\n",
        "\n",
        "        # The if/else makes sure we only update the actor every second step.\n",
        "        if self.delay < (self.delay_threshold-1):\n",
        "            self.delay += 1\n",
        "            delayBool = True\n",
        "        elif self.delay == (self.delay_threshold-1):\n",
        "            delayBool = False\n",
        "            #update the actor network\n",
        "            with tf.GradientTape() as tape:\n",
        "                #gets the policy of the actor in a state\n",
        "                action_policy = self.actor(states)\n",
        "\n",
        "                #loss of the actor is the negative value of the critic because we\n",
        "                #want to maximize the value but gradient descent minimizes\n",
        "                # We are using Critic1 Loss here, as this is how it's done in the original paper.\n",
        "                \n",
        "                # It would be interesting to plot how c1 and c2 loss differ over time.\n",
        "\n",
        "                actor_loss = -(self.critic1(states, action_policy))\n",
        "\n",
        "                #the loss is a average of all the losses\n",
        "                actor_loss = tf.math.reduce_mean(actor_loss)\n",
        "\n",
        "            #gradients of the loss in respect to the parameters of the actor network\n",
        "            actor_network_gradient = tape.gradient(actor_loss,\n",
        "                                        self.actor.trainable_variables)\n",
        "\n",
        "\n",
        "            #optimizing the network gradients\n",
        "            self.actor.optimizer.apply_gradients(zip(\n",
        "                actor_network_gradient, self.actor.trainable_variables))\n",
        "\n",
        "            self.delay = 0\n",
        "        else:\n",
        "            raise Exception(f\"Delay has been set incorrectly. Delay is {self.delay}, Threshold is {self.threshold}.\")\n",
        "\n",
        "        self.update_network_parameters(delay=delayBool)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KNaC99aZ1NO3",
        "outputId": "81f6dec0-382e-420a-c1bb-151a5caf081a"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os \n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# modules\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TD3 is an improvement on DDPG\n",
        "\n",
        "# Actor network only updated every 2 steps\n",
        "\n",
        "# 2 separate critic networks (and a target network for each of them)\n",
        "\n",
        "\n",
        "# target policy smoothing\n",
        "# TD3 reduces this variance by adding a small amount of random noise to the target and averaging over mini batches. \n",
        "# The range of noise is clipped in order to keep the target value close to the original action.\n",
        "\n",
        "#TODO: Implement TD3\n",
        "\n",
        "#TODO: Improve the loading loop, make sure the algorithm remembers what came before\n",
        "# adjusting episodes\n",
        "# adjusting plots\n",
        "# adjusting tensorboard graphs etc\n",
        "# statefulness\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #for the case you just want to load a previous model\n",
        "    load_checkpoint = False\n",
        "\n",
        "    #TODO maybe add some error handling if no checkpoint to load exists\n",
        "    module_dir = \"\"\n",
        "\n",
        "    #Housekeeping variables\n",
        "    last_score = 0\n",
        "    last_avg_score = 0\n",
        "    last_save = 0\n",
        "    avg_delta = []\n",
        "    avg_steps = []\n",
        "\n",
        "    t = t_start = time.localtime()\n",
        "    current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t)\n",
        "    print(f\"\\n----------------- Training started at {current_time}. -------------------\\ncheckpoint: {load_checkpoint}\")\n",
        "\n",
        "\n",
        "    figure_dir = module_dir+f'plots/'\n",
        "    figure_file = figure_dir+f'walker{current_time.replace(\":\",\"_\")}.png'\n",
        "\n",
        "    log_dir = module_dir+'logs/' + current_time.replace(\":\",\"_\")\n",
        "    \n",
        "    #Tensorboard writer  \n",
        "    writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "\n",
        "    #initialize the environment for the agent and initialize the agent\n",
        "    \n",
        "    #tf.debugging.set_log_device_placement(True)\n",
        "    env = gym.make('BipedalWalker-v3')\n",
        "    #env = gym.make('BipedalWalkerHardcore-v3')\n",
        "\n",
        "\n",
        "    n_actions = env.action_space.shape[0]\n",
        "\n",
        "    noise = 0.4\n",
        "    # NEW batch 128\n",
        "    agent = Agent(alpha=0.00005, beta=0.0005, input_dims=env.observation_space.shape, tau=0.001, env=env,\n",
        "                  batch_size=128, dense1=512, dense2=512, n_actions=n_actions, noise = noise, module_dir = module_dir)\n",
        "\n",
        "\n",
        "    episodes = 5000 #250\n",
        "\n",
        "\n",
        "\n",
        "    #set bestscore to minimum\n",
        "    best_score = env.reward_range[0]\n",
        "    score_history = []\n",
        "\n",
        "\n",
        "    #initializes the model with one random sample batch if model are loaded\n",
        "    #you can't load an empty model for some reason\n",
        "    #these are all dummy variables etc until load_models overwrites them\n",
        "    if load_checkpoint:\n",
        "        n_steps = 0\n",
        "        while n_steps <= agent.batch_size:\n",
        "            observation = env.reset()\n",
        "            action = env.action_space.sample()\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            agent.remember(observation, action, reward, observation_, done)\n",
        "            n_steps += 1\n",
        "        agent.learn()\n",
        "        agent.load_models()\n",
        "\n",
        "    # ---------------------------------------\n",
        "    # main learning loop\n",
        "    # ---------------------------------------\n",
        "    try:\n",
        "        for i in range(episodes):\n",
        "            if i == 3:\n",
        "                tf.profiler.experimental.server.start(6009)\n",
        "                print(\"profiler started\")\n",
        "                # launch tensorboard with \"tensorboard --logdir logs\"\n",
        "                # capture profile\n",
        "            #if i == 13:\n",
        "            #    tf.profiler.experimental.stop\n",
        "            #    print(\"profiler stopped\")\n",
        "\n",
        "\n",
        "            current_episode = i\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "            \n",
        "            #regulates the noise over the course of training as exponential\n",
        "            #decay to get smaller noise at the end, the noise is the\n",
        "            #standarddeviation of a normal distribution\n",
        "            #(numbers from trial and error)\n",
        "            agent.noise = noise * np.exp(-i/1500)\n",
        "\n",
        "            #while the environment is running the model chooses actions, saves states,\n",
        "            #rewards, actions and observations in the buffer and trains the networks\n",
        "            #on them\n",
        "            steps = 0\n",
        "            while not done:\n",
        "                action = agent.choose_action(observation)\n",
        "\n",
        "                observation_, reward, done, info = env.step(action)\n",
        "                steps += 1\n",
        "                score += reward\n",
        "                agent.remember(observation, action, reward, observation_, done)\n",
        "                agent.learn()\n",
        "\n",
        "                #saves previous observation\n",
        "                observation = observation_\n",
        "\n",
        "            score_history.append(score)\n",
        "            avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "            #saves the model if the average score is better than the best previous\n",
        "            if avg_score > best_score:\n",
        "                best_score = avg_score\n",
        "                agent.save_models()\n",
        "                last_save = current_episode\n",
        "\n",
        "            #calculating and giving some info on training progress\n",
        "            t_new = time.localtime()\n",
        "            current_time = time.strftime(\"%H:%M:%S\", t_new)\n",
        "            t_delta = time.mktime(t_new)-time.mktime(t)\n",
        "            t = t_new\n",
        "            \n",
        "            # perhaps add a decaying factor for ETA\n",
        "            avg_delta.append(t_delta)\n",
        "            avg_delta_mean = np.mean(avg_delta)\n",
        "            avg_delta_std = np.var(avg_delta)\n",
        "\n",
        "            ETA_avg = (episodes-i)*avg_delta_mean\n",
        "            ETA_min = (episodes-i)*max((avg_delta_mean-avg_delta_std),min(avg_delta))\n",
        "            ETA_max = (episodes-i)*(avg_delta_mean+avg_delta_std)\n",
        "\n",
        "            avg_steps.append(steps)\n",
        "            per_step = t_delta/steps\n",
        "            steps_per_score = score/steps\n",
        "\n",
        "            print(f\"{current_time} \\n\"\n",
        "            f'Episode: **{i+1}**/{episodes}, Score: {score:.0f} (Δ{score-last_score:5.1f})\\n'\n",
        "            f'Average score: {avg_score:.1f} (Δ{avg_score-last_avg_score:5.2f})\\n'\n",
        "            f'Episode time: {t_delta:.1f}s, average: {avg_delta_mean:.1f}s (±{avg_delta_std:4.2f}),', \n",
        "            f'ETA: {timespan_format(ETA_avg)} ({timespan_format(ETA_min)} to {timespan_format(ETA_max)})\\n'\n",
        "            f'Steps: {steps}. Time per step: {per_step:.1e}s. Reward per step: {steps_per_score:.2f}.\\n' \n",
        "            f'It has been {i - last_save} episode(s) since the model was last saved, with a score of {best_score:.0f} (Δ{avg_score-best_score:2.2f}).\\n')\n",
        "\n",
        "            last_score = score\n",
        "            last_avg_score = avg_score\n",
        "            \n",
        "\n",
        "            with writer.as_default():\n",
        "                tf.summary.scalar('Average Score', avg_score, step=i)\n",
        "                tf.summary.scalar('Score', score, step=i)\n",
        "                tf.summary.scalar('ETA', ETA_avg, step=i)\n",
        "                tf.summary.scalar('Calculation time per step', per_step, step=i)\n",
        "                tf.summary.scalar('Calculation time per episode', t_delta, step=i)\n",
        "                tf.summary.scalar('Steps', steps, step=i)\n",
        "                if ((i+1) % 50) == 0: #writer.flush and learning plot has a large performance impact, so only do it every 50 episodes\n",
        "                    writer.flush()\n",
        "                    x = [j+1 for j in range(current_episode+1)]\n",
        "                    plot_learning_curve(x, score_history, figure_file)\n",
        "                \n",
        "\n",
        "\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        episodes = current_episode\n",
        "        print(\"Manually shutting down training.\")\n",
        "    \n",
        "    #plots the whole score history\n",
        "    x = [i+1 for i in range(episodes)]\n",
        "    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "t2 = time.localtime()\n",
        "current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t2)\n",
        "t_delta = time.mktime(t2)-time.mktime(t_start)\n",
        "print(f\"\\n----------------- Training ended at {current_time}. Duration was {t_delta/60:.2f} minutes.-----------------\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n",
            "\n",
            "----------------- Training started at 2021-04-01-11:34:48. -------------------\n",
            "checkpoint: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... saving models ...\n",
            "11:34:56 \n",
            "Episode: **1**/5000, Score: -102 (Δ-101.8)\n",
            "Average score: -101.8 (Δ-101.77)\n",
            "Episode time: 8.0s, average: 8.0s (±0.00), ETA: 11:06:40 (11:06:40 to 11:06:40)\n",
            "Steps: 69. Time per step: 1.2e-01s. Reward per step: -1.47.\n",
            "It has been 0 episode(s) since the model was last saved, with a score of -102 (Δ0.00).\n",
            "\n",
            "11:34:58 \n",
            "Episode: **2**/5000, Score: -103 (Δ -0.9)\n",
            "Average score: -102.2 (Δ-0.45)\n",
            "Episode time: 2.0s, average: 5.0s (±9.00), ETA: 06:56:35 (02:46:38 to 19:26:26)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.66.\n",
            "It has been 1 episode(s) since the model was last saved, with a score of -102 (Δ-0.45).\n",
            "\n",
            "11:35:01 \n",
            "Episode: **3**/5000, Score: -106 (Δ -3.4)\n",
            "Average score: -103.5 (Δ-1.28)\n",
            "Episode time: 3.0s, average: 4.3s (±6.89), ETA: 06:00:58 (02:46:36 to 15:34:49)\n",
            "Steps: 64. Time per step: 4.7e-02s. Reward per step: -1.66.\n",
            "It has been 2 episode(s) since the model was last saved, with a score of -102 (Δ-1.73).\n",
            "\n",
            "profiler started\n",
            "11:35:07 \n",
            "Episode: **4**/5000, Score: -128 (Δ-21.6)\n",
            "Average score: -109.5 (Δ-6.05)\n",
            "Episode time: 6.0s, average: 4.8s (±5.69), ETA: 06:35:36 (02:46:34 to 14:29:16)\n",
            "Steps: 158. Time per step: 3.8e-02s. Reward per step: -0.81.\n",
            "It has been 3 episode(s) since the model was last saved, with a score of -102 (Δ-7.78).\n",
            "\n",
            "11:35:09 \n",
            "Episode: **5**/5000, Score: -107 (Δ 20.9)\n",
            "Average score: -109.0 (Δ 0.54)\n",
            "Episode time: 2.0s, average: 4.2s (±5.76), ETA: 05:49:43 (02:46:32 to 13:49:20)\n",
            "Steps: 60. Time per step: 3.3e-02s. Reward per step: -1.78.\n",
            "It has been 4 episode(s) since the model was last saved, with a score of -102 (Δ-7.24).\n",
            "\n",
            "11:35:12 \n",
            "Episode: **6**/5000, Score: -103 (Δ  4.1)\n",
            "Average score: -108.0 (Δ 1.04)\n",
            "Episode time: 3.0s, average: 4.0s (±5.00), ETA: 05:33:00 (02:46:30 to 12:29:15)\n",
            "Steps: 69. Time per step: 4.3e-02s. Reward per step: -1.49.\n",
            "It has been 5 episode(s) since the model was last saved, with a score of -102 (Δ-6.20).\n",
            "\n",
            "11:35:17 \n",
            "Episode: **7**/5000, Score: -109 (Δ -6.5)\n",
            "Average score: -108.2 (Δ-0.19)\n",
            "Episode time: 5.0s, average: 4.1s (±4.41), ETA: 05:44:49 (02:46:28 to 11:51:44)\n",
            "Steps: 108. Time per step: 4.6e-02s. Reward per step: -1.01.\n",
            "It has been 6 episode(s) since the model was last saved, with a score of -102 (Δ-6.39).\n",
            "\n",
            "11:35:42 \n",
            "Episode: **8**/5000, Score: -172 (Δ-62.6)\n",
            "Average score: -116.1 (Δ-7.96)\n",
            "Episode time: 25.0s, average: 6.8s (±51.44), ETA: 09:21:43 (02:46:26 to 03::08:42:10)\n",
            "Steps: 619. Time per step: 4.0e-02s. Reward per step: -0.28.\n",
            "It has been 7 episode(s) since the model was last saved, with a score of -102 (Δ-14.36).\n",
            "\n",
            "11:36:37 \n",
            "Episode: **9**/5000, Score: -221 (Δ-49.0)\n",
            "Average score: -127.8 (Δ-11.64)\n",
            "Episode time: 55.0s, average: 12.1s (±275.65), ETA: 16:47:39 (02:46:24 to 16::15:02:05)\n",
            "Steps: 1374. Time per step: 4.0e-02s. Reward per step: -0.16.\n",
            "It has been 8 episode(s) since the model was last saved, with a score of -102 (Δ-25.99).\n",
            "\n",
            "11:36:40 \n",
            "Episode: **10**/5000, Score: -100 (Δ121.2)\n",
            "Average score: -125.0 (Δ 2.81)\n",
            "Episode time: 3.0s, average: 11.2s (±255.56), ETA: 15:31:39 (02:46:22 to 15::09:49:59)\n",
            "Steps: 76. Time per step: 3.9e-02s. Reward per step: -1.31.\n",
            "It has been 9 episode(s) since the model was last saved, with a score of -102 (Δ-23.18).\n",
            "\n",
            "11:36:45 \n",
            "Episode: **11**/5000, Score: -106 (Δ -6.0)\n",
            "Average score: -123.2 (Δ 1.76)\n",
            "Episode time: 5.0s, average: 10.6s (±235.50), ETA: 14:44:35 (02:46:20 to 14::05:10:41)\n",
            "Steps: 138. Time per step: 3.6e-02s. Reward per step: -0.77.\n",
            "It has been 10 episode(s) since the model was last saved, with a score of -102 (Δ-21.43).\n",
            "\n",
            "11:36:48 \n",
            "Episode: **12**/5000, Score: -123 (Δ-17.3)\n",
            "Average score: -123.2 (Δ 0.02)\n",
            "Episode time: 3.0s, average: 10.0s (±220.33), ETA: 13:51:30 (02:46:18 to 13::07:12:13)\n",
            "Steps: 73. Time per step: 4.1e-02s. Reward per step: -1.68.\n",
            "It has been 11 episode(s) since the model was last saved, with a score of -102 (Δ-21.41).\n",
            "\n",
            "11:36:51 \n",
            "Episode: **13**/5000, Score: -102 (Δ 20.8)\n",
            "Average score: -121.6 (Δ 1.61)\n",
            "Episode time: 3.0s, average: 9.5s (±206.86), ETA: 13:06:34 (02:46:16 to 12::11:43:51)\n",
            "Steps: 67. Time per step: 4.5e-02s. Reward per step: -1.53.\n",
            "It has been 12 episode(s) since the model was last saved, with a score of -102 (Δ-19.79).\n",
            "\n",
            "11:36:55 \n",
            "Episode: **14**/5000, Score: -100 (Δ  2.1)\n",
            "Average score: -120.0 (Δ 1.53)\n",
            "Episode time: 4.0s, average: 9.1s (±194.07), ETA: 12:33:59 (02:46:14 to 11::17:24:08)\n",
            "Steps: 87. Time per step: 4.6e-02s. Reward per step: -1.15.\n",
            "It has been 13 episode(s) since the model was last saved, with a score of -102 (Δ-18.26).\n",
            "\n",
            "11:37:59 \n",
            "Episode: **15**/5000, Score: -139 (Δ-38.6)\n",
            "Average score: -121.3 (Δ-1.24)\n",
            "Episode time: 64.0s, average: 12.7s (±368.86), ETA: 17:38:08 (02:46:12 to 22::00:30:35)\n",
            "Steps: 1600. Time per step: 4.0e-02s. Reward per step: -0.09.\n",
            "It has been 14 episode(s) since the model was last saved, with a score of -102 (Δ-19.50).\n",
            "\n",
            "11:38:02 \n",
            "Episode: **16**/5000, Score: -106 (Δ 32.9)\n",
            "Average score: -120.3 (Δ 0.97)\n",
            "Episode time: 3.0s, average: 12.1s (±351.36), ETA: 16:47:23 (02:46:10 to 20::23:19:30)\n",
            "Steps: 62. Time per step: 4.8e-02s. Reward per step: -1.71.\n",
            "It has been 15 episode(s) since the model was last saved, with a score of -102 (Δ-18.53).\n",
            "\n",
            "11:38:04 \n",
            "Episode: **17**/5000, Score: -108 (Δ -2.6)\n",
            "Average score: -119.6 (Δ 0.70)\n",
            "Episode time: 2.0s, average: 11.5s (±336.37), ETA: 15:57:43 (02:46:08 to 20::01:38:35)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.31.\n",
            "It has been 16 episode(s) since the model was last saved, with a score of -102 (Δ-17.83).\n",
            "\n",
            "11:38:07 \n",
            "Episode: **18**/5000, Score: -102 (Δ  6.7)\n",
            "Average score: -118.6 (Δ 1.00)\n",
            "Episode time: 3.0s, average: 11.1s (±321.50), ETA: 15:18:10 (02:46:06 to 19::04:18:29)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.43.\n",
            "It has been 17 episode(s) since the model was last saved, with a score of -102 (Δ-16.83).\n",
            "\n",
            "11:38:09 \n",
            "Episode: **19**/5000, Score: -102 (Δ -0.2)\n",
            "Average score: -117.7 (Δ 0.88)\n",
            "Episode time: 2.0s, average: 10.6s (±308.66), ETA: 14:38:24 (02:46:04 to 18::09:47:52)\n",
            "Steps: 70. Time per step: 2.9e-02s. Reward per step: -1.46.\n",
            "It has been 18 episode(s) since the model was last saved, with a score of -102 (Δ-15.95).\n",
            "\n",
            "11:38:12 \n",
            "Episode: **20**/5000, Score: -101 (Δ  0.7)\n",
            "Average score: -116.9 (Δ 0.83)\n",
            "Episode time: 3.0s, average: 10.2s (±295.96), ETA: 14:06:46 (02:46:02 to 17::15:36:23)\n",
            "Steps: 75. Time per step: 4.0e-02s. Reward per step: -1.35.\n",
            "It has been 19 episode(s) since the model was last saved, with a score of -102 (Δ-15.12).\n",
            "\n",
            "11:38:15 \n",
            "Episode: **21**/5000, Score: -102 (Δ -0.9)\n",
            "Average score: -116.2 (Δ 0.71)\n",
            "Episode time: 3.0s, average: 9.9s (±284.22), ETA: 13:38:09 (02:46:00 to 16::22:48:13)\n",
            "Steps: 64. Time per step: 4.7e-02s. Reward per step: -1.59.\n",
            "It has been 20 episode(s) since the model was last saved, with a score of -102 (Δ-14.42).\n",
            "\n",
            "11:38:19 \n",
            "Episode: **22**/5000, Score: -101 (Δ  1.0)\n",
            "Average score: -115.5 (Δ 0.69)\n",
            "Episode time: 4.0s, average: 9.6s (±272.79), ETA: 13:15:53 (02:45:58 to 16::06:32:41)\n",
            "Steps: 90. Time per step: 4.4e-02s. Reward per step: -1.12.\n",
            "It has been 21 episode(s) since the model was last saved, with a score of -102 (Δ-13.73).\n",
            "\n",
            "11:38:21 \n",
            "Episode: **23**/5000, Score: -106 (Δ -5.2)\n",
            "Average score: -115.1 (Δ 0.40)\n",
            "Episode time: 2.0s, average: 9.3s (±263.32), ETA: 12:48:21 (02:45:56 to 15::16:55:24)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.17.\n",
            "It has been 22 episode(s) since the model was last saved, with a score of -102 (Δ-13.33).\n",
            "\n",
            "11:38:23 \n",
            "Episode: **24**/5000, Score: -105 (Δ  1.1)\n",
            "Average score: -114.7 (Δ 0.42)\n",
            "Episode time: 2.0s, average: 9.0s (±254.46), ETA: 12:23:06 (02:45:54 to 15::04:10:16)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.10.\n",
            "It has been 23 episode(s) since the model was last saved, with a score of -102 (Δ-12.91).\n",
            "\n",
            "11:38:24 \n",
            "Episode: **25**/5000, Score: -106 (Δ -0.5)\n",
            "Average score: -114.3 (Δ 0.36)\n",
            "Episode time: 1.0s, average: 8.6s (±246.71), ETA: 11:56:33 (01:22:56 to 14::16:57:04)\n",
            "Steps: 46. Time per step: 2.2e-02s. Reward per step: -2.30.\n",
            "It has been 24 episode(s) since the model was last saved, with a score of -102 (Δ-12.55).\n",
            "\n",
            "11:38:27 \n",
            "Episode: **26**/5000, Score: -102 (Δ  3.8)\n",
            "Average score: -113.8 (Δ 0.48)\n",
            "Episode time: 3.0s, average: 8.4s (±238.40), ETA: 11:38:25 (01:22:55 to 14::05:05:35)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.43.\n",
            "It has been 25 episode(s) since the model was last saved, with a score of -102 (Δ-12.07).\n",
            "\n",
            "11:38:31 \n",
            "Episode: **27**/5000, Score: -101 (Δ  0.6)\n",
            "Average score: -113.4 (Δ 0.47)\n",
            "Episode time: 4.0s, average: 8.3s (±230.27), ETA: 11:24:42 (01:22:54 to 13::17:33:45)\n",
            "Steps: 77. Time per step: 5.2e-02s. Reward per step: -1.31.\n",
            "It has been 26 episode(s) since the model was last saved, with a score of -102 (Δ-11.60).\n",
            "\n",
            "11:38:34 \n",
            "Episode: **28**/5000, Score: -103 (Δ -1.5)\n",
            "Average score: -113.0 (Δ 0.38)\n",
            "Episode time: 3.0s, average: 8.1s (±222.99), ETA: 11:08:59 (01:22:53 to 13::07:11:33)\n",
            "Steps: 82. Time per step: 3.7e-02s. Reward per step: -1.25.\n",
            "It has been 27 episode(s) since the model was last saved, with a score of -102 (Δ-11.22).\n",
            "\n",
            "11:38:36 \n",
            "Episode: **29**/5000, Score: -106 (Δ -2.8)\n",
            "Average score: -112.7 (Δ 0.26)\n",
            "Episode time: 2.0s, average: 7.9s (±216.53), ETA: 10:51:30 (01:22:52 to 12::21:54:51)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.20.\n",
            "It has been 28 episode(s) since the model was last saved, with a score of -102 (Δ-10.97).\n",
            "\n",
            "11:38:38 \n",
            "Episode: **30**/5000, Score: -104 (Δ  1.2)\n",
            "Average score: -112.5 (Δ 0.28)\n",
            "Episode time: 2.0s, average: 7.7s (±210.42), ETA: 10:35:11 (01:22:51 to 12::13:08:40)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -2.05.\n",
            "It has been 29 episode(s) since the model was last saved, with a score of -102 (Δ-10.69).\n",
            "\n",
            "11:38:40 \n",
            "Episode: **31**/5000, Score: -101 (Δ  3.0)\n",
            "Average score: -112.1 (Δ 0.36)\n",
            "Episode time: 2.0s, average: 7.5s (±204.64), ETA: 10:19:55 (01:22:50 to 12::04:50:40)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.64.\n",
            "It has been 30 episode(s) since the model was last saved, with a score of -102 (Δ-10.33).\n",
            "\n",
            "11:38:43 \n",
            "Episode: **32**/5000, Score: -103 (Δ -1.9)\n",
            "Average score: -111.8 (Δ 0.28)\n",
            "Episode time: 3.0s, average: 7.3s (±198.85), ETA: 10:08:11 (01:22:49 to 11::20:36:20)\n",
            "Steps: 61. Time per step: 4.9e-02s. Reward per step: -1.69.\n",
            "It has been 31 episode(s) since the model was last saved, with a score of -102 (Δ-10.05).\n",
            "\n",
            "11:38:46 \n",
            "Episode: **33**/5000, Score: -102 (Δ  1.6)\n",
            "Average score: -111.5 (Δ 0.31)\n",
            "Episode time: 3.0s, average: 7.2s (±193.38), ETA: 09:57:10 (01:22:48 to 11::12:48:58)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.43.\n",
            "It has been 32 episode(s) since the model was last saved, with a score of -102 (Δ-9.75).\n",
            "\n",
            "11:38:48 \n",
            "Episode: **34**/5000, Score: -103 (Δ -1.1)\n",
            "Average score: -111.3 (Δ 0.26)\n",
            "Episode time: 2.0s, average: 7.1s (±188.47), ETA: 09:44:21 (01:22:47 to 11::05:46:17)\n",
            "Steps: 70. Time per step: 2.9e-02s. Reward per step: -1.47.\n",
            "It has been 33 episode(s) since the model was last saved, with a score of -102 (Δ-9.49).\n",
            "\n",
            "11:38:50 \n",
            "Episode: **35**/5000, Score: -105 (Δ -2.1)\n",
            "Average score: -111.1 (Δ 0.18)\n",
            "Episode time: 2.0s, average: 6.9s (±183.79), ETA: 09:32:16 (01:22:46 to 10::23:04:11)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -2.06.\n",
            "It has been 34 episode(s) since the model was last saved, with a score of -102 (Δ-9.30).\n",
            "\n",
            "11:38:53 \n",
            "Episode: **36**/5000, Score: -101 (Δ  4.1)\n",
            "Average score: -110.8 (Δ 0.29)\n",
            "Episode time: 3.0s, average: 6.8s (±179.10), ETA: 09:23:10 (01:22:45 to 10::16:23:46)\n",
            "Steps: 67. Time per step: 4.5e-02s. Reward per step: -1.50.\n",
            "It has been 35 episode(s) since the model was last saved, with a score of -102 (Δ-9.02).\n",
            "\n",
            "11:38:56 \n",
            "Episode: **37**/5000, Score: -103 (Δ -1.8)\n",
            "Average score: -110.6 (Δ 0.22)\n",
            "Episode time: 3.0s, average: 6.7s (±174.64), ETA: 09:14:32 (01:22:44 to 10::10:03:12)\n",
            "Steps: 68. Time per step: 4.4e-02s. Reward per step: -1.51.\n",
            "It has been 36 episode(s) since the model was last saved, with a score of -102 (Δ-8.79).\n",
            "\n",
            "11:38:58 \n",
            "Episode: **38**/5000, Score: -101 (Δ  1.7)\n",
            "Average score: -110.3 (Δ 0.26)\n",
            "Episode time: 2.0s, average: 6.6s (±170.61), ETA: 09:04:11 (01:22:43 to 10::04:16:40)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -1.80.\n",
            "It has been 37 episode(s) since the model was last saved, with a score of -102 (Δ-8.54).\n",
            "\n",
            "11:39:00 \n",
            "Episode: **39**/5000, Score: -102 (Δ -1.2)\n",
            "Average score: -110.1 (Δ 0.21)\n",
            "Episode time: 2.0s, average: 6.5s (±166.76), ETA: 08:54:22 (01:22:42 to 09::22:45:32)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.08.\n",
            "It has been 38 episode(s) since the model was last saved, with a score of -102 (Δ-8.32).\n",
            "\n",
            "11:39:02 \n",
            "Episode: **40**/5000, Score: -102 (Δ  0.1)\n",
            "Average score: -109.9 (Δ 0.20)\n",
            "Episode time: 2.0s, average: 6.3s (±163.08), ETA: 08:45:02 (01:22:41 to 09::17:28:50)\n",
            "Steps: 59. Time per step: 3.4e-02s. Reward per step: -1.73.\n",
            "It has been 39 episode(s) since the model was last saved, with a score of -102 (Δ-8.12).\n",
            "\n",
            "11:39:06 \n",
            "Episode: **41**/5000, Score: -102 (Δ  0.4)\n",
            "Average score: -109.7 (Δ 0.20)\n",
            "Episode time: 4.0s, average: 6.3s (±159.23), ETA: 08:40:12 (01:22:40 to 09::12:03:20)\n",
            "Steps: 81. Time per step: 4.9e-02s. Reward per step: -1.25.\n",
            "It has been 40 episode(s) since the model was last saved, with a score of -102 (Δ-7.91).\n",
            "\n",
            "11:39:08 \n",
            "Episode: **42**/5000, Score: -99 (Δ  2.6)\n",
            "Average score: -109.4 (Δ 0.26)\n",
            "Episode time: 2.0s, average: 6.2s (±155.87), ETA: 08:31:39 (01:22:39 to 09::07:14:10)\n",
            "Steps: 57. Time per step: 3.5e-02s. Reward per step: -1.74.\n",
            "It has been 41 episode(s) since the model was last saved, with a score of -102 (Δ-7.66).\n",
            "\n",
            "11:39:10 \n",
            "Episode: **43**/5000, Score: -100 (Δ -0.8)\n",
            "Average score: -109.2 (Δ 0.22)\n",
            "Episode time: 2.0s, average: 6.1s (±152.64), ETA: 08:23:29 (01:22:38 to 09::02:36:51)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.85.\n",
            "It has been 42 episode(s) since the model was last saved, with a score of -102 (Δ-7.43).\n",
            "\n",
            "11:39:13 \n",
            "Episode: **44**/5000, Score: -99 (Δ  0.5)\n",
            "Average score: -109.0 (Δ 0.23)\n",
            "Episode time: 3.0s, average: 6.0s (±149.39), ETA: 08:17:35 (01:22:37 to 08::21:59:20)\n",
            "Steps: 69. Time per step: 4.3e-02s. Reward per step: -1.44.\n",
            "It has been 43 episode(s) since the model was last saved, with a score of -102 (Δ-7.21).\n",
            "\n",
            "11:39:16 \n",
            "Episode: **45**/5000, Score: -101 (Δ -1.8)\n",
            "Average score: -108.8 (Δ 0.18)\n",
            "Episode time: 3.0s, average: 6.0s (±146.26), ETA: 08:11:56 (01:22:36 to 08::17:33:24)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.42.\n",
            "It has been 44 episode(s) since the model was last saved, with a score of -102 (Δ-7.03).\n",
            "\n",
            "11:40:19 \n",
            "Episode: **46**/5000, Score: -91 (Δ 10.0)\n",
            "Average score: -108.4 (Δ 0.38)\n",
            "Episode time: 63.0s, average: 7.2s (±212.29), ETA: 09:54:14 (01:22:35 to 12::14:05:41)\n",
            "Steps: 1600. Time per step: 3.9e-02s. Reward per step: -0.06.\n",
            "It has been 45 episode(s) since the model was last saved, with a score of -102 (Δ-6.65).\n",
            "\n",
            "11:41:24 \n",
            "Episode: **47**/5000, Score: -126 (Δ-34.8)\n",
            "Average score: -108.8 (Δ-0.37)\n",
            "Episode time: 65.0s, average: 8.4s (±277.35), ETA: 11:35:40 (01:22:34 to 16::09:15:36)\n",
            "Steps: 1600. Time per step: 4.1e-02s. Reward per step: -0.08.\n",
            "It has been 46 episode(s) since the model was last saved, with a score of -102 (Δ-7.02).\n",
            "\n",
            "11:42:28 \n",
            "Episode: **48**/5000, Score: -127 (Δ -1.0)\n",
            "Average score: -109.2 (Δ-0.38)\n",
            "Episode time: 64.0s, average: 9.6s (±334.58), ETA: 13:11:06 (01:22:33 to 19::17:30:23)\n",
            "Steps: 1600. Time per step: 4.0e-02s. Reward per step: -0.08.\n",
            "It has been 47 episode(s) since the model was last saved, with a score of -102 (Δ-7.40).\n",
            "\n",
            "11:43:32 \n",
            "Episode: **49**/5000, Score: -120 (Δ  7.4)\n",
            "Average score: -109.4 (Δ-0.21)\n",
            "Episode time: 64.0s, average: 10.7s (±386.95), ETA: 14:42:36 (01:22:32 to 22::18:58:38)\n",
            "Steps: 1600. Time per step: 4.0e-02s. Reward per step: -0.07.\n",
            "It has been 48 episode(s) since the model was last saved, with a score of -102 (Δ-7.61).\n",
            "\n",
            "11:43:36 \n",
            "Episode: **50**/5000, Score: -99 (Δ 20.1)\n",
            "Average score: -109.2 (Δ 0.20)\n",
            "Episode time: 4.0s, average: 10.6s (±380.09), ETA: 14:31:23 (01:22:31 to 22::09:14:50)\n",
            "Steps: 90. Time per step: 4.4e-02s. Reward per step: -1.10.\n",
            "It has been 49 episode(s) since the model was last saved, with a score of -102 (Δ-7.41).\n",
            "\n",
            "11:44:40 \n",
            "Episode: **51**/5000, Score: -130 (Δ-30.7)\n",
            "Average score: -109.6 (Δ-0.41)\n",
            "Episode time: 64.0s, average: 11.6s (±427.53), ETA: 15:57:39 (01:22:30 to 25::03:49:05)\n",
            "Steps: 1600. Time per step: 4.0e-02s. Reward per step: -0.08.\n",
            "It has been 50 episode(s) since the model was last saved, with a score of -102 (Δ-7.82).\n",
            "\n",
            "11:44:44 \n",
            "Episode: **52**/5000, Score: -98 (Δ 31.6)\n",
            "Average score: -109.4 (Δ 0.21)\n",
            "Episode time: 4.0s, average: 11.5s (±420.40), ETA: 15:45:23 (01:22:29 to 24::17:41:34)\n",
            "Steps: 85. Time per step: 4.7e-02s. Reward per step: -1.16.\n",
            "It has been 51 episode(s) since the model was last saved, with a score of -102 (Δ-7.61).\n",
            "\n",
            "11:44:47 \n",
            "Episode: **53**/5000, Score: -99 (Δ -0.4)\n",
            "Average score: -109.2 (Δ 0.20)\n",
            "Episode time: 3.0s, average: 11.3s (±413.80), ETA: 15:32:02 (01:22:28 to 24::08:16:23)\n",
            "Steps: 96. Time per step: 3.1e-02s. Reward per step: -1.03.\n",
            "It has been 52 episode(s) since the model was last saved, with a score of -102 (Δ-7.41).\n",
            "\n",
            "11:44:49 \n",
            "Episode: **54**/5000, Score: -119 (Δ-20.3)\n",
            "Average score: -109.4 (Δ-0.18)\n",
            "Episode time: 2.0s, average: 11.1s (±407.71), ETA: 15:17:38 (01:22:27 to 23::23:32:57)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.38.\n",
            "It has been 53 episode(s) since the model was last saved, with a score of -102 (Δ-7.59).\n",
            "\n",
            "11:44:52 \n",
            "Episode: **55**/5000, Score: -123 (Δ -4.2)\n",
            "Average score: -109.6 (Δ-0.25)\n",
            "Episode time: 3.0s, average: 11.0s (±401.47), ETA: 15:05:16 (01:22:26 to 23::14:39:59)\n",
            "Steps: 66. Time per step: 4.5e-02s. Reward per step: -1.87.\n",
            "It has been 54 episode(s) since the model was last saved, with a score of -102 (Δ-7.85).\n",
            "\n",
            "11:44:55 \n",
            "Episode: **56**/5000, Score: -100 (Δ 23.7)\n",
            "Average score: -109.4 (Δ 0.18)\n",
            "Episode time: 3.0s, average: 10.8s (±395.42), ETA: 14:53:20 (01:22:25 to 23::06:02:35)\n",
            "Steps: 77. Time per step: 3.9e-02s. Reward per step: -1.29.\n",
            "It has been 55 episode(s) since the model was last saved, with a score of -102 (Δ-7.67).\n",
            "\n",
            "11:44:58 \n",
            "Episode: **57**/5000, Score: -99 (Δ  0.6)\n",
            "Average score: -109.3 (Δ 0.18)\n",
            "Episode time: 3.0s, average: 10.7s (±389.54), ETA: 14:41:49 (01:22:24 to 22::21:40:08)\n",
            "Steps: 76. Time per step: 3.9e-02s. Reward per step: -1.30.\n",
            "It has been 56 episode(s) since the model was last saved, with a score of -102 (Δ-7.49).\n",
            "\n",
            "Manually shutting down training.\n",
            "\n",
            "----------------- Training ended at 2021-04-01-11:44:59. Duration was 10.18 minutes.-----------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}