{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpS3X73JUdEmNGGpPiFKT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chevron9/iannwtf/blob/marlene/DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18kHLkzg7c5t"
      },
      "source": [
        "import os, time\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "from keras import initializers\r\n",
        "#from initializers import random_uniform"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NxRwcNw7z1P"
      },
      "source": [
        "#Noise for solving Exploration-Exploitation dilemma\r\n",
        "class OUActionNoise(object):\r\n",
        "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\r\n",
        "        self.theta = theta\r\n",
        "        self.mu = mu\r\n",
        "        self.sigma = sigma\r\n",
        "        self.dt = dt\r\n",
        "        self.x0 = x0\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def __call__(self):\r\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\r\n",
        "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\r\n",
        "        self.x_prev = x\r\n",
        "        return x\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(\r\n",
        "                                                            self.mu, self.sigma)\r\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRMLNTZL7xN7"
      },
      "source": [
        "# memory for updating q-value function\r\n",
        "#keeps track of states, actions, rewards and samples them at random\r\n",
        "class ReplayBuffer(object):\r\n",
        "    def __init__(self, max_size, input_shape, n_actions):\r\n",
        "        self.mem_size = max_size\r\n",
        "        self.mem_cntr = 0\r\n",
        "        #here 24x24\r\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape))\r\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\r\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions))\r\n",
        "        self.reward_memory = np.zeros(self.mem_size)\r\n",
        "        #no furhter states follow --> 0 reward\r\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\r\n",
        "\r\n",
        "    def store_transition(self, state, action, reward, state_, done):\r\n",
        "        index = self.mem_cntr % self.mem_size\r\n",
        "        self.state_memory[index] = state\r\n",
        "        self.new_state_memory[index] = state_\r\n",
        "        self.action_memory[index] = action\r\n",
        "        self.reward_memory[index] = reward\r\n",
        "        #gets multiplicated! need 0 if episode is over\r\n",
        "        self.terminal_memory[index] = 1 - done\r\n",
        "        self.mem_cntr += 1\r\n",
        "\r\n",
        "    def sample_buffer(self, batch_size):\r\n",
        "        #until where memory filled!\r\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\r\n",
        "\r\n",
        "        batch = np.random.choice(max_mem, batch_size)\r\n",
        "\r\n",
        "        states = self.state_memory[batch]\r\n",
        "        actions = self.action_memory[batch]\r\n",
        "        rewards = self.reward_memory[batch]\r\n",
        "        states_ = self.new_state_memory[batch]\r\n",
        "        terminal = self.terminal_memory[batch]\r\n",
        "\r\n",
        "        return states, actions, rewards, states_, terminal\r\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJjwcHtI7t2Q"
      },
      "source": [
        "#fc: fully connected\r\n",
        "class Actor(object):\r\n",
        "    #learning rate, session, fullyconnected layers, directory to save checkpoints\r\n",
        "    def __init__(self, lr, n_actions, name, input_dims, sess, fc1_dims,\r\n",
        "                 fc2_dims, action_bound, batch_size=64, chkpt_dir='tmp2/ddpg'):\r\n",
        "        self.lr = lr\r\n",
        "        self.n_actions = n_actions\r\n",
        "        self.name = name\r\n",
        "        self.fc1_dims = fc1_dims\r\n",
        "        self.fc2_dims = fc2_dims\r\n",
        "        #nächste zeile raus?\r\n",
        "        self.chkpt_dir = chkpt_dir\r\n",
        "        self.input_dims = input_dims\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.sess = sess\r\n",
        "        self.action_bound = action_bound\r\n",
        "        #load network\r\n",
        "        self.build_network()\r\n",
        "        self.params = tf.trainable_variables(scope=self.name)\r\n",
        "        self.saver = tf.train.Saver()\r\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, name + '_ddpg.ckpt')\r\n",
        "\r\n",
        "        #manually calculate the gradient of the critic with respect to the actions\r\n",
        "        #that the actor took + calculate gradient of the probability\r\n",
        "        #distribution mu with respect to the parameters (self.params)\r\n",
        "        self.unnormalized_actor_gradients = tf.gradients(\r\n",
        "            self.mu, self.params, -self.action_gradient)\r\n",
        "\r\n",
        "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size),\r\n",
        "                                        self.unnormalized_actor_gradients))\r\n",
        "\r\n",
        "        self.optimize = tf.train.AdamOptimizer(self.lr).\\\r\n",
        "                    apply_gradients(zip(self.actor_gradients, self.params))\r\n",
        "\r\n",
        "    # shape=[NONe- determines batchsize in progress]\r\n",
        "    def build_network(self):\r\n",
        "        with tf.variable_scope(self.name):\r\n",
        "            self.input = tf.placeholder(tf.float32,\r\n",
        "                                        shape=[None, *self.input_dims],\r\n",
        "                                        name='inputs')\r\n",
        "\r\n",
        "            self.action_gradient = tf.placeholder(tf.float32,\r\n",
        "                                          shape=[None, self.n_actions],\r\n",
        "                                          name='gradients')\r\n",
        "\r\n",
        "            f1 = 1. / np.sqrt(self.fc1_dims)\r\n",
        "            dense1 = tf.layers.dense(self.input, units=self.fc1_dims,\r\n",
        "                                     kernel_initializer=random_uniform(-f1, f1),\r\n",
        "                                     bias_initializer=random_uniform(-f1, f1))\r\n",
        "            #helps generalization, to normalize inputs\r\n",
        "            batch1 = tf.layers.batch_normalization(dense1)\r\n",
        "            layer1_activation = tf.nn.relu(batch1)\r\n",
        "\r\n",
        "\r\n",
        "            f2 = 1. / np.sqrt(self.fc2_dims)\r\n",
        "            dense2 = tf.layers.dense(layer1_activation, units=self.fc2_dims,\r\n",
        "                                     kernel_initializer=random_uniform(-f2, f2),\r\n",
        "                                     bias_initializer=random_uniform(-f2, f2))\r\n",
        "            batch2 = tf.layers.batch_normalization(dense2)\r\n",
        "            layer2_activation = tf.nn.relu(batch2)\r\n",
        "\r\n",
        "            f3 = 0.003\r\n",
        "            #size of action space, values between max an min of action space\r\n",
        "            mu = tf.layers.dense(layer2_activation, units=self.n_actions,\r\n",
        "                            activation='tanh',\r\n",
        "                            kernel_initializer= random_uniform(-f3, f3),\r\n",
        "                            bias_initializer=random_uniform(-f3, f3))\r\n",
        "            #bound could have +-2 or +-1 ( do not miss out on actions)\r\n",
        "            self.mu = tf.multiply(mu, self.action_bound)\r\n",
        "\r\n",
        "    # run input through network\r\n",
        "    def predict(self, inputs):\r\n",
        "        return self.sess.run(self.mu, feed_dict={self.input: inputs})\r\n",
        "\r\n",
        "    #optimization step\r\n",
        "    def train(self, inputs, gradients):\r\n",
        "        self.sess.run(self.optimize,\r\n",
        "                      feed_dict={self.input: inputs,\r\n",
        "                                 self.action_gradient: gradients})\r\n",
        "\r\n",
        "    #loads session from file and stack it on curreetn session\r\n",
        "    def load_checkpoint(self):\r\n",
        "        print(\"...Loading checkpoint...\")\r\n",
        "        self.saver.restore(self.sess, self.checkpoint_file)\r\n",
        "\r\n",
        "    def save_checkpoint(self):\r\n",
        "        print(\"...Saving checkpoint...\")\r\n",
        "        self.saver.save(self.sess, self.checkpoint_file)\r\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sktClDN_7qaA"
      },
      "source": [
        "#constructor same as actor class\r\n",
        "class Critic(object):\r\n",
        "    def __init__(self, lr, n_actions, name, input_dims, sess, fc1_dims, fc2_dims,\r\n",
        "                 batch_size=64, chkpt_dir='tmp2/ddpg'):\r\n",
        "        self.lr = lr\r\n",
        "        self.n_actions = n_actions\r\n",
        "        self.name = name\r\n",
        "        self.fc1_dims = fc1_dims\r\n",
        "        self.fc2_dims = fc2_dims\r\n",
        "        self.chkpt_dir = chkpt_dir\r\n",
        "        self.input_dims = input_dims\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.sess = sess\r\n",
        "        self.build_network()\r\n",
        "        self.params = tf.trainable_variables(scope=self.name)\r\n",
        "        self.saver = tf.train.Saver()\r\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, name +'_ddpg.ckpt')\r\n",
        "        #optimize loss!\r\n",
        "        self.optimize = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\r\n",
        "\r\n",
        "        self.action_gradients = tf.gradients(self.q, self.actions)\r\n",
        "\r\n",
        "    def build_network(self):\r\n",
        "        with tf.variable_scope(self.name):\r\n",
        "            self.input = tf.placeholder(tf.float32,\r\n",
        "                                        shape=[None, *self.input_dims],\r\n",
        "                                        name='inputs')\r\n",
        "\r\n",
        "            self.actions = tf.placeholder(tf.float32,\r\n",
        "                                          shape=[None, self.n_actions],\r\n",
        "                                          name='actions')\r\n",
        "\r\n",
        "            self.q_target = tf.placeholder(tf.float32,\r\n",
        "                                           shape=[None,1],\r\n",
        "                                           name='targets')\r\n",
        "\r\n",
        "            f1 = 1. / np.sqrt(self.fc1_dims)\r\n",
        "            dense1 = tf.layers.dense(self.input, units=self.fc1_dims,\r\n",
        "                                     kernel_initializer=random_uniform(-f1, f1),\r\n",
        "                                     bias_initializer=random_uniform(-f1, f1))\r\n",
        "            batch1 = tf.layers.batch_normalization(dense1)\r\n",
        "            layer1_activation = tf.nn.relu(batch1)\r\n",
        "\r\n",
        "            f2 = 1. / np.sqrt(self.fc2_dims)\r\n",
        "            dense2 = tf.layers.dense(layer1_activation, units=self.fc2_dims,\r\n",
        "                                     kernel_initializer=random_uniform(-f2, f2),\r\n",
        "                                     bias_initializer=random_uniform(-f2, f2))\r\n",
        "            batch2 = tf.layers.batch_normalization(dense2)\r\n",
        "            #layer2_activation = tf.nn.relu(batch2)\r\n",
        "            #layer2_activation = tf.nn.relu(dense2)\r\n",
        "\r\n",
        "            action_in = tf.layers.dense(self.actions, units=self.fc2_dims,\r\n",
        "                                        activation='relu')\r\n",
        "            #batch2 = tf.nn.relu(batch2)\r\n",
        "            # no activation on action_in and relu activation on state_actions seems to\r\n",
        "            # perform poorly.\r\n",
        "            # relu activation on action_in and relu activation on state_actions\r\n",
        "            # does reasonably well.\r\n",
        "            # relu on batch2 and relu on action in performs poorly\r\n",
        "\r\n",
        "            #state_actions = tf.concat([layer2_activation, action_in], axis=1)\r\n",
        "            state_actions = tf.add(batch2, action_in)\r\n",
        "            #debateably!\r\n",
        "            state_actions = tf.nn.relu(state_actions)\r\n",
        "\r\n",
        "            f3 = 0.003\r\n",
        "            #double activation from paper\r\n",
        "            self.q = tf.layers.dense(state_actions, units=1,\r\n",
        "                               kernel_initializer=random_uniform(-f3, f3),\r\n",
        "                               bias_initializer=random_uniform(-f3, f3),\r\n",
        "                               #l2 from paper\r\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(0.01))\r\n",
        "\r\n",
        "            self.loss = tf.losses.mean_squared_error(self.q_target, self.q)\r\n",
        "\r\n",
        "    def predict(self, inputs, actions):\r\n",
        "        return self.sess.run(self.q,\r\n",
        "                             feed_dict={self.input: inputs,\r\n",
        "                                        self.actions: actions})\r\n",
        "    def train(self, inputs, actions, q_target):\r\n",
        "        return self.sess.run(self.optimize,\r\n",
        "                      feed_dict={self.input: inputs,\r\n",
        "                                 self.actions: actions,\r\n",
        "                                 self.q_target: q_target})\r\n",
        "\r\n",
        "    def get_action_gradients(self, inputs, actions):\r\n",
        "        return self.sess.run(self.action_gradients,\r\n",
        "                             feed_dict={self.input: inputs,\r\n",
        "                                        self.actions: actions})\r\n",
        "    def load_checkpoint(self):\r\n",
        "        print(\"...Loading checkpoint...\")\r\n",
        "        self.saver.restore(self.sess, self.checkpoint_file)\r\n",
        "\r\n",
        "    def save_checkpoint(self):\r\n",
        "        print(\"...Saving checkpoint...\")\r\n",
        "        self.saver.save(self.sess, self.checkpoint_file)\r\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQjH5J6p7lco"
      },
      "source": [
        "class Agent(object):\r\n",
        "    #alpha: learning rate for actor, beta:learning rate for critic, tau: multiplicative\r\n",
        "    #factor for the soft update of the network parameters, gamma: discount factor for\r\n",
        "    #the agents calcuations of the Belman equation (how much to discount future rewards)\r\n",
        "    #numbers from paper\r\n",
        "    def __init__(self, alpha, beta, input_dims, tau, env, gamma=0.99, n_actions=2,\r\n",
        "                 max_size=1000000, layer1_size=400, layer2_size=300,\r\n",
        "                 batch_size=64, chkpt_dir='tmp/ddpg'):\r\n",
        "        self.gamma = gamma\r\n",
        "        self.tau = tau\r\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.sess = tf.Session()\r\n",
        "        #4 networks, target networks to prevent the problem of using the\r\n",
        "        #same network for both calculating actions and calculating the value\r\n",
        "        #of that action - Params maximization bias (?)\r\n",
        "        self.actor = Actor(alpha, n_actions, 'Actor', input_dims, self.sess,\r\n",
        "                           layer1_size, layer2_size, env.action_space.high,\r\n",
        "                            chkpt_dir=chkpt_dir)\r\n",
        "        self.critic = Critic(beta, n_actions, 'Critic', input_dims,self.sess,\r\n",
        "                             layer1_size, layer2_size, chkpt_dir=chkpt_dir)\r\n",
        "\r\n",
        "        self.target_actor = Actor(alpha, n_actions, 'TargetActor',\r\n",
        "                                  input_dims, self.sess, layer1_size,\r\n",
        "                                  layer2_size, env.action_space.high,\r\n",
        "                                  chkpt_dir=chkpt_dir)\r\n",
        "        self.target_critic = Critic(beta, n_actions, 'TargetCritic', input_dims,\r\n",
        "                                    self.sess, layer1_size, layer2_size,\r\n",
        "                                    chkpt_dir=chkpt_dir)\r\n",
        "\r\n",
        "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        #update the target critic params with the value of tau multiplied by the\r\n",
        "        #critc parameters plus 1-tau * the targetcritic parameters\r\n",
        "\r\n",
        "        # define ops here in __init__ otherwise time to execute the op\r\n",
        "        # increases with each execution.\r\n",
        "        #assign: assign one vector to another\r\n",
        "        self.update_critic = \\\r\n",
        "        [self.target_critic.params[i].assign(\r\n",
        "                      tf.multiply(self.critic.params[i], self.tau) \\\r\n",
        "                    + tf.multiply(self.target_critic.params[i], 1. - self.tau))\r\n",
        "         for i in range(len(self.target_critic.params))]\r\n",
        "\r\n",
        "        #same for actor\r\n",
        "        self.update_actor = \\\r\n",
        "        [self.target_actor.params[i].assign(\r\n",
        "                      tf.multiply(self.actor.params[i], self.tau) \\\r\n",
        "                    + tf.multiply(self.target_actor.params[i], 1. - self.tau))\r\n",
        "         for i in range(len(self.target_actor.params))]\r\n",
        "\r\n",
        "        #initialize random variables in network\r\n",
        "        self.sess.run(tf.global_variables_initializer())\r\n",
        "        #first time update, store that it is the first time\r\n",
        "        self.update_network_parameters(first=True)\r\n",
        "\r\n",
        "    def update_network_parameters(self, first=False):\r\n",
        "        if first:\r\n",
        "            old_tau = self.tau\r\n",
        "            self.tau = 1.0\r\n",
        "            self.target_critic.sess.run(self.update_critic)\r\n",
        "            self.target_actor.sess.run(self.update_actor)\r\n",
        "            self.tau = old_tau\r\n",
        "        else:\r\n",
        "            self.target_critic.sess.run(self.update_critic)\r\n",
        "            self.target_actor.sess.run(self.update_actor)\r\n",
        "\r\n",
        "    def remember(self, state, action, reward, new_state, done):\r\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\r\n",
        "\r\n",
        "    def choose_action(self, state):\r\n",
        "        #reshape to 1x number of observations\r\n",
        "        state = state[np.newaxis, :]\r\n",
        "        mu = self.actor.predict(state) # returns list of list\r\n",
        "        #noise = self.noise()\r\n",
        "        ## TODO: mu_prime = mu + self.noise()\r\n",
        "        mu_prime = mu# + noise\r\n",
        "\r\n",
        "        return mu_prime[0]\r\n",
        "\r\n",
        "    def learn(self):\r\n",
        "        #let the replay buffer fill up the batch size and then start sampling\r\n",
        "        if self.memory.mem_cntr < self.batch_size:\r\n",
        "            return\r\n",
        "        state, action, reward, new_state, done = \\\r\n",
        "                                      self.memory.sample_buffer(self.batch_size)\r\n",
        "        #feedforward to get values, from papaer (two feedforward in one step)\r\n",
        "        critic_value_ = self.target_critic.predict(new_state,\r\n",
        "                                           self.target_actor.predict(new_state))\r\n",
        "        target = []\r\n",
        "        #get target, iterate over memory manually\r\n",
        "        for j in range(self.batch_size):\r\n",
        "            target.append(reward[j] + self.gamma*critic_value_[j]*done[j])\r\n",
        "        target = np.reshape(target, (self.batch_size, 1))\r\n",
        "\r\n",
        "        #critic training\r\n",
        "        _ = self.critic.train(state, action, target)\r\n",
        "\r\n",
        "        #gradients from the critic, values of the states from the actor and using those to train the actor\r\n",
        "        a_outs = self.actor.predict(state)\r\n",
        "        grads = self.critic.get_action_gradients(state, a_outs)\r\n",
        "\r\n",
        "        self.actor.train(state, grads[0])\r\n",
        "\r\n",
        "        #update network parameters\r\n",
        "        self.update_network_parameters()\r\n",
        "\r\n",
        "        #learning in each step of iteration --> computationally costly\r\n",
        "\r\n",
        "    def save_models(self):\r\n",
        "        self.actor.save_checkpoint()\r\n",
        "        self.target_actor.save_checkpoint()\r\n",
        "        self.critic.save_checkpoint()\r\n",
        "        self.target_critic.save_checkpoint()\r\n",
        "\r\n",
        "    def load_models(self):\r\n",
        "        self.actor.load_checkpoint()\r\n",
        "        self.target_actor.load_checkpoint()\r\n",
        "        self.critic.load_checkpoint()\r\n",
        "        self.target_critic.load_checkpoint()\r\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "2n98QSTr8dvv",
        "outputId": "c2aa4749-a2c5-436a-bae7-efedd46f63b9"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "!pip install utils\r\n",
        "#from utils import plotLearning\r\n",
        "from gym import wrappers\r\n",
        "import os\r\n",
        "\r\n",
        "#tf.set_random_seed(0)\r\n",
        "if __name__ == '__main__':\r\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n",
        "\r\n",
        "    env = gym.make('bipedal_walker')\r\n",
        "    #\r\n",
        "    agent = Agent(alpha=0.00005, beta=0.0005, input_dims=[24], tau=0.001, env=env,\r\n",
        "                  batch_size=64,  layer1_size=400, layer2_size=300, n_actions=4,\r\n",
        "                  ## TODO: nächste Zeile weg?\r\n",
        "                  chkpt_dir='tmp/ddpg')\r\n",
        "    #to get kind of repeatability\r\n",
        "    np.random.seed(0)\r\n",
        "    #agent.load_models()\r\n",
        "    #env = wrappers.Monitor(env, \"tmp/walker2d\",\r\n",
        "    #                            video_callable=lambda episode_id: True, force=True)\r\n",
        "    score_history = []\r\n",
        "    for i in range(5000):\r\n",
        "        obs = env.reset()\r\n",
        "        done = False\r\n",
        "        score = 0\r\n",
        "        while not done:\r\n",
        "            act = agent.choose_action(obs)\r\n",
        "            new_state, reward, done, info = env.step(act)\r\n",
        "            agent.remember(obs, act, reward, new_state, int(done))\r\n",
        "            agent.learn()\r\n",
        "            score += reward\r\n",
        "            obs = new_state\r\n",
        "            #see how it looks like\r\n",
        "            env.render()\r\n",
        "        score_history.append(score)\r\n",
        "        print('episode ', i, 'score %.2f' % score,\r\n",
        "                ## TODO: next line a 2?\r\n",
        "              'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))\r\n",
        "        if i % 25 == 0:\r\n",
        "            agent.save_models()\r\n",
        "    filename = 'WalkerTF-alpha00005-beta0005-400-300-original-5000games-testing.png'\r\n",
        "    plotLearning(score_history, filename, window=100)\r\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-b21657e09e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bipedal_walker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     agent = Agent(alpha=0.00005, beta=0.0005, input_dims=[24], tau=0.001, env=env,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_id_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to look up malformed environment ID: {}. (Currently all IDs must be of the form {}.)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Attempted to look up malformed environment ID: b'bipedal_walker'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)"
          ]
        }
      ]
    }
  ]
}