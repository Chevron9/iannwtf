{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddpg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMBMp+AR3UBo//Lmq4brAtn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chevron9/iannwtf/blob/master/ddpg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsSbPZt2wKxh",
        "outputId": "fdfeeb00-eb4c-4715-eaef-a85b037a8c6c"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "\n",
        "import gym\n",
        "env = gym.make(\"BipedalWalker-v3\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX_s3Q4LwNWN",
        "outputId": "e9b0755d-05dd-4d7b-bed4-6a6d600b7547"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os \n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYoGfSs2zgJz"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plots the scores of the course of training\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgD-CyT4zqCn"
      },
      "source": [
        "def timespan_format(timespan):\n",
        "    timespan = round(timespan)\n",
        "    h_full = timespan / (60*60)\n",
        "    h = timespan // (60*60)\n",
        "    m = (timespan % (60*60)) // 60\n",
        "    s = timespan % 60\n",
        "    time = f\"{h:02}:{m:02}:{s:02}\"\n",
        "    if h_full > 24:\n",
        "        d = timespan // (60*60*24)\n",
        "        h = (timespan % (60*60*24)) // (60*60)\n",
        "        time = f\"{d:02}::{h:02}:{m:02}:{s:02}\"\n",
        "    return time\n",
        "\n",
        "timespan_format(303601)\n",
        "test = False\n",
        "if test:\n",
        "    assert timespan_format(60) == \"00:01:00\"\n",
        "    assert timespan_format(60*60) == \"01:00:00\"\n",
        "    assert timespan_format(60*60+1) == \"01:00:01\"\n",
        "    assert timespan_format(60*60+60) == \"01:01:00\"\n",
        "    assert timespan_format(24*60*60+1) == \"01::00:00:01\"\n",
        "    assert timespan_format(303601) == \"03::12:20:01\" # 3 days 12 hours 20 minutes 1 second\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVmB-P2_wTm4"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "#all the networks of the model\n",
        "\n",
        "\n",
        "#critic network that takes the state and the action and puts out the value of the\n",
        "#the action in the given state\n",
        "class CriticNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512,\n",
        "            name='critic', chkpt_dir='tmp/'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        #Dimensions of the dense layers\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "\n",
        "\n",
        "        #says where the weights are saved\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_ddpg.h5')\n",
        "\n",
        "\n",
        "        #Optimization: using LeakyReLU as per https://arxiv.org/pdf/1709.06560.pdf\n",
        "\n",
        "        #dense layers with kernel and bias initializer(from an other implementation)\n",
        "        #and relu activation\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "        #denselayer with 1 neuron that gives the estimated q value of the\n",
        "        #state-action pair\n",
        "        f3 = 0.003\n",
        "        self.q = Dense(1, activation=None, kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3) ,\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f3, f3),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, action, training = True):\n",
        "        #feeds the network state and action pairs\n",
        "        action_value = self.dense_layer1(tf.concat([state, action], axis=1))\n",
        "\n",
        "        action_value = self.dense_layer2(action_value)\n",
        "\n",
        "        q = self.q(action_value)\n",
        "\n",
        "        #gives back an estimation of a q value\n",
        "        return q\n",
        "\n",
        "\n",
        "#critic network that takes the state and outputs a probability\n",
        "#distribution over all possible actions\n",
        "class ActorNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512, n_actions=4, name='actor',\n",
        "            chkpt_dir='tmp/'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_ddpg.h5')\n",
        "\n",
        "        #first dense layer\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        #second dense layer\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "\n",
        "        #output layer with tanh activation to get an output vector of length actionspace\n",
        "        #with values between -1 and 1 to fit to the action boundaries\n",
        "        f3 = 0.003\n",
        "        self.mu = Dense(self.n_actions, activation='tanh', kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3) , bias_initializer\n",
        "         = tf.keras.initializers.RandomUniform(-f3, f3))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, training = True):\n",
        "\n",
        "        actions = self.dense_layer1(state)\n",
        "        actions = self.dense_layer2(actions)\n",
        "\n",
        "        #gives back the actions the agent should take (deterministic policy)\n",
        "        actions = self.mu(actions)\n",
        "\n",
        "\n",
        "        return actions\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIiiKOnIwWGs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replaybuffer that stores all informations about the agents transitions\n",
        "# in np arrays\n",
        "class ReplayBuffer:\n",
        "    #initializes the memories with zeros with sizes depending on a big maximal size,\n",
        "    #the input_dimensions(output of the environment) or the numbers of possible\n",
        "    #actions\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.max_size = max_size\n",
        "        self.current_position = 0\n",
        "        self.state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.new_state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.action_memory = np.zeros((self.max_size, n_actions))\n",
        "        self.reward_memory = np.zeros(self.max_size)\n",
        "        self.terminal_memory = np.zeros(self.max_size, dtype=np.bool)\n",
        "\n",
        "    #stores new transitions in memory\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.current_position % self.max_size\n",
        "\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.current_position+= 1\n",
        "\n",
        "    #gives back a random batch of of transition samples\n",
        "    def sample_buffer(self, batch_size):\n",
        "        #makes sure to have the correct current size of the memory\n",
        "        max_mem = min(self.current_position, self.max_size)\n",
        "\n",
        "        #selects a random batch of indexes in the memory size\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        #retrieves the batch from memory\n",
        "        states = self.state_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, dones"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yWD6XYVwZNH"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# the agent class where the all the important parameters and systems of the\n",
        "# model are managed\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, input_dims, alpha=0.001, beta=0.002, env=None,\n",
        "            gamma=0.99, n_actions=4, max_size=1000000, tau=0.001,\n",
        "            dense1=512, dense2=512, batch_size=64, noise=0.3, module_dir = \"\"):\n",
        "\n",
        "        #initializing network-parameters\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.n_actions = n_actions\n",
        "        self.noise = noise\n",
        "\n",
        "        #retrieves the maximum and minimum of the actionvalues\n",
        "        self.max_action = env.action_space.high[0]\n",
        "        self.min_action = env.action_space.low[0]\n",
        "\n",
        "        #initializes the Replaybuffer which stores what the agents does\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "\n",
        "        # initializing the Networks with given parameters\n",
        "        # target_actor and target_critic are just initialized as the actor and\n",
        "        # critic networks\n",
        "\n",
        "        chkpt_dir = module_dir+\"/tmp\"\n",
        "        self.actor = ActorNetwork(n_actions=n_actions, name='actor', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_actor = ActorNetwork(n_actions=n_actions, name='target_actor',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "        self.critic = CriticNetwork(name='critic',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_critic = CriticNetwork(name='target_critic', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "\n",
        "        #compile the networks with learningrates\n",
        "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "\n",
        "        self.critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "        self.target_critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "\n",
        "        self.update_network_parameters(tau=1) #Hard copy, since this is the initialization\n",
        "\n",
        "    #updates the target networks\n",
        "    #soft copies the target and actor network dependent on tau\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_actor.weights\n",
        "        for i, weight in enumerate(self.actor.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_actor.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic.weights\n",
        "        for i, weight in enumerate(self.critic.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_critic.set_weights(weights)\n",
        "\n",
        "    #stores the state, action, reward transition\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    #saves models in files\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.save_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.save_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    #loads models from files\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.load_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    #choose_action with help of the actor network, adds noise if it for training\n",
        "    def choose_action(self, observation, evaluate=False):\n",
        "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "        actions = self.actor(state)\n",
        "\n",
        "        # inject eploration noise\n",
        "        if not evaluate:\n",
        "\n",
        "            actions += tf.random.normal(shape=[self.n_actions],\n",
        "                    mean=0.0, stddev=self.noise)\n",
        "\n",
        "        #makes sure that action boundaries are met\n",
        "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
        "\n",
        "        return actions[0]\n",
        "\n",
        "    #learn function of the networks\n",
        "    def learn(self):\n",
        "        #starts to learn when there are enough samples to fill a batch\n",
        "        if self.memory.current_position < self.batch_size:\n",
        "            return\n",
        "\n",
        "        #gets batch form memory\n",
        "        state, action, reward, new_state, done = \\\n",
        "                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        #convert np arrays to tensors to feed them to the networks\n",
        "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
        "\n",
        "        #update critic network\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            #target actor decides which action to take\n",
        "            target_actions = self.target_actor(states_)\n",
        "            #target critic evaluates the value of the actions in the given states\n",
        "            critic_value_ = tf.squeeze(self.target_critic(\n",
        "                                states_, target_actions), 1)\n",
        "\n",
        "            #critic network evaluate the actual states and actions the model took\n",
        "            critic_value = tf.squeeze(self.critic(states, actions), 1)\n",
        "\n",
        "            #target says what value of the action in a certain state should\n",
        "            #be like\n",
        "            target = rewards + self.gamma*critic_value_*(1-done)\n",
        "\n",
        "            #takes the MSE of the target and the actual critic value as the loss\n",
        "            critic_loss = keras.losses.MSE(target, critic_value)\n",
        "\n",
        "\n",
        "        #gets the gradients of the loss in respect to the parameters of the network\n",
        "        critic_network_gradient = tape.gradient(critic_loss,\n",
        "                                            self.critic.trainable_variables)\n",
        "\n",
        "        #aplies the gradients to the critic network\n",
        "        self.critic.optimizer.apply_gradients(zip(\n",
        "            critic_network_gradient, self.critic.trainable_variables))\n",
        "\n",
        "        #update the actor network\n",
        "        with tf.GradientTape() as tape:\n",
        "            #gets the policy of the actor in a state\n",
        "            action_policy = self.actor(states)\n",
        "\n",
        "            #loss of the actor is the negative value of the critic because we\n",
        "            #want to maximize the value but gradient decent minimizes\n",
        "            actor_loss = -self.critic(states, action_policy)\n",
        "\n",
        "            #the loss is a average of all the losses\n",
        "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
        "\n",
        "        #gradients of the loss in respect to the parameters of the actor network\n",
        "        actor_network_gradient = tape.gradient(actor_loss,\n",
        "                                    self.actor.trainable_variables)\n",
        "\n",
        "\n",
        "        #optimizing the network gradients\n",
        "        self.actor.optimizer.apply_gradients(zip(\n",
        "            actor_network_gradient, self.actor.trainable_variables))\n",
        "\n",
        "        self.update_network_parameters()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IbFPh3tnwdlA",
        "outputId": "fb74133a-708e-497a-fd04-96ca711bcb9f"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os\n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import gym.envs.box2d\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "# modules\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    tf.keras.backend.clear_session()\n",
        "    #for the case you just want to load a previous model\n",
        "    load_checkpoint = False\n",
        "\n",
        "    #Housekeeping variables\n",
        "    last_score = 0\n",
        "    last_avg_score = 0\n",
        "    last_save = 0\n",
        "    avg_delta = []\n",
        "    avg_steps = []\n",
        "\n",
        "    t = t_start = time.localtime()\n",
        "    current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t)\n",
        "    print(f\"\\n----------------- Training started at {current_time}. -------------------\\ncheckpoint: {load_checkpoint}\")\n",
        "\n",
        "    module_dir = \"\"\n",
        "    figure_dir = module_dir+f'plots/'\n",
        "    figure_file = figure_dir+f'walker{current_time.replace(\":\",\"_\")}.png'\n",
        "\n",
        "    log_dir = module_dir+'logs/' + current_time.replace(\":\",\"_\")\n",
        "\n",
        "    #Tensorboard writer\n",
        "    writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "\n",
        "    #initialize the environment for the agent and initialize the agent\n",
        "\n",
        "    #tf.debugging.set_log_device_placement(True)\n",
        "    env = gym.make(\"BipedalWalker-v3\")\n",
        "    env._max_episode_steps = 1200\n",
        "    #env = gym.make('BipedalWalkerHardcore-v3')\n",
        "\n",
        "\n",
        "    n_actions = env.action_space.shape[0]\n",
        "\n",
        "    noise = 0.3\n",
        "    # NEW batch 128\n",
        "    agent = Agent(alpha=0.00005, beta=0.0005, input_dims=env.observation_space.shape, tau=0.001, env=env,\n",
        "                  batch_size=64, dense1=400, dense2=300, n_actions=n_actions, noise = noise, module_dir = module_dir)\n",
        "\n",
        "\n",
        "    episodes = 4000\n",
        "\n",
        "\n",
        "\n",
        "    #set bestscore to minimum\n",
        "    best_score = env.reward_range[0]\n",
        "    score_history = []\n",
        "\n",
        "\n",
        "    #initializes the model with one random sample batch if model are loaded\n",
        "    #you can't load an empty model for some reason\n",
        "    #these are all dummy variables etc until load_models overwrites them\n",
        "    if load_checkpoint:\n",
        "        n_steps = 0\n",
        "        while n_steps <= agent.batch_size:\n",
        "            observation = env.reset()\n",
        "            action = env.action_space.sample()\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            agent.remember(observation, action, reward, observation_, done)\n",
        "            n_steps += 1\n",
        "        agent.learn()\n",
        "        agent.load_models()\n",
        "\n",
        "    # ---------------------------------------\n",
        "    # main learning loop\n",
        "    # ---------------------------------------\n",
        "    \n",
        "    try:\n",
        "        for i in range(episodes):\n",
        "            if i == 3:\n",
        "                tf.profiler.experimental.server.start(6009)\n",
        "                print(\"profiler started\")\n",
        "\n",
        "                %load_ext tensorboard\n",
        "                %tensorboard --logdir logs\n",
        "                \n",
        "                # launch tensorboard with \"tensorboard --logdir logs\"\n",
        "                # capture profile\n",
        "            #if i == 13:\n",
        "            #    tf.profiler.experimental.stop\n",
        "            #    print(\"profiler stopped\")\n",
        "\n",
        "\n",
        "            current_episode = i\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "\n",
        "            #regulates the noise over the course of training as exponential\n",
        "            #decay to get smaller noise at the end, the noise is the\n",
        "            #standarddeviation of a normal distribution\n",
        "            #(numbers from trial and error)\n",
        "            agent.noise = noise * np.exp(-i/1500)\n",
        "\n",
        "            #while the environment is running the model chooses actions, saves states,\n",
        "            #rewards, actions and observations in the buffer and trains the networks\n",
        "            #on them\n",
        "            steps = 0\n",
        "            while not done:\n",
        "                action = agent.choose_action(observation)\n",
        "\n",
        "                observation_, reward, done, info = env.step(action)\n",
        "                steps += 1\n",
        "                score += reward\n",
        "                agent.remember(observation, action, reward, observation_, done)\n",
        "                agent.learn()\n",
        "\n",
        "                #saves previous observation\n",
        "                observation = observation_\n",
        "\n",
        "            score_history.append(score)\n",
        "            avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "            #saves the model if the average score is better than the best previous\n",
        "            if avg_score > best_score:\n",
        "                best_score = avg_score\n",
        "                agent.save_models()\n",
        "                last_save = current_episode\n",
        "\n",
        "            #calculating and giving some info on training progress\n",
        "            t_new = time.localtime()\n",
        "            current_time = time.strftime(\"%H:%M:%S\", t_new)\n",
        "            t_delta = time.mktime(t_new)-time.mktime(t)\n",
        "            t = t_new\n",
        "            avg_delta.append(t_delta)\n",
        "            avg_delta_mean = np.mean(avg_delta)\n",
        "            avg_delta_std = np.var(avg_delta)\n",
        "\n",
        "            ETA_avg = (episodes-i)*avg_delta_mean\n",
        "            ETA_min = (episodes-i)*max((avg_delta_mean-avg_delta_std),min(avg_delta))\n",
        "            ETA_max = (episodes-i)*(avg_delta_mean+avg_delta_std)\n",
        "\n",
        "            avg_steps.append(steps)\n",
        "            per_step = t_delta/steps\n",
        "            steps_per_score = score/steps\n",
        "\n",
        "            print(f\"{current_time} \\n\"\n",
        "            f'Episode: **{i+1}**/{episodes}, Score: {score:.0f} (Δ{score-last_score:5.1f})\\n'\n",
        "            f'Average score: {avg_score:.1f} (Δ{avg_score-last_avg_score:5.2f})\\n'\n",
        "            f'Episode time: {t_delta:.1f}s, average: {avg_delta_mean:.1f}s (±{avg_delta_std:4.2f}),',\n",
        "            f'ETA: {timespan_format(ETA_avg)} ({timespan_format(ETA_min)} to {timespan_format(ETA_max)})\\n'\n",
        "            f'Steps: {steps}. Time per step: {per_step:.1e}s. Reward per step: {steps_per_score:.2f}.\\n'\n",
        "            f'It has been {i - last_save} episode(s) since the model was last saved, with a score of {best_score:.0f} (Δ{avg_score-best_score:2.2f}).\\n')\n",
        "\n",
        "            last_score = score\n",
        "            last_avg_score = avg_score\n",
        "\n",
        "\n",
        "            with writer.as_default():\n",
        "                tf.summary.scalar('Average Score', avg_score, step=i)\n",
        "                tf.summary.scalar('ETA', ETA_avg, step=i)\n",
        "                tf.summary.scalar('Calculation time per step', per_step, step=i)\n",
        "                tf.summary.scalar('Calculation time per episode', t_delta, step=i)\n",
        "                tf.summary.scalar('Steps', steps, step=i)\n",
        "                if ((i+1) % 50) == 0: #writer.flush and learning plot has a large performance impact, so only do it every 50 episodes\n",
        "                    writer.flush()\n",
        "                    x = [j+1 for j in range(current_episode+1)]\n",
        "                    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        episodes = current_episode\n",
        "        print(\"Manually shutting down training.\")\n",
        "\n",
        "    #plots the whole score history\n",
        "    x = [i+1 for i in range(episodes)]\n",
        "    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "t2 = time.localtime()\n",
        "current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t2)\n",
        "t_delta = time.mktime(t2)-time.mktime(t_start)\n",
        "print(f\"\\n----------------- Training ended at {current_time}. Duration was {t_delta/60:.2f} minutes.-----------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n",
            "\n",
            "----------------- Training started at 2021-04-01-13:18:27. -------------------\n",
            "checkpoint: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... saving models ...\n",
            "13:18:30 \n",
            "Episode: **1**/4000, Score: -119 (Δ-119.3)\n",
            "Average score: -119.3 (Δ-119.27)\n",
            "Episode time: 3.0s, average: 3.0s (±0.00), ETA: 03:20:00 (03:20:00 to 03:20:00)\n",
            "Steps: 123. Time per step: 2.4e-02s. Reward per step: -0.97.\n",
            "It has been 0 episode(s) since the model was last saved, with a score of -119 (Δ0.00).\n",
            "\n",
            "... saving models ...\n",
            "13:18:34 \n",
            "Episode: **2**/4000, Score: -111 (Δ  8.6)\n",
            "Average score: -114.9 (Δ 4.32)\n",
            "Episode time: 4.0s, average: 3.5s (±0.25), ETA: 03:53:16 (03:36:37 to 04:09:56)\n",
            "Steps: 98. Time per step: 4.1e-02s. Reward per step: -1.13.\n",
            "It has been 0 episode(s) since the model was last saved, with a score of -115 (Δ0.00).\n",
            "\n",
            "... saving models ...\n",
            "13:19:18 \n",
            "Episode: **3**/4000, Score: -62 (Δ 49.0)\n",
            "Average score: -97.2 (Δ17.77)\n",
            "Episode time: 44.0s, average: 17.0s (±364.67), ETA: 18:52:46 (03:19:54 to 17::15:51:43)\n",
            "Steps: 1200. Time per step: 3.7e-02s. Reward per step: -0.05.\n",
            "It has been 0 episode(s) since the model was last saved, with a score of -97 (Δ0.00).\n",
            "\n",
            "profiler started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 127), started 0:23:36 ago. (Use '!kill 127' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "... saving models ...\n",
            "13:20:01 \n",
            "Episode: **4**/4000, Score: -60 (Δ  1.5)\n",
            "Average score: -87.9 (Δ 9.27)\n",
            "Episode time: 43.0s, average: 23.5s (±400.25), ETA: 01::02:05:30 (03:19:51 to 19::14:28:49)\n",
            "Steps: 1200. Time per step: 3.6e-02s. Reward per step: -0.05.\n",
            "It has been 0 episode(s) since the model was last saved, with a score of -88 (Δ0.00).\n",
            "\n",
            "13:20:04 \n",
            "Episode: **5**/4000, Score: -103 (Δ-43.2)\n",
            "Average score: -91.0 (Δ-3.08)\n",
            "Episode time: 3.0s, average: 19.4s (±387.44), ETA: 21:32:02 (03:19:48 to 18::19:35:33)\n",
            "Steps: 64. Time per step: 4.7e-02s. Reward per step: -1.61.\n",
            "It has been 1 episode(s) since the model was last saved, with a score of -88 (Δ-3.08).\n",
            "\n",
            "13:20:09 \n",
            "Episode: **6**/4000, Score: -106 (Δ -2.7)\n",
            "Average score: -93.5 (Δ-2.50)\n",
            "Episode time: 5.0s, average: 17.0s (±351.67), ETA: 18:51:55 (03:19:45 to 17::01:07:03)\n",
            "Steps: 155. Time per step: 3.2e-02s. Reward per step: -0.68.\n",
            "It has been 2 episode(s) since the model was last saved, with a score of -88 (Δ-5.58).\n",
            "\n",
            "13:20:13 \n",
            "Episode: **7**/4000, Score: -105 (Δ  0.8)\n",
            "Average score: -95.1 (Δ-1.67)\n",
            "Episode time: 4.0s, average: 15.1s (±322.12), ETA: 16:48:01 (03:19:42 to 15::14:10:38)\n",
            "Steps: 109. Time per step: 3.7e-02s. Reward per step: -0.96.\n",
            "It has been 3 episode(s) since the model was last saved, with a score of -88 (Δ-7.25).\n",
            "\n",
            "13:20:16 \n",
            "Episode: **8**/4000, Score: -119 (Δ-13.5)\n",
            "Average score: -98.1 (Δ-2.94)\n",
            "Episode time: 3.0s, average: 13.6s (±297.98), ETA: 15:06:45 (03:19:39 to 14::09:37:36)\n",
            "Steps: 65. Time per step: 4.6e-02s. Reward per step: -1.83.\n",
            "It has been 4 episode(s) since the model was last saved, with a score of -88 (Δ-10.18).\n",
            "\n",
            "13:20:33 \n",
            "Episode: **9**/4000, Score: -123 (Δ -4.3)\n",
            "Average score: -100.8 (Δ-2.76)\n",
            "Episode time: 17.0s, average: 14.0s (±266.00), ETA: 15:31:28 (03:19:36 to 12::22:29:20)\n",
            "Steps: 467. Time per step: 3.6e-02s. Reward per step: -0.26.\n",
            "It has been 5 episode(s) since the model was last saved, with a score of -88 (Δ-12.95).\n",
            "\n",
            "13:20:37 \n",
            "Episode: **10**/4000, Score: -119 (Δ  4.2)\n",
            "Average score: -102.6 (Δ-1.79)\n",
            "Episode time: 4.0s, average: 13.0s (±248.40), ETA: 14:24:43 (03:19:33 to 12::01:47:27)\n",
            "Steps: 125. Time per step: 3.2e-02s. Reward per step: -0.95.\n",
            "It has been 6 episode(s) since the model was last saved, with a score of -88 (Δ-14.73).\n",
            "\n",
            "13:20:41 \n",
            "Episode: **11**/4000, Score: -119 (Δ -0.4)\n",
            "Average score: -104.1 (Δ-1.50)\n",
            "Episode time: 4.0s, average: 12.2s (±232.51), ETA: 13:30:05 (03:19:30 to 11::07:12:10)\n",
            "Steps: 110. Time per step: 3.6e-02s. Reward per step: -1.08.\n",
            "It has been 7 episode(s) since the model was last saved, with a score of -88 (Δ-16.23).\n",
            "\n",
            "13:20:43 \n",
            "Episode: **12**/4000, Score: -112 (Δ  7.4)\n",
            "Average score: -104.8 (Δ-0.63)\n",
            "Episode time: 2.0s, average: 11.3s (±221.06), ETA: 12:33:29 (02:12:58 to 10::17:29:59)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -2.03.\n",
            "It has been 8 episode(s) since the model was last saved, with a score of -88 (Δ-16.86).\n",
            "\n",
            "13:20:48 \n",
            "Episode: **13**/4000, Score: -122 (Δ-10.7)\n",
            "Average score: -106.1 (Δ-1.35)\n",
            "Episode time: 5.0s, average: 10.8s (±206.90), ETA: 12:00:54 (02:12:56 to 10::01:12:49)\n",
            "Steps: 132. Time per step: 3.8e-02s. Reward per step: -0.93.\n",
            "It has been 9 episode(s) since the model was last saved, with a score of -88 (Δ-18.21).\n",
            "\n",
            "13:20:53 \n",
            "Episode: **14**/4000, Score: -115 (Δ  6.9)\n",
            "Average score: -106.8 (Δ-0.67)\n",
            "Episode time: 5.0s, average: 10.4s (±194.39), ETA: 11:32:59 (02:12:54 to 09::10:50:03)\n",
            "Steps: 124. Time per step: 4.0e-02s. Reward per step: -0.93.\n",
            "It has been 10 episode(s) since the model was last saved, with a score of -88 (Δ-18.88).\n",
            "\n",
            "13:20:57 \n",
            "Episode: **15**/4000, Score: -123 (Δ -7.7)\n",
            "Average score: -107.9 (Δ-1.09)\n",
            "Episode time: 4.0s, average: 10.0s (±184.00), ETA: 11:04:20 (02:12:52 to 08::22:48:04)\n",
            "Steps: 131. Time per step: 3.1e-02s. Reward per step: -0.94.\n",
            "It has been 11 episode(s) since the model was last saved, with a score of -88 (Δ-19.97).\n",
            "\n",
            "13:21:00 \n",
            "Episode: **16**/4000, Score: -116 (Δ  7.3)\n",
            "Average score: -108.4 (Δ-0.50)\n",
            "Episode time: 3.0s, average: 9.6s (±175.37), ETA: 10:35:07 (02:12:50 to 08::12:42:40)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.38.\n",
            "It has been 12 episode(s) since the model was last saved, with a score of -88 (Δ-20.47).\n",
            "\n",
            "13:21:03 \n",
            "Episode: **17**/4000, Score: -113 (Δ  2.4)\n",
            "Average score: -108.7 (Δ-0.30)\n",
            "Episode time: 3.0s, average: 9.2s (±167.44), ETA: 10:09:19 (02:12:48 to 08::03:27:18)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.60.\n",
            "It has been 13 episode(s) since the model was last saved, with a score of -88 (Δ-20.77).\n",
            "\n",
            "13:21:06 \n",
            "Episode: **18**/4000, Score: -111 (Δ  2.6)\n",
            "Average score: -108.8 (Δ-0.12)\n",
            "Episode time: 3.0s, average: 8.8s (±160.14), ETA: 09:46:23 (02:12:46 to 07::18:56:56)\n",
            "Steps: 88. Time per step: 3.4e-02s. Reward per step: -1.26.\n",
            "It has been 14 episode(s) since the model was last saved, with a score of -88 (Δ-20.90).\n",
            "\n",
            "13:21:11 \n",
            "Episode: **19**/4000, Score: -113 (Δ -1.7)\n",
            "Average score: -109.0 (Δ-0.20)\n",
            "Episode time: 5.0s, average: 8.6s (±152.44), ETA: 09:32:51 (02:12:44 to 07::10:10:00)\n",
            "Steps: 145. Time per step: 3.4e-02s. Reward per step: -0.78.\n",
            "It has been 15 episode(s) since the model was last saved, with a score of -88 (Δ-21.10).\n",
            "\n",
            "13:21:13 \n",
            "Episode: **20**/4000, Score: -116 (Δ -3.6)\n",
            "Average score: -109.4 (Δ-0.36)\n",
            "Episode time: 2.0s, average: 8.3s (±146.91), ETA: 09:10:42 (02:12:42 to 07::03:38:11)\n",
            "Steps: 36. Time per step: 5.6e-02s. Reward per step: -3.23.\n",
            "It has been 16 episode(s) since the model was last saved, with a score of -88 (Δ-21.45).\n",
            "\n",
            "13:21:14 \n",
            "Episode: **21**/4000, Score: -116 (Δ  0.0)\n",
            "Average score: -109.7 (Δ-0.32)\n",
            "Episode time: 1.0s, average: 8.0s (±142.33), ETA: 08:47:30 (01:06:20 to 06::22:08:48)\n",
            "Steps: 37. Time per step: 2.7e-02s. Reward per step: -3.14.\n",
            "It has been 17 episode(s) since the model was last saved, with a score of -88 (Δ-21.78).\n",
            "\n",
            "13:21:16 \n",
            "Episode: **22**/4000, Score: -119 (Δ -3.1)\n",
            "Average score: -110.1 (Δ-0.44)\n",
            "Episode time: 2.0s, average: 7.7s (±137.40), ETA: 08:29:26 (01:06:19 to 06::16:21:16)\n",
            "Steps: 40. Time per step: 5.0e-02s. Reward per step: -2.98.\n",
            "It has been 18 episode(s) since the model was last saved, with a score of -88 (Δ-22.21).\n",
            "\n",
            "13:21:21 \n",
            "Episode: **23**/4000, Score: -114 (Δ  5.3)\n",
            "Average score: -110.3 (Δ-0.17)\n",
            "Episode time: 5.0s, average: 7.6s (±131.72), ETA: 08:21:34 (01:06:18 to 06::09:54:53)\n",
            "Steps: 144. Time per step: 3.5e-02s. Reward per step: -0.79.\n",
            "It has been 19 episode(s) since the model was last saved, with a score of -88 (Δ-22.38).\n",
            "\n",
            "13:21:22 \n",
            "Episode: **24**/4000, Score: -114 (Δ  0.3)\n",
            "Average score: -110.4 (Δ-0.14)\n",
            "Episode time: 1.0s, average: 7.3s (±127.96), ETA: 08:03:19 (01:06:17 to 06::05:24:42)\n",
            "Steps: 33. Time per step: 3.0e-02s. Reward per step: -3.45.\n",
            "It has been 20 episode(s) since the model was last saved, with a score of -88 (Δ-22.53).\n",
            "\n",
            "13:21:23 \n",
            "Episode: **25**/4000, Score: -116 (Δ -2.0)\n",
            "Average score: -110.6 (Δ-0.21)\n",
            "Episode time: 1.0s, average: 7.0s (±124.36), ETA: 07:46:31 (01:06:16 to 06::01:07:20)\n",
            "Steps: 36. Time per step: 2.8e-02s. Reward per step: -3.21.\n",
            "It has been 21 episode(s) since the model was last saved, with a score of -88 (Δ-22.74).\n",
            "\n",
            "13:21:27 \n",
            "Episode: **26**/4000, Score: -112 (Δ  4.1)\n",
            "Average score: -110.7 (Δ-0.04)\n",
            "Episode time: 4.0s, average: 6.9s (±119.92), ETA: 07:38:39 (01:06:15 to 05::20:03:10)\n",
            "Steps: 101. Time per step: 4.0e-02s. Reward per step: -1.11.\n",
            "It has been 22 episode(s) since the model was last saved, with a score of -88 (Δ-22.77).\n",
            "\n",
            "13:21:32 \n",
            "Episode: **27**/4000, Score: -124 (Δ-12.5)\n",
            "Average score: -111.2 (Δ-0.50)\n",
            "Episode time: 5.0s, average: 6.9s (±115.61), ETA: 07:33:49 (01:06:14 to 05::15:10:54)\n",
            "Steps: 127. Time per step: 3.9e-02s. Reward per step: -0.98.\n",
            "It has been 23 episode(s) since the model was last saved, with a score of -88 (Δ-23.27).\n",
            "\n",
            "13:21:36 \n",
            "Episode: **28**/4000, Score: -112 (Δ 12.4)\n",
            "Average score: -111.2 (Δ-0.02)\n",
            "Episode time: 4.0s, average: 6.8s (±111.76), ETA: 07:26:58 (01:06:13 to 05::10:47:16)\n",
            "Steps: 114. Time per step: 3.5e-02s. Reward per step: -0.98.\n",
            "It has been 24 episode(s) since the model was last saved, with a score of -88 (Δ-23.29).\n",
            "\n",
            "13:21:40 \n",
            "Episode: **29**/4000, Score: -111 (Δ  1.0)\n",
            "Average score: -111.2 (Δ 0.02)\n",
            "Episode time: 4.0s, average: 6.7s (±108.16), ETA: 07:20:34 (01:06:12 to 05::06:40:34)\n",
            "Steps: 113. Time per step: 3.5e-02s. Reward per step: -0.98.\n",
            "It has been 25 episode(s) since the model was last saved, with a score of -88 (Δ-23.27).\n",
            "\n",
            "13:21:41 \n",
            "Episode: **30**/4000, Score: -117 (Δ -6.5)\n",
            "Average score: -111.4 (Δ-0.20)\n",
            "Episode time: 1.0s, average: 6.5s (±105.58), ETA: 07:07:59 (01:06:11 to 05::03:35:46)\n",
            "Steps: 37. Time per step: 2.7e-02s. Reward per step: -3.17.\n",
            "It has been 26 episode(s) since the model was last saved, with a score of -88 (Δ-23.47).\n",
            "\n",
            "13:21:44 \n",
            "Episode: **31**/4000, Score: -118 (Δ -1.3)\n",
            "Average score: -111.6 (Δ-0.23)\n",
            "Episode time: 3.0s, average: 6.4s (±102.55), ETA: 07:00:29 (01:06:10 to 05::00:05:58)\n",
            "Steps: 79. Time per step: 3.8e-02s. Reward per step: -1.50.\n",
            "It has been 27 episode(s) since the model was last saved, with a score of -88 (Δ-23.70).\n",
            "\n",
            "13:21:46 \n",
            "Episode: **32**/4000, Score: -112 (Δ  6.8)\n",
            "Average score: -111.6 (Δ-0.00)\n",
            "Episode time: 2.0s, average: 6.2s (±99.92), ETA: 06:51:22 (01:06:09 to 04::21:01:08)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.33.\n",
            "It has been 28 episode(s) since the model was last saved, with a score of -88 (Δ-23.70).\n",
            "\n",
            "13:21:51 \n",
            "Episode: **33**/4000, Score: -114 (Δ -2.0)\n",
            "Average score: -111.7 (Δ-0.06)\n",
            "Episode time: 5.0s, average: 6.2s (±96.94), ETA: 06:48:49 (01:06:08 to 04::17:39:34)\n",
            "Steps: 144. Time per step: 3.5e-02s. Reward per step: -0.79.\n",
            "It has been 29 episode(s) since the model was last saved, with a score of -88 (Δ-23.77).\n",
            "\n",
            "13:21:56 \n",
            "Episode: **34**/4000, Score: -125 (Δ-11.7)\n",
            "Average score: -112.1 (Δ-0.40)\n",
            "Episode time: 5.0s, average: 6.1s (±94.13), ETA: 06:46:25 (01:06:07 to 04::14:29:41)\n",
            "Steps: 131. Time per step: 3.8e-02s. Reward per step: -0.96.\n",
            "It has been 30 episode(s) since the model was last saved, with a score of -88 (Δ-24.17).\n",
            "\n",
            "13:22:00 \n",
            "Episode: **35**/4000, Score: -112 (Δ 13.9)\n",
            "Average score: -112.1 (Δ 0.02)\n",
            "Episode time: 4.0s, average: 6.1s (±91.56), ETA: 06:42:16 (01:06:06 to 04::11:34:39)\n",
            "Steps: 103. Time per step: 3.9e-02s. Reward per step: -1.08.\n",
            "It has been 31 episode(s) since the model was last saved, with a score of -88 (Δ-24.15).\n",
            "\n",
            "13:22:04 \n",
            "Episode: **36**/4000, Score: -124 (Δ-12.1)\n",
            "Average score: -112.4 (Δ-0.32)\n",
            "Episode time: 4.0s, average: 6.0s (±89.14), ETA: 06:38:20 (01:06:05 to 04::08:48:53)\n",
            "Steps: 117. Time per step: 3.4e-02s. Reward per step: -1.06.\n",
            "It has been 32 episode(s) since the model was last saved, with a score of -88 (Δ-24.48).\n",
            "\n",
            "13:22:07 \n",
            "Episode: **37**/4000, Score: -120 (Δ  4.0)\n",
            "Average score: -112.6 (Δ-0.20)\n",
            "Episode time: 3.0s, average: 5.9s (±86.97), ETA: 06:32:50 (01:06:04 to 04::06:18:39)\n",
            "Steps: 90. Time per step: 3.3e-02s. Reward per step: -1.33.\n",
            "It has been 33 episode(s) since the model was last saved, with a score of -88 (Δ-24.67).\n",
            "\n",
            "13:22:10 \n",
            "Episode: **38**/4000, Score: -118 (Δ  1.5)\n",
            "Average score: -112.7 (Δ-0.15)\n",
            "Episode time: 3.0s, average: 5.9s (±84.90), ETA: 06:27:37 (01:06:03 to 04::03:55:30)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.48.\n",
            "It has been 34 episode(s) since the model was last saved, with a score of -88 (Δ-24.82).\n",
            "\n",
            "13:22:13 \n",
            "Episode: **39**/4000, Score: -119 (Δ -0.8)\n",
            "Average score: -112.9 (Δ-0.16)\n",
            "Episode time: 3.0s, average: 5.8s (±82.93), ETA: 06:22:39 (01:06:02 to 04::01:38:57)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.42.\n",
            "It has been 35 episode(s) since the model was last saved, with a score of -88 (Δ-24.98).\n",
            "\n",
            "13:22:15 \n",
            "Episode: **40**/4000, Score: -120 (Δ -1.3)\n",
            "Average score: -113.1 (Δ-0.19)\n",
            "Episode time: 2.0s, average: 5.7s (±81.21), ETA: 06:16:18 (01:06:01 to 03::23:37:31)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.86.\n",
            "It has been 36 episode(s) since the model was last saved, with a score of -88 (Δ-25.16).\n",
            "\n",
            "13:22:16 \n",
            "Episode: **41**/4000, Score: -117 (Δ  3.8)\n",
            "Average score: -113.1 (Δ-0.08)\n",
            "Episode time: 1.0s, average: 5.6s (±79.75), ETA: 06:08:38 (01:06:00 to 03::21:52:27)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.77.\n",
            "It has been 37 episode(s) since the model was last saved, with a score of -88 (Δ-25.25).\n",
            "\n",
            "13:22:18 \n",
            "Episode: **42**/4000, Score: -121 (Δ -4.9)\n",
            "Average score: -113.3 (Δ-0.20)\n",
            "Episode time: 2.0s, average: 5.5s (±78.15), ETA: 06:02:54 (01:05:59 to 03::19:59:49)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.82.\n",
            "It has been 38 episode(s) since the model was last saved, with a score of -88 (Δ-25.45).\n",
            "\n",
            "13:22:19 \n",
            "Episode: **43**/4000, Score: -119 (Δ  2.9)\n",
            "Average score: -113.5 (Δ-0.12)\n",
            "Episode time: 1.0s, average: 5.4s (±76.80), ETA: 05:55:55 (01:05:58 to 03::18:21:58)\n",
            "Steps: 41. Time per step: 2.4e-02s. Reward per step: -2.89.\n",
            "It has been 39 episode(s) since the model was last saved, with a score of -88 (Δ-25.57).\n",
            "\n",
            "13:22:25 \n",
            "Episode: **44**/4000, Score: -116 (Δ  2.6)\n",
            "Average score: -113.5 (Δ-0.06)\n",
            "Episode time: 6.0s, average: 5.4s (±75.06), ETA: 05:56:44 (01:05:57 to 03::16:26:56)\n",
            "Steps: 149. Time per step: 4.0e-02s. Reward per step: -0.78.\n",
            "It has been 40 episode(s) since the model was last saved, with a score of -88 (Δ-25.62).\n",
            "\n",
            "13:22:28 \n",
            "Episode: **45**/4000, Score: -119 (Δ -2.9)\n",
            "Average score: -113.6 (Δ-0.12)\n",
            "Episode time: 3.0s, average: 5.4s (±73.52), ETA: 05:53:07 (01:05:56 to 03::14:40:24)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.43.\n",
            "It has been 41 episode(s) since the model was last saved, with a score of -88 (Δ-25.74).\n",
            "\n",
            "13:22:31 \n",
            "Episode: **46**/4000, Score: -120 (Δ -0.9)\n",
            "Average score: -113.8 (Δ-0.13)\n",
            "Episode time: 3.0s, average: 5.3s (±72.04), ETA: 05:49:39 (01:05:55 to 03::12:58:08)\n",
            "Steps: 92. Time per step: 3.3e-02s. Reward per step: -1.30.\n",
            "It has been 42 episode(s) since the model was last saved, with a score of -88 (Δ-25.87).\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}