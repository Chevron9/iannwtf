{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddpg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9X8IrGs+zVzJwkYhd4pZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chevron9/iannwtf/blob/master/ddpg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsSbPZt2wKxh",
        "outputId": "1aa6f76c-03c8-46b3-ce7f-3a7db8ac3994"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "import gym\n",
        "env = gym.make(\"BipedalWalker-v3\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX_s3Q4LwNWN",
        "outputId": "c58814df-61e3-444c-e665-8770ea50d413"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os \n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYoGfSs2zgJz"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plots the scores of the course of training\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgD-CyT4zqCn"
      },
      "source": [
        "def timespan_format(timespan):\n",
        "    timespan = round(timespan)\n",
        "    h_full = timespan / (60*60)\n",
        "    h = timespan // (60*60)\n",
        "    m = (timespan % (60*60)) // 60\n",
        "    s = timespan % 60\n",
        "    time = f\"{h:02}:{m:02}:{s:02}\"\n",
        "    if h_full > 24:\n",
        "        d = timespan // (60*60*24)\n",
        "        h = (timespan % (60*60*24)) // (60*60)\n",
        "        time = f\"{d:02}::{h:02}:{m:02}:{s:02}\"\n",
        "    return time\n",
        "\n",
        "timespan_format(303601)\n",
        "test = False\n",
        "if test:\n",
        "    assert timespan_format(60) == \"00:01:00\"\n",
        "    assert timespan_format(60*60) == \"01:00:00\"\n",
        "    assert timespan_format(60*60+1) == \"01:00:01\"\n",
        "    assert timespan_format(60*60+60) == \"01:01:00\"\n",
        "    assert timespan_format(24*60*60+1) == \"01::00:00:01\"\n",
        "    assert timespan_format(303601) == \"03::12:20:01\" # 3 days 12 hours 20 minutes 1 second\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVmB-P2_wTm4"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "#all the networks of the model\n",
        "\n",
        "\n",
        "#critic network that takes the state and the action and puts out the value of the\n",
        "#the action in the given state\n",
        "class CriticNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512,\n",
        "            name='critic', chkpt_dir='tmp/'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        #Dimensions of the dense layers\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "\n",
        "\n",
        "        #says where the weights are saved\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_ddpg.h5')\n",
        "\n",
        "\n",
        "        #Optimization: using LeakyReLU as per https://arxiv.org/pdf/1709.06560.pdf\n",
        "\n",
        "        #dense layers with kernel and bias initializer(from an other implementation)\n",
        "        #and relu activation\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "        #denselayer with 1 neuron that gives the estimated q value of the\n",
        "        #state-action pair\n",
        "        f3 = 0.003\n",
        "        self.q = Dense(1, activation=None, kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3) ,\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f3, f3),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, action, training = True):\n",
        "        #feeds the network state and action pairs\n",
        "        action_value = self.dense_layer1(tf.concat([state, action], axis=1))\n",
        "\n",
        "        action_value = self.dense_layer2(action_value)\n",
        "\n",
        "        q = self.q(action_value)\n",
        "\n",
        "        #gives back an estimation of a q value\n",
        "        return q\n",
        "\n",
        "\n",
        "#critic network that takes the state and outputs a probability\n",
        "#distribution over all possible actions\n",
        "class ActorNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512, n_actions=4, name='actor',\n",
        "            chkpt_dir='tmp/'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_ddpg.h5')\n",
        "\n",
        "        #first dense layer\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        #second dense layer\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "\n",
        "        #output layer with tanh activation to get an output vector of length actionspace\n",
        "        #with values between -1 and 1 to fit to the action boundaries\n",
        "        f3 = 0.003\n",
        "        self.mu = Dense(self.n_actions, activation='tanh', kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3) , bias_initializer\n",
        "         = tf.keras.initializers.RandomUniform(-f3, f3))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, training = True):\n",
        "\n",
        "        actions = self.dense_layer1(state)\n",
        "        actions = self.dense_layer2(actions)\n",
        "\n",
        "        #gives back the actions the agent should take (deterministic policy)\n",
        "        actions = self.mu(actions)\n",
        "\n",
        "\n",
        "        return actions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIiiKOnIwWGs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replaybuffer that stores all informations about the agents transitions\n",
        "# in np arrays\n",
        "class ReplayBuffer:\n",
        "    #initializes the memories with zeros with sizes depending on a big maximal size,\n",
        "    #the input_dimensions(output of the environment) or the numbers of possible\n",
        "    #actions\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.max_size = max_size\n",
        "        self.current_position = 0\n",
        "        self.state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.new_state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.action_memory = np.zeros((self.max_size, n_actions))\n",
        "        self.reward_memory = np.zeros(self.max_size)\n",
        "        self.terminal_memory = np.zeros(self.max_size, dtype=np.bool)\n",
        "\n",
        "    #stores new transitions in memory\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.current_position % self.max_size\n",
        "\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.current_position+= 1\n",
        "\n",
        "    #gives back a random batch of of transition samples\n",
        "    def sample_buffer(self, batch_size):\n",
        "        #makes sure to have the correct current size of the memory\n",
        "        max_mem = min(self.current_position, self.max_size)\n",
        "\n",
        "        #selects a random batch of indexes in the memory size\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        #retrieves the batch from memory\n",
        "        states = self.state_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, dones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yWD6XYVwZNH"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# the agent class where the all the important parameters and systems of the\n",
        "# model are managed\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, input_dims, alpha=0.001, beta=0.002, env=None,\n",
        "            gamma=0.99, n_actions=4, max_size=1000000, tau=0.001,\n",
        "            dense1=512, dense2=512, batch_size=64, noise=0.3, module_dir = \"\"):\n",
        "\n",
        "        #initializing network-parameters\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.n_actions = n_actions\n",
        "        self.noise = noise\n",
        "\n",
        "        #retrieves the maximum and minimum of the actionvalues\n",
        "        self.max_action = env.action_space.high[0]\n",
        "        self.min_action = env.action_space.low[0]\n",
        "\n",
        "        #initializes the Replaybuffer which stores what the agents does\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "\n",
        "        # initializing the Networks with given parameters\n",
        "        # target_actor and target_critic are just initialized as the actor and\n",
        "        # critic networks\n",
        "\n",
        "        chkpt_dir = module_dir+\"/tmp\"\n",
        "        self.actor = ActorNetwork(n_actions=n_actions, name='actor', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_actor = ActorNetwork(n_actions=n_actions, name='target_actor',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "        self.critic = CriticNetwork(name='critic',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_critic = CriticNetwork(name='target_critic', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "\n",
        "        #compile the networks with learningrates\n",
        "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "\n",
        "        self.critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "        self.target_critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "\n",
        "        self.update_network_parameters(tau=1) #Hard copy, since this is the initialization\n",
        "\n",
        "    #updates the target networks\n",
        "    #soft copies the target and actor network dependent on tau\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_actor.weights\n",
        "        for i, weight in enumerate(self.actor.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_actor.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic.weights\n",
        "        for i, weight in enumerate(self.critic.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_critic.set_weights(weights)\n",
        "\n",
        "    #stores the state, action, reward transition\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    #saves models in files\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.save_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.save_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    #loads models from files\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.load_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    #choose_action with help of the actor network, adds noise if it for training\n",
        "    def choose_action(self, observation, evaluate=False):\n",
        "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "        actions = self.actor(state)\n",
        "\n",
        "        # inject eploration noise\n",
        "        if not evaluate:\n",
        "\n",
        "            actions += tf.random.normal(shape=[self.n_actions],\n",
        "                    mean=0.0, stddev=self.noise)\n",
        "\n",
        "        #makes sure that action boundaries are met\n",
        "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
        "\n",
        "        return actions[0]\n",
        "\n",
        "    #learn function of the networks\n",
        "    def learn(self):\n",
        "        #starts to learn when there are enough samples to fill a batch\n",
        "        if self.memory.current_position < self.batch_size:\n",
        "            return\n",
        "\n",
        "        #gets batch form memory\n",
        "        state, action, reward, new_state, done = \\\n",
        "                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        #convert np arrays to tensors to feed them to the networks\n",
        "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
        "\n",
        "        #update critic network\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            #target actor decides which action to take\n",
        "            target_actions = self.target_actor(states_)\n",
        "            #target critic evaluates the value of the actions in the given states\n",
        "            critic_value_ = tf.squeeze(self.target_critic(\n",
        "                                states_, target_actions), 1)\n",
        "\n",
        "            #critic network evaluate the actual states and actions the model took\n",
        "            critic_value = tf.squeeze(self.critic(states, actions), 1)\n",
        "\n",
        "            #target says what value of the action in a certain state should\n",
        "            #be like\n",
        "            target = rewards + self.gamma*critic_value_*(1-done)\n",
        "\n",
        "            #takes the MSE of the target and the actual critic value as the loss\n",
        "            critic_loss = keras.losses.MSE(target, critic_value)\n",
        "\n",
        "\n",
        "        #gets the gradients of the loss in respect to the parameters of the network\n",
        "        critic_network_gradient = tape.gradient(critic_loss,\n",
        "                                            self.critic.trainable_variables)\n",
        "\n",
        "        #aplies the gradients to the critic network\n",
        "        self.critic.optimizer.apply_gradients(zip(\n",
        "            critic_network_gradient, self.critic.trainable_variables))\n",
        "\n",
        "        #update the actor network\n",
        "        with tf.GradientTape() as tape:\n",
        "            #gets the policy of the actor in a state\n",
        "            action_policy = self.actor(states)\n",
        "\n",
        "            #loss of the actor is the negative value of the critic because we\n",
        "            #want to maximize the value but gradient decent minimizes\n",
        "            actor_loss = -self.critic(states, action_policy)\n",
        "\n",
        "            #the loss is a average of all the losses\n",
        "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
        "\n",
        "        #gradients of the loss in respect to the parameters of the actor network\n",
        "        actor_network_gradient = tape.gradient(actor_loss,\n",
        "                                    self.actor.trainable_variables)\n",
        "\n",
        "\n",
        "        #optimizing the network gradients\n",
        "        self.actor.optimizer.apply_gradients(zip(\n",
        "            actor_network_gradient, self.actor.trainable_variables))\n",
        "\n",
        "        self.update_network_parameters()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppcQq5rdHQfg"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbFPh3tnwdlA",
        "outputId": "ad78ff64-730f-444a-e2ea-f04813bcd991"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os\n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import gym.envs.box2d\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "# modules\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    #for the case you just want to load a previous model\n",
        "    load_checkpoint = False\n",
        "\n",
        "    #Housekeeping variables\n",
        "    last_score = 0\n",
        "    last_avg_score = 0\n",
        "    last_save = 0\n",
        "    avg_delta = []\n",
        "    avg_steps = []\n",
        "\n",
        "    t = t_start = time.localtime()\n",
        "    current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t)\n",
        "    print(f\"\\n----------------- Training started at {current_time}. -------------------\\ncheckpoint: {load_checkpoint}\")\n",
        "\n",
        "    module_dir = \"\"\n",
        "    figure_dir = module_dir+f'plots/'\n",
        "    figure_file = figure_dir+f'walker{current_time.replace(\":\",\"_\")}.png'\n",
        "\n",
        "    log_dir = module_dir+'logs/' + current_time.replace(\":\",\"_\")\n",
        "\n",
        "    #Tensorboard writer\n",
        "    writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "\n",
        "    #initialize the environment for the agent and initialize the agent\n",
        "\n",
        "    #tf.debugging.set_log_device_placement(True)\n",
        "    env = gym.make(\"BipedalWalker-v3\")\n",
        "    env._max_episode_steps = 1200\n",
        "    #env = gym.make('BipedalWalkerHardcore-v3')\n",
        "\n",
        "\n",
        "    n_actions = env.action_space.shape[0]\n",
        "\n",
        "    noise = 0.3\n",
        "    # NEW batch 128\n",
        "    agent = Agent(alpha=0.00005, beta=0.0005, input_dims=env.observation_space.shape, tau=0.001, env=env,\n",
        "                  batch_size=64, dense1=400, dense2=300, n_actions=n_actions, noise = noise, module_dir = module_dir)\n",
        "\n",
        "\n",
        "    episodes = 4000\n",
        "\n",
        "\n",
        "\n",
        "    #set bestscore to minimum\n",
        "    best_score = env.reward_range[0]\n",
        "    score_history = []\n",
        "\n",
        "\n",
        "    #initializes the model with one random sample batch if model are loaded\n",
        "    #you can't load an empty model for some reason\n",
        "    #these are all dummy variables etc until load_models overwrites them\n",
        "    if load_checkpoint:\n",
        "        n_steps = 0\n",
        "        while n_steps <= agent.batch_size:\n",
        "            observation = env.reset()\n",
        "            action = env.action_space.sample()\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            agent.remember(observation, action, reward, observation_, done)\n",
        "            n_steps += 1\n",
        "        agent.learn()\n",
        "        agent.load_models()\n",
        "\n",
        "    # ---------------------------------------\n",
        "    # main learning loop\n",
        "    # ---------------------------------------\n",
        "    try:\n",
        "        for i in range(episodes):\n",
        "            if i == 3:\n",
        "                tf.profiler.experimental.server.start(6009)\n",
        "                print(\"profiler started\")\n",
        "                %load_ext tensorboard\n",
        "                %tensorboard --logdir logs\n",
        "                # launch tensorboard with \"tensorboard --logdir logs\"\n",
        "                # capture profile\n",
        "            #if i == 13:\n",
        "            #    tf.profiler.experimental.stop\n",
        "            #    print(\"profiler stopped\")\n",
        "\n",
        "\n",
        "            current_episode = i\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "\n",
        "            #regulates the noise over the course of training as exponential\n",
        "            #decay to get smaller noise at the end, the noise is the\n",
        "            #standarddeviation of a normal distribution\n",
        "            #(numbers from trial and error)\n",
        "            agent.noise = noise * np.exp(-i/1500)\n",
        "\n",
        "            #while the environment is running the model chooses actions, saves states,\n",
        "            #rewards, actions and observations in the buffer and trains the networks\n",
        "            #on them\n",
        "            steps = 0\n",
        "            while not done:\n",
        "                action = agent.choose_action(observation)\n",
        "\n",
        "                observation_, reward, done, info = env.step(action)\n",
        "                steps += 1\n",
        "                score += reward\n",
        "                agent.remember(observation, action, reward, observation_, done)\n",
        "                agent.learn()\n",
        "\n",
        "                #saves previous observation\n",
        "                observation = observation_\n",
        "\n",
        "            score_history.append(score)\n",
        "            avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "            #saves the model if the average score is better than the best previous\n",
        "            if avg_score > best_score:\n",
        "                best_score = avg_score\n",
        "                agent.save_models()\n",
        "                last_save = current_episode\n",
        "\n",
        "            #calculating and giving some info on training progress\n",
        "            t_new = time.localtime()\n",
        "            current_time = time.strftime(\"%H:%M:%S\", t_new)\n",
        "            t_delta = time.mktime(t_new)-time.mktime(t)\n",
        "            t = t_new\n",
        "            avg_delta.append(t_delta)\n",
        "            avg_delta_mean = np.mean(avg_delta)\n",
        "            avg_delta_std = np.var(avg_delta)\n",
        "\n",
        "            ETA_avg = (episodes-i)*avg_delta_mean\n",
        "            ETA_min = (episodes-i)*max((avg_delta_mean-avg_delta_std),min(avg_delta))\n",
        "            ETA_max = (episodes-i)*(avg_delta_mean+avg_delta_std)\n",
        "\n",
        "            avg_steps.append(steps)\n",
        "            per_step = t_delta/steps\n",
        "            steps_per_score = score/steps\n",
        "\n",
        "            print(f\"{current_time} \\n\"\n",
        "            f'Episode: **{i+1}**/{episodes}, Score: {score:.0f} (Δ{score-last_score:5.1f})\\n'\n",
        "            f'Average score: {avg_score:.1f} (Δ{avg_score-last_avg_score:5.2f})\\n'\n",
        "            f'Episode time: {t_delta:.1f}s, average: {avg_delta_mean:.1f}s (±{avg_delta_std:4.2f}),',\n",
        "            f'ETA: {timespan_format(ETA_avg)} ({timespan_format(ETA_min)} to {timespan_format(ETA_max)})\\n'\n",
        "            f'Steps: {steps}. Time per step: {per_step:.1e}s. Reward per step: {steps_per_score:.2f}.\\n'\n",
        "            f'It has been {i - last_save} episode(s) since the model was last saved, with a score of {best_score:.0f} (Δ{avg_score-best_score:2.2f}).\\n')\n",
        "\n",
        "            last_score = score\n",
        "            last_avg_score = avg_score\n",
        "\n",
        "\n",
        "            with writer.as_default():\n",
        "                tf.summary.scalar('Average Score', avg_score, step=i)\n",
        "                tf.summary.scalar('ETA', ETA_avg, step=i)\n",
        "                tf.summary.scalar('Calculation time per step', per_step, step=i)\n",
        "                tf.summary.scalar('Calculation time per episode', t_delta, step=i)\n",
        "                tf.summary.scalar('Steps', steps, step=i)\n",
        "                if ((i+1) % 50) == 0: #writer.flush and learning plot has a large performance impact, so only do it every 50 episodes\n",
        "                    writer.flush()\n",
        "                    x = [j+1 for j in range(current_episode+1)]\n",
        "                    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        episodes = current_episode\n",
        "        print(\"Manually shutting down training.\")\n",
        "\n",
        "    #plots the whole score history\n",
        "    x = [i+1 for i in range(episodes)]\n",
        "    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "t2 = time.localtime()\n",
        "current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t2)\n",
        "t_delta = time.mktime(t2)-time.mktime(t_start)\n",
        "print(f\"\\n----------------- Training ended at {current_time}. Duration was {t_delta/60:.2f} minutes.-----------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n",
            "\n",
            "----------------- Training started at 2021-04-01-11:42:38. -------------------\n",
            "checkpoint: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... saving models ...\n",
            "11:42:46 \n",
            "Episode: **1**/4000, Score: -108 (Δ-107.9)\n",
            "Average score: -107.9 (Δ-107.94)\n",
            "Episode time: 8.0s, average: 8.0s (±0.00), ETA: 08:53:20 (08:53:20 to 08:53:20)\n",
            "Steps: 250. Time per step: 3.2e-02s. Reward per step: -0.43.\n",
            "It has been 0 episode(s) since the model was last saved, with a score of -108 (Δ0.00).\n",
            "\n",
            "11:42:49 \n",
            "Episode: **2**/4000, Score: -110 (Δ -2.3)\n",
            "Average score: -109.1 (Δ-1.14)\n",
            "Episode time: 3.0s, average: 5.5s (±6.25), ETA: 06:06:34 (03:19:57 to 13:03:08)\n",
            "Steps: 75. Time per step: 4.0e-02s. Reward per step: -1.47.\n",
            "It has been 1 episode(s) since the model was last saved, with a score of -108 (Δ-1.14).\n",
            "\n",
            "11:42:52 \n",
            "Episode: **3**/4000, Score: -118 (Δ -7.4)\n",
            "Average score: -111.9 (Δ-2.84)\n",
            "Episode time: 3.0s, average: 4.7s (±5.56), ETA: 05:10:57 (03:19:54 to 11:21:08)\n",
            "Steps: 82. Time per step: 3.7e-02s. Reward per step: -1.43.\n",
            "It has been 2 episode(s) since the model was last saved, with a score of -108 (Δ-3.98).\n",
            "\n",
            "profiler started\n",
            "11:42:55 \n",
            "Episode: **4**/4000, Score: -116 (Δ  1.4)\n",
            "Average score: -113.0 (Δ-1.07)\n",
            "Episode time: 3.0s, average: 4.2s (±4.69), ETA: 04:43:07 (03:19:51 to 09:55:23)\n",
            "Steps: 82. Time per step: 3.7e-02s. Reward per step: -1.42.\n",
            "It has been 3 episode(s) since the model was last saved, with a score of -108 (Δ-5.04).\n",
            "\n",
            "11:42:57 \n",
            "Episode: **5**/4000, Score: -121 (Δ -5.2)\n",
            "Average score: -114.7 (Δ-1.68)\n",
            "Episode time: 2.0s, average: 3.8s (±4.56), ETA: 04:13:05 (02:13:12 to 09:16:47)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.64.\n",
            "It has been 4 episode(s) since the model was last saved, with a score of -108 (Δ-6.72).\n",
            "\n",
            "11:43:02 \n",
            "Episode: **6**/4000, Score: -103 (Δ 17.9)\n",
            "Average score: -112.8 (Δ 1.86)\n",
            "Episode time: 5.0s, average: 4.0s (±4.00), ETA: 04:26:20 (02:13:10 to 08:52:40)\n",
            "Steps: 126. Time per step: 4.0e-02s. Reward per step: -0.82.\n",
            "It has been 5 episode(s) since the model was last saved, with a score of -108 (Δ-4.85).\n",
            "\n",
            "11:43:03 \n",
            "Episode: **7**/4000, Score: -102 (Δ  1.5)\n",
            "Average score: -111.2 (Δ 1.55)\n",
            "Episode time: 1.0s, average: 3.6s (±4.53), ETA: 03:57:44 (01:06:34 to 08:59:20)\n",
            "Steps: 44. Time per step: 2.3e-02s. Reward per step: -2.32.\n",
            "It has been 6 episode(s) since the model was last saved, with a score of -108 (Δ-3.31).\n",
            "\n",
            "11:43:07 \n",
            "Episode: **8**/4000, Score: -111 (Δ -9.1)\n",
            "Average score: -111.2 (Δ 0.03)\n",
            "Episode time: 4.0s, average: 3.6s (±3.98), ETA: 04:01:15 (01:06:33 to 08:26:24)\n",
            "Steps: 98. Time per step: 4.1e-02s. Reward per step: -1.13.\n",
            "It has been 7 episode(s) since the model was last saved, with a score of -108 (Δ-3.28).\n",
            "\n",
            "11:43:09 \n",
            "Episode: **9**/4000, Score: -124 (Δ-12.5)\n",
            "Average score: -112.6 (Δ-1.37)\n",
            "Episode time: 2.0s, average: 3.4s (±3.80), ETA: 03:49:10 (01:06:32 to 08:02:10)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.87.\n",
            "It has been 8 episode(s) since the model was last saved, with a score of -108 (Δ-4.65).\n",
            "\n",
            "11:43:13 \n",
            "Episode: **10**/4000, Score: -113 (Δ 10.1)\n",
            "Average score: -112.7 (Δ-0.08)\n",
            "Episode time: 4.0s, average: 3.5s (±3.45), ETA: 03:52:48 (01:06:31 to 07:42:17)\n",
            "Steps: 117. Time per step: 3.4e-02s. Reward per step: -0.97.\n",
            "It has been 9 episode(s) since the model was last saved, with a score of -108 (Δ-4.73).\n",
            "\n",
            "11:43:15 \n",
            "Episode: **11**/4000, Score: -123 (Δ -9.6)\n",
            "Average score: -113.6 (Δ-0.94)\n",
            "Episode time: 2.0s, average: 3.4s (±3.32), ETA: 03:43:41 (01:06:30 to 07:24:37)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.93.\n",
            "It has been 10 episode(s) since the model was last saved, with a score of -108 (Δ-5.67).\n",
            "\n",
            "11:43:16 \n",
            "Episode: **12**/4000, Score: -122 (Δ  0.6)\n",
            "Average score: -114.3 (Δ-0.73)\n",
            "Episode time: 1.0s, average: 3.2s (±3.47), ETA: 03:30:32 (01:06:29 to 07:21:23)\n",
            "Steps: 43. Time per step: 2.3e-02s. Reward per step: -2.85.\n",
            "It has been 11 episode(s) since the model was last saved, with a score of -108 (Δ-6.40).\n",
            "\n",
            "11:43:18 \n",
            "Episode: **13**/4000, Score: -123 (Δ -0.2)\n",
            "Average score: -115.0 (Δ-0.63)\n",
            "Episode time: 2.0s, average: 3.1s (±3.30), ETA: 03:24:31 (01:06:28 to 07:03:58)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.92.\n",
            "It has been 12 episode(s) since the model was last saved, with a score of -108 (Δ-7.03).\n",
            "\n",
            "11:43:19 \n",
            "Episode: **14**/4000, Score: -120 (Δ  2.3)\n",
            "Average score: -115.4 (Δ-0.38)\n",
            "Episode time: 1.0s, average: 2.9s (±3.35), ETA: 03:14:36 (01:06:27 to 06:57:21)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.86.\n",
            "It has been 13 episode(s) since the model was last saved, with a score of -108 (Δ-7.41).\n",
            "\n",
            "11:43:21 \n",
            "Episode: **15**/4000, Score: -123 (Δ -2.4)\n",
            "Average score: -115.8 (Δ-0.49)\n",
            "Episode time: 2.0s, average: 2.9s (±3.18), ETA: 03:10:27 (01:06:26 to 06:41:51)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.85.\n",
            "It has been 14 episode(s) since the model was last saved, with a score of -108 (Δ-7.90).\n",
            "\n",
            "11:43:22 \n",
            "Episode: **16**/4000, Score: -124 (Δ -0.9)\n",
            "Average score: -116.3 (Δ-0.48)\n",
            "Episode time: 1.0s, average: 2.8s (±3.19), ETA: 03:02:39 (01:06:25 to 06:34:21)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.94.\n",
            "It has been 15 episode(s) since the model was last saved, with a score of -108 (Δ-8.38).\n",
            "\n",
            "11:43:24 \n",
            "Episode: **17**/4000, Score: -122 (Δ  1.2)\n",
            "Average score: -116.7 (Δ-0.35)\n",
            "Episode time: 2.0s, average: 2.7s (±3.03), ETA: 02:59:40 (01:06:24 to 06:20:56)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.91.\n",
            "It has been 16 episode(s) since the model was last saved, with a score of -108 (Δ-8.74).\n",
            "\n",
            "11:43:26 \n",
            "Episode: **18**/4000, Score: -123 (Δ -0.3)\n",
            "Average score: -117.0 (Δ-0.33)\n",
            "Episode time: 2.0s, average: 2.7s (±2.89), ETA: 02:57:01 (01:06:23 to 06:08:48)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.92.\n",
            "It has been 17 episode(s) since the model was last saved, with a score of -108 (Δ-9.07).\n",
            "\n",
            "11:43:27 \n",
            "Episode: **19**/4000, Score: -123 (Δ  0.0)\n",
            "Average score: -117.3 (Δ-0.30)\n",
            "Episode time: 1.0s, average: 2.6s (±2.88), ETA: 02:51:09 (01:06:22 to 06:01:59)\n",
            "Steps: 43. Time per step: 2.3e-02s. Reward per step: -2.85.\n",
            "It has been 18 episode(s) since the model was last saved, with a score of -108 (Δ-9.37).\n",
            "\n",
            "11:43:29 \n",
            "Episode: **20**/4000, Score: -122 (Δ  0.4)\n",
            "Average score: -117.6 (Δ-0.25)\n",
            "Episode time: 2.0s, average: 2.5s (±2.75), ETA: 02:49:12 (01:06:21 to 05:51:29)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.91.\n",
            "It has been 19 episode(s) since the model was last saved, with a score of -108 (Δ-9.62).\n",
            "\n",
            "11:43:30 \n",
            "Episode: **21**/4000, Score: -122 (Δ  0.1)\n",
            "Average score: -117.8 (Δ-0.22)\n",
            "Episode time: 1.0s, average: 2.5s (±2.73), ETA: 02:44:15 (01:06:20 to 05:45:03)\n",
            "Steps: 43. Time per step: 2.3e-02s. Reward per step: -2.84.\n",
            "It has been 20 episode(s) since the model was last saved, with a score of -108 (Δ-9.84).\n",
            "\n",
            "11:43:32 \n",
            "Episode: **22**/4000, Score: -122 (Δ  0.0)\n",
            "Average score: -118.0 (Δ-0.20)\n",
            "Episode time: 2.0s, average: 2.5s (±2.61), ETA: 02:42:47 (01:06:19 to 05:35:58)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.78.\n",
            "It has been 21 episode(s) since the model was last saved, with a score of -108 (Δ-10.03).\n",
            "\n",
            "11:43:34 \n",
            "Episode: **23**/4000, Score: -122 (Δ  0.2)\n",
            "Average score: -118.1 (Δ-0.17)\n",
            "Episode time: 2.0s, average: 2.4s (±2.51), ETA: 02:41:26 (01:06:18 to 05:27:37)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.90.\n",
            "It has been 22 episode(s) since the model was last saved, with a score of -108 (Δ-10.20).\n",
            "\n",
            "11:43:35 \n",
            "Episode: **24**/4000, Score: -123 (Δ -1.3)\n",
            "Average score: -118.4 (Δ-0.21)\n",
            "Episode time: 1.0s, average: 2.4s (±2.48), ETA: 02:37:25 (01:06:17 to 05:22:06)\n",
            "Steps: 41. Time per step: 2.4e-02s. Reward per step: -3.01.\n",
            "It has been 23 episode(s) since the model was last saved, with a score of -108 (Δ-10.42).\n",
            "\n",
            "11:43:37 \n",
            "Episode: **25**/4000, Score: -122 (Δ  1.0)\n",
            "Average score: -118.5 (Δ-0.15)\n",
            "Episode time: 2.0s, average: 2.4s (±2.39), ETA: 02:36:23 (01:06:16 to 05:14:48)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.91.\n",
            "It has been 24 episode(s) since the model was last saved, with a score of -108 (Δ-10.57).\n",
            "\n",
            "11:43:39 \n",
            "Episode: **26**/4000, Score: -121 (Δ  0.9)\n",
            "Average score: -118.6 (Δ-0.11)\n",
            "Episode time: 2.0s, average: 2.3s (±2.30), ETA: 02:35:26 (01:06:15 to 05:08:01)\n",
            "Steps: 45. Time per step: 4.4e-02s. Reward per step: -2.70.\n",
            "It has been 25 episode(s) since the model was last saved, with a score of -108 (Δ-10.68).\n",
            "\n",
            "11:43:48 \n",
            "Episode: **27**/4000, Score: -135 (Δ-13.3)\n",
            "Average score: -119.2 (Δ-0.59)\n",
            "Episode time: 9.0s, average: 2.6s (±3.80), ETA: 02:51:43 (01:06:14 to 07:03:12)\n",
            "Steps: 236. Time per step: 3.8e-02s. Reward per step: -0.57.\n",
            "It has been 26 episode(s) since the model was last saved, with a score of -108 (Δ-11.27).\n",
            "\n",
            "11:44:02 \n",
            "Episode: **28**/4000, Score: -140 (Δ -5.2)\n",
            "Average score: -119.9 (Δ-0.73)\n",
            "Episode time: 14.0s, average: 3.0s (±8.14), ETA: 03:18:39 (01:06:13 to 12:17:51)\n",
            "Steps: 374. Time per step: 3.7e-02s. Reward per step: -0.37.\n",
            "It has been 27 episode(s) since the model was last saved, with a score of -108 (Δ-12.01).\n",
            "\n",
            "11:44:04 \n",
            "Episode: **29**/4000, Score: -108 (Δ 32.0)\n",
            "Average score: -119.5 (Δ 0.42)\n",
            "Episode time: 2.0s, average: 3.0s (±7.90), ETA: 03:16:19 (01:06:12 to 11:58:59)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.86.\n",
            "It has been 28 episode(s) since the model was last saved, with a score of -108 (Δ-11.59).\n",
            "\n",
            "11:44:06 \n",
            "Episode: **30**/4000, Score: -107 (Δ  1.2)\n",
            "Average score: -119.1 (Δ 0.43)\n",
            "Episode time: 2.0s, average: 2.9s (±7.66), ETA: 03:14:08 (01:06:11 to 11:41:15)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.84.\n",
            "It has been 29 episode(s) since the model was last saved, with a score of -108 (Δ-11.16).\n",
            "\n",
            "11:44:48 \n",
            "Episode: **31**/4000, Score: -198 (Δ-91.2)\n",
            "Average score: -121.6 (Δ-2.54)\n",
            "Episode time: 42.0s, average: 4.2s (±55.06), ETA: 04:37:28 (01:06:10 to 02::17:20:34)\n",
            "Steps: 1126. Time per step: 3.7e-02s. Reward per step: -0.18.\n",
            "It has been 30 episode(s) since the model was last saved, with a score of -108 (Δ-13.69).\n",
            "\n",
            "11:44:50 \n",
            "Episode: **32**/4000, Score: -110 (Δ 87.6)\n",
            "Average score: -121.3 (Δ 0.36)\n",
            "Episode time: 2.0s, average: 4.1s (±53.48), ETA: 04:32:52 (01:06:09 to 02::15:30:52)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.39.\n",
            "It has been 31 episode(s) since the model was last saved, with a score of -108 (Δ-13.33).\n",
            "\n",
            "11:44:51 \n",
            "Episode: **33**/4000, Score: -109 (Δ  1.2)\n",
            "Average score: -120.9 (Δ 0.37)\n",
            "Episode time: 1.0s, average: 4.0s (±52.15), ETA: 04:26:32 (01:06:08 to 02::13:55:26)\n",
            "Steps: 47. Time per step: 2.1e-02s. Reward per step: -2.32.\n",
            "It has been 32 episode(s) since the model was last saved, with a score of -108 (Δ-12.96).\n",
            "\n",
            "11:44:54 \n",
            "Episode: **34**/4000, Score: -111 (Δ -1.9)\n",
            "Average score: -120.6 (Δ 0.29)\n",
            "Episode time: 3.0s, average: 4.0s (±50.65), ETA: 04:24:28 (01:06:07 to 02::12:13:05)\n",
            "Steps: 73. Time per step: 4.1e-02s. Reward per step: -1.52.\n",
            "It has been 33 episode(s) since the model was last saved, with a score of -108 (Δ-12.67).\n",
            "\n",
            "11:44:56 \n",
            "Episode: **35**/4000, Score: -107 (Δ  3.8)\n",
            "Average score: -120.2 (Δ 0.39)\n",
            "Episode time: 2.0s, average: 3.9s (±49.31), ETA: 04:20:37 (01:06:06 to 02::10:40:05)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -2.10.\n",
            "It has been 34 episode(s) since the model was last saved, with a score of -108 (Δ-12.28).\n",
            "\n",
            "11:44:58 \n",
            "Episode: **36**/4000, Score: -111 (Δ -3.9)\n",
            "Average score: -120.0 (Δ 0.26)\n",
            "Episode time: 2.0s, average: 3.9s (±48.04), ETA: 04:16:59 (01:06:05 to 02::09:11:51)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -2.13.\n",
            "It has been 35 episode(s) since the model was last saved, with a score of -108 (Δ-12.03).\n",
            "\n",
            "11:45:00 \n",
            "Episode: **37**/4000, Score: -114 (Δ -3.2)\n",
            "Average score: -119.8 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 3.8s (±46.84), ETA: 04:13:33 (01:06:04 to 02::07:48:01)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -2.08.\n",
            "It has been 36 episode(s) since the model was last saved, with a score of -108 (Δ-11.87).\n",
            "\n",
            "11:45:02 \n",
            "Episode: **38**/4000, Score: -109 (Δ  5.0)\n",
            "Average score: -119.5 (Δ 0.28)\n",
            "Episode time: 2.0s, average: 3.8s (±45.69), ETA: 04:10:18 (01:06:03 to 02::06:28:17)\n",
            "Steps: 61. Time per step: 3.3e-02s. Reward per step: -1.79.\n",
            "It has been 37 episode(s) since the model was last saved, with a score of -108 (Δ-11.59).\n",
            "\n",
            "11:45:05 \n",
            "Episode: **39**/4000, Score: -114 (Δ -5.3)\n",
            "Average score: -119.4 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 3.8s (±44.54), ETA: 04:08:54 (01:06:02 to 02::05:09:47)\n",
            "Steps: 57. Time per step: 5.3e-02s. Reward per step: -2.01.\n",
            "It has been 38 episode(s) since the model was last saved, with a score of -108 (Δ-11.46).\n",
            "\n",
            "11:45:06 \n",
            "Episode: **40**/4000, Score: -115 (Δ -0.4)\n",
            "Average score: -119.3 (Δ 0.11)\n",
            "Episode time: 1.0s, average: 3.7s (±43.61), ETA: 04:04:16 (01:06:01 to 02::04:03:15)\n",
            "Steps: 52. Time per step: 1.9e-02s. Reward per step: -2.21.\n",
            "It has been 39 episode(s) since the model was last saved, with a score of -108 (Δ-11.34).\n",
            "\n",
            "11:45:09 \n",
            "Episode: **41**/4000, Score: -107 (Δ  7.5)\n",
            "Average score: -119.0 (Δ 0.29)\n",
            "Episode time: 3.0s, average: 3.7s (±42.56), ETA: 04:03:04 (01:06:00 to 02::02:51:54)\n",
            "Steps: 63. Time per step: 4.8e-02s. Reward per step: -1.70.\n",
            "It has been 40 episode(s) since the model was last saved, with a score of -108 (Δ-11.05).\n",
            "\n",
            "11:46:09 \n",
            "Episode: **42**/4000, Score: -119 (Δ-12.1)\n",
            "Average score: -119.0 (Δ-0.01)\n",
            "Episode time: 60.0s, average: 5.0s (±115.26), ETA: 05:31:29 (01:05:59 to 05::12:16:49)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.07.\n",
            "It has been 41 episode(s) since the model was last saved, with a score of -108 (Δ-11.06).\n",
            "\n",
            "11:46:11 \n",
            "Episode: **43**/4000, Score: -101 (Δ 18.8)\n",
            "Average score: -118.6 (Δ 0.43)\n",
            "Episode time: 2.0s, average: 5.0s (±112.79), ETA: 05:26:46 (01:05:58 to 05::09:27:03)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.73.\n",
            "It has been 42 episode(s) since the model was last saved, with a score of -108 (Δ-10.63).\n",
            "\n",
            "11:46:13 \n",
            "Episode: **44**/4000, Score: -107 (Δ -6.2)\n",
            "Average score: -118.3 (Δ 0.27)\n",
            "Episode time: 2.0s, average: 4.9s (±110.42), ETA: 05:22:15 (01:05:57 to 05::06:44:23)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.18.\n",
            "It has been 43 episode(s) since the model was last saved, with a score of -108 (Δ-10.37).\n",
            "\n",
            "11:46:15 \n",
            "Episode: **45**/4000, Score: -107 (Δ -0.3)\n",
            "Average score: -118.1 (Δ 0.25)\n",
            "Episode time: 2.0s, average: 4.8s (±108.15), ETA: 05:17:57 (01:05:56 to 05::04:08:23)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.19.\n",
            "It has been 44 episode(s) since the model was last saved, with a score of -108 (Δ-10.12).\n",
            "\n",
            "11:46:16 \n",
            "Episode: **46**/4000, Score: -108 (Δ -0.4)\n",
            "Average score: -117.8 (Δ 0.23)\n",
            "Episode time: 1.0s, average: 4.7s (±106.11), ETA: 05:12:23 (01:05:55 to 05::01:46:32)\n",
            "Steps: 49. Time per step: 2.0e-02s. Reward per step: -2.20.\n",
            "It has been 45 episode(s) since the model was last saved, with a score of -108 (Δ-9.89).\n",
            "\n",
            "11:46:20 \n",
            "Episode: **47**/4000, Score: -101 (Δ  6.1)\n",
            "Average score: -117.5 (Δ 0.35)\n",
            "Episode time: 4.0s, average: 4.7s (±103.86), ETA: 05:11:16 (01:05:54 to 04::23:15:37)\n",
            "Steps: 90. Time per step: 4.4e-02s. Reward per step: -1.13.\n",
            "It has been 46 episode(s) since the model was last saved, with a score of -108 (Δ-9.54).\n",
            "\n",
            "11:46:22 \n",
            "Episode: **48**/4000, Score: -107 (Δ -5.8)\n",
            "Average score: -117.3 (Δ 0.21)\n",
            "Episode time: 2.0s, average: 4.7s (±101.85), ETA: 05:07:27 (01:05:53 to 04::20:57:29)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -2.10.\n",
            "It has been 47 episode(s) since the model was last saved, with a score of -108 (Δ-9.33).\n",
            "\n",
            "11:46:26 \n",
            "Episode: **49**/4000, Score: -114 (Δ -6.5)\n",
            "Average score: -117.2 (Δ 0.07)\n",
            "Episode time: 4.0s, average: 4.7s (±99.78), ETA: 05:06:29 (01:05:52 to 04::18:38:30)\n",
            "Steps: 97. Time per step: 4.1e-02s. Reward per step: -1.17.\n",
            "It has been 48 episode(s) since the model was last saved, with a score of -108 (Δ-9.26).\n",
            "\n",
            "11:46:29 \n",
            "Episode: **50**/4000, Score: -108 (Δ  5.5)\n",
            "Average score: -117.0 (Δ 0.18)\n",
            "Episode time: 3.0s, average: 4.6s (±97.84), ETA: 05:04:14 (01:05:51 to 04::16:26:42)\n",
            "Steps: 89. Time per step: 3.4e-02s. Reward per step: -1.22.\n",
            "It has been 49 episode(s) since the model was last saved, with a score of -108 (Δ-9.08).\n",
            "\n",
            "11:47:29 \n",
            "Episode: **51**/4000, Score: -116 (Δ -7.2)\n",
            "Average score: -117.0 (Δ 0.03)\n",
            "Episode time: 60.0s, average: 5.7s (±154.87), ETA: 06:15:38 (01:05:50 to 07::08:11:32)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.07.\n",
            "It has been 50 episode(s) since the model was last saved, with a score of -108 (Δ-9.05).\n",
            "\n",
            "11:47:32 \n",
            "Episode: **52**/4000, Score: -110 (Δ  5.1)\n",
            "Average score: -116.9 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 5.7s (±152.03), ETA: 06:12:07 (01:05:49 to 07::04:58:29)\n",
            "Steps: 88. Time per step: 3.4e-02s. Reward per step: -1.25.\n",
            "It has been 51 episode(s) since the model was last saved, with a score of -108 (Δ-8.92).\n",
            "\n",
            "11:47:35 \n",
            "Episode: **53**/4000, Score: -114 (Δ -3.8)\n",
            "Average score: -116.8 (Δ 0.05)\n",
            "Episode time: 3.0s, average: 5.6s (±149.30), ETA: 06:08:44 (01:05:48 to 07::01:52:24)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.41.\n",
            "It has been 52 episode(s) since the model was last saved, with a score of -108 (Δ-8.87).\n",
            "\n",
            "11:47:37 \n",
            "Episode: **54**/4000, Score: -120 (Δ -5.6)\n",
            "Average score: -116.9 (Δ-0.05)\n",
            "Episode time: 2.0s, average: 5.5s (±146.77), ETA: 06:04:15 (01:05:47 to 06::22:59:05)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -2.35.\n",
            "It has been 53 episode(s) since the model was last saved, with a score of -108 (Δ-8.93).\n",
            "\n",
            "11:47:41 \n",
            "Episode: **55**/4000, Score: -110 (Δ  9.8)\n",
            "Average score: -116.7 (Δ 0.13)\n",
            "Episode time: 4.0s, average: 5.5s (±144.14), ETA: 06:02:19 (01:05:46 to 06::20:01:59)\n",
            "Steps: 99. Time per step: 4.0e-02s. Reward per step: -1.11.\n",
            "It has been 54 episode(s) since the model was last saved, with a score of -108 (Δ-8.80).\n",
            "\n",
            "11:47:44 \n",
            "Episode: **56**/4000, Score: -124 (Δ-13.7)\n",
            "Average score: -116.9 (Δ-0.12)\n",
            "Episode time: 3.0s, average: 5.5s (±141.68), ETA: 05:59:17 (01:05:45 to 06::17:14:34)\n",
            "Steps: 82. Time per step: 3.7e-02s. Reward per step: -1.51.\n",
            "It has been 55 episode(s) since the model was last saved, with a score of -108 (Δ-8.92).\n",
            "\n",
            "11:47:46 \n",
            "Episode: **57**/4000, Score: -122 (Δ  1.2)\n",
            "Average score: -117.0 (Δ-0.10)\n",
            "Episode time: 2.0s, average: 5.4s (±139.40), ETA: 05:55:11 (01:05:44 to 06::14:38:19)\n",
            "Steps: 66. Time per step: 3.0e-02s. Reward per step: -1.85.\n",
            "It has been 56 episode(s) since the model was last saved, with a score of -108 (Δ-9.02).\n",
            "\n",
            "11:47:50 \n",
            "Episode: **58**/4000, Score: -122 (Δ  0.1)\n",
            "Average score: -117.1 (Δ-0.09)\n",
            "Episode time: 4.0s, average: 5.4s (±137.03), ETA: 05:53:31 (01:05:43 to 06::11:58:34)\n",
            "Steps: 86. Time per step: 4.7e-02s. Reward per step: -1.42.\n",
            "It has been 57 episode(s) since the model was last saved, with a score of -108 (Δ-9.11).\n",
            "\n",
            "11:47:54 \n",
            "Episode: **59**/4000, Score: -110 (Δ 12.6)\n",
            "Average score: -116.9 (Δ 0.13)\n",
            "Episode time: 4.0s, average: 5.4s (±134.74), ETA: 05:51:53 (01:05:42 to 06::09:24:09)\n",
            "Steps: 111. Time per step: 3.6e-02s. Reward per step: -0.99.\n",
            "It has been 58 episode(s) since the model was last saved, with a score of -108 (Δ-8.99).\n",
            "\n",
            "11:47:59 \n",
            "Episode: **60**/4000, Score: -123 (Δ-13.1)\n",
            "Average score: -117.0 (Δ-0.10)\n",
            "Episode time: 5.0s, average: 5.3s (±132.49), ETA: 05:51:24 (01:05:41 to 06::06:54:04)\n",
            "Steps: 130. Time per step: 3.8e-02s. Reward per step: -0.94.\n",
            "It has been 59 episode(s) since the model was last saved, with a score of -108 (Δ-9.08).\n",
            "\n",
            "11:48:03 \n",
            "Episode: **61**/4000, Score: -125 (Δ -2.7)\n",
            "Average score: -117.2 (Δ-0.14)\n",
            "Episode time: 4.0s, average: 5.3s (±130.35), ETA: 05:49:52 (01:05:40 to 06::04:29:37)\n",
            "Steps: 129. Time per step: 3.1e-02s. Reward per step: -0.97.\n",
            "It has been 60 episode(s) since the model was last saved, with a score of -108 (Δ-9.22).\n",
            "\n",
            "11:48:07 \n",
            "Episode: **62**/4000, Score: -126 (Δ -0.4)\n",
            "Average score: -117.3 (Δ-0.14)\n",
            "Episode time: 4.0s, average: 5.3s (±128.28), ETA: 05:48:22 (01:05:39 to 06::02:09:45)\n",
            "Steps: 81. Time per step: 4.9e-02s. Reward per step: -1.55.\n",
            "It has been 61 episode(s) since the model was last saved, with a score of -108 (Δ-9.36).\n",
            "\n",
            "11:48:10 \n",
            "Episode: **63**/4000, Score: -112 (Δ 13.5)\n",
            "Average score: -117.2 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 5.3s (±126.32), ETA: 05:45:53 (01:05:38 to 05::23:56:57)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.41.\n",
            "It has been 62 episode(s) since the model was last saved, with a score of -108 (Δ-9.29).\n",
            "\n",
            "11:48:17 \n",
            "Episode: **64**/4000, Score: -129 (Δ-16.5)\n",
            "Average score: -117.4 (Δ-0.18)\n",
            "Episode time: 7.0s, average: 5.3s (±124.40), ETA: 05:47:34 (01:05:37 to 05::21:50:02)\n",
            "Steps: 189. Time per step: 3.7e-02s. Reward per step: -0.68.\n",
            "It has been 63 episode(s) since the model was last saved, with a score of -108 (Δ-9.47).\n",
            "\n",
            "11:48:27 \n",
            "Episode: **65**/4000, Score: -136 (Δ -7.0)\n",
            "Average score: -117.7 (Δ-0.29)\n",
            "Episode time: 10.0s, average: 5.4s (±122.82), ETA: 05:52:13 (01:05:36 to 05::20:09:03)\n",
            "Steps: 290. Time per step: 3.4e-02s. Reward per step: -0.47.\n",
            "It has been 64 episode(s) since the model was last saved, with a score of -108 (Δ-9.75).\n",
            "\n",
            "11:48:31 \n",
            "Episode: **66**/4000, Score: -130 (Δ  6.0)\n",
            "Average score: -117.9 (Δ-0.19)\n",
            "Episode time: 4.0s, average: 5.3s (±120.98), ETA: 05:50:46 (01:05:35 to 05::18:05:21)\n",
            "Steps: 90. Time per step: 4.4e-02s. Reward per step: -1.44.\n",
            "It has been 65 episode(s) since the model was last saved, with a score of -108 (Δ-9.94).\n",
            "\n",
            "11:48:35 \n",
            "Episode: **67**/4000, Score: -135 (Δ -4.6)\n",
            "Average score: -118.1 (Δ-0.25)\n",
            "Episode time: 4.0s, average: 5.3s (±119.21), ETA: 05:49:22 (01:05:34 to 05::16:05:17)\n",
            "Steps: 105. Time per step: 3.8e-02s. Reward per step: -1.28.\n",
            "It has been 66 episode(s) since the model was last saved, with a score of -108 (Δ-10.19).\n",
            "\n",
            "11:49:35 \n",
            "Episode: **68**/4000, Score: -110 (Δ 24.3)\n",
            "Average score: -118.0 (Δ 0.12)\n",
            "Episode time: 60.0s, average: 6.1s (±160.76), ETA: 06:41:59 (01:05:33 to 07::14:19:55)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.07.\n",
            "It has been 67 episode(s) since the model was last saved, with a score of -108 (Δ-10.07).\n",
            "\n",
            "11:50:35 \n",
            "Episode: **69**/4000, Score: -117 (Δ -6.5)\n",
            "Average score: -118.0 (Δ 0.02)\n",
            "Episode time: 60.0s, average: 6.9s (±199.88), ETA: 07:33:02 (01:05:32 to 09::09:51:36)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.07.\n",
            "It has been 68 episode(s) since the model was last saved, with a score of -108 (Δ-10.05).\n",
            "\n",
            "11:50:40 \n",
            "Episode: **70**/4000, Score: -132 (Δ-15.8)\n",
            "Average score: -118.2 (Δ-0.21)\n",
            "Episode time: 5.0s, average: 6.9s (±197.07), ETA: 07:31:08 (01:05:31 to 09::06:42:40)\n",
            "Steps: 126. Time per step: 4.0e-02s. Reward per step: -1.05.\n",
            "It has been 69 episode(s) since the model was last saved, with a score of -108 (Δ-10.26).\n",
            "\n",
            "11:51:40 \n",
            "Episode: **71**/4000, Score: -130 (Δ  2.3)\n",
            "Average score: -118.4 (Δ-0.17)\n",
            "Episode time: 60.0s, average: 7.6s (±233.47), ETA: 08:20:01 (01:05:30 to 10::23:12:24)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.08.\n",
            "It has been 70 episode(s) since the model was last saved, with a score of -108 (Δ-10.43).\n",
            "\n",
            "11:51:46 \n",
            "Episode: **72**/4000, Score: -132 (Δ -2.2)\n",
            "Average score: -118.6 (Δ-0.19)\n",
            "Episode time: 6.0s, average: 7.6s (±230.27), ETA: 08:18:24 (01:05:29 to 10::19:36:57)\n",
            "Steps: 153. Time per step: 3.9e-02s. Reward per step: -0.86.\n",
            "It has been 71 episode(s) since the model was last saved, with a score of -108 (Δ-10.62).\n",
            "\n",
            "11:51:50 \n",
            "Episode: **73**/4000, Score: -131 (Δ  1.3)\n",
            "Average score: -118.7 (Δ-0.17)\n",
            "Episode time: 4.0s, average: 7.6s (±227.29), ETA: 08:15:02 (01:05:28 to 10::16:14:47)\n",
            "Steps: 107. Time per step: 3.7e-02s. Reward per step: -1.22.\n",
            "It has been 72 episode(s) since the model was last saved, with a score of -108 (Δ-10.79).\n",
            "\n",
            "11:51:53 \n",
            "Episode: **74**/4000, Score: -129 (Δ  2.1)\n",
            "Average score: -118.9 (Δ-0.14)\n",
            "Episode time: 3.0s, average: 7.5s (±224.49), ETA: 08:10:52 (01:05:27 to 10::13:03:57)\n",
            "Steps: 89. Time per step: 3.4e-02s. Reward per step: -1.45.\n",
            "It has been 73 episode(s) since the model was last saved, with a score of -108 (Δ-10.93).\n",
            "\n",
            "11:51:57 \n",
            "Episode: **75**/4000, Score: -118 (Δ 11.0)\n",
            "Average score: -118.9 (Δ 0.01)\n",
            "Episode time: 4.0s, average: 7.5s (±221.66), ETA: 08:07:42 (01:05:26 to 10::09:51:43)\n",
            "Steps: 106. Time per step: 3.8e-02s. Reward per step: -1.11.\n",
            "It has been 74 episode(s) since the model was last saved, with a score of -108 (Δ-10.91).\n",
            "\n",
            "11:52:02 \n",
            "Episode: **76**/4000, Score: -139 (Δ-21.2)\n",
            "Average score: -119.1 (Δ-0.27)\n",
            "Episode time: 5.0s, average: 7.4s (±218.82), ETA: 08:05:28 (01:05:25 to 10::06:40:07)\n",
            "Steps: 117. Time per step: 4.3e-02s. Reward per step: -1.19.\n",
            "It has been 75 episode(s) since the model was last saved, with a score of -108 (Δ-11.18).\n",
            "\n",
            "11:52:06 \n",
            "Episode: **77**/4000, Score: -120 (Δ 18.7)\n",
            "Average score: -119.1 (Δ-0.02)\n",
            "Episode time: 4.0s, average: 7.4s (±216.13), ETA: 08:02:26 (01:05:24 to 10::03:37:23)\n",
            "Steps: 113. Time per step: 3.5e-02s. Reward per step: -1.07.\n",
            "It has been 76 episode(s) since the model was last saved, with a score of -108 (Δ-11.20).\n",
            "\n",
            "11:52:09 \n",
            "Episode: **78**/4000, Score: -117 (Δ  3.5)\n",
            "Average score: -119.1 (Δ 0.03)\n",
            "Episode time: 3.0s, average: 7.3s (±213.60), ETA: 07:58:38 (01:05:23 to 10::00:44:41)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.41.\n",
            "It has been 77 episode(s) since the model was last saved, with a score of -108 (Δ-11.17).\n",
            "\n",
            "11:52:12 \n",
            "Episode: **79**/4000, Score: -116 (Δ  0.7)\n",
            "Average score: -119.1 (Δ 0.04)\n",
            "Episode time: 3.0s, average: 7.3s (±211.13), ETA: 07:54:57 (01:05:22 to 09::21:55:56)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.40.\n",
            "It has been 78 episode(s) since the model was last saved, with a score of -108 (Δ-11.13).\n",
            "\n",
            "11:52:43 \n",
            "Episode: **80**/4000, Score: -191 (Δ-74.6)\n",
            "Average score: -120.0 (Δ-0.90)\n",
            "Episode time: 31.0s, average: 7.6s (±215.45), ETA: 08:14:13 (01:05:21 to 10::02:53:37)\n",
            "Steps: 833. Time per step: 3.7e-02s. Reward per step: -0.23.\n",
            "It has been 79 episode(s) since the model was last saved, with a score of -108 (Δ-12.03).\n",
            "\n",
            "11:53:00 \n",
            "Episode: **81**/4000, Score: -155 (Δ 36.3)\n",
            "Average score: -120.4 (Δ-0.43)\n",
            "Episode time: 17.0s, average: 7.7s (±213.87), ETA: 08:21:42 (01:05:20 to 10::01:14:41)\n",
            "Steps: 439. Time per step: 3.9e-02s. Reward per step: -0.35.\n",
            "It has been 80 episode(s) since the model was last saved, with a score of -108 (Δ-12.46).\n",
            "\n",
            "11:53:14 \n",
            "Episode: **82**/4000, Score: -161 (Δ -6.7)\n",
            "Average score: -120.9 (Δ-0.50)\n",
            "Episode time: 14.0s, average: 7.8s (±211.75), ETA: 08:26:36 (01:05:19 to 09::22:57:06)\n",
            "Steps: 377. Time per step: 3.7e-02s. Reward per step: -0.43.\n",
            "It has been 81 episode(s) since the model was last saved, with a score of -108 (Δ-12.96).\n",
            "\n",
            "11:53:22 \n",
            "Episode: **83**/4000, Score: -147 (Δ 13.8)\n",
            "Average score: -121.2 (Δ-0.32)\n",
            "Episode time: 8.0s, average: 7.8s (±209.19), ETA: 08:26:40 (01:05:18 to 09::20:07:06)\n",
            "Steps: 221. Time per step: 3.6e-02s. Reward per step: -0.67.\n",
            "It has been 82 episode(s) since the model was last saved, with a score of -108 (Δ-13.28).\n",
            "\n",
            "11:53:27 \n",
            "Episode: **84**/4000, Score: -130 (Δ 17.1)\n",
            "Average score: -121.3 (Δ-0.11)\n",
            "Episode time: 5.0s, average: 7.7s (±206.79), ETA: 08:24:23 (01:05:17 to 09::17:24:36)\n",
            "Steps: 123. Time per step: 4.1e-02s. Reward per step: -1.06.\n",
            "It has been 83 episode(s) since the model was last saved, with a score of -108 (Δ-13.38).\n",
            "\n",
            "11:53:32 \n",
            "Episode: **85**/4000, Score: -114 (Δ 15.9)\n",
            "Average score: -121.2 (Δ 0.08)\n",
            "Episode time: 5.0s, average: 7.7s (±204.45), ETA: 08:22:10 (01:05:16 to 09::14:45:47)\n",
            "Steps: 134. Time per step: 3.7e-02s. Reward per step: -0.85.\n",
            "It has been 84 episode(s) since the model was last saved, with a score of -108 (Δ-13.30).\n",
            "\n",
            "11:53:39 \n",
            "Episode: **86**/4000, Score: -143 (Δ-28.4)\n",
            "Average score: -121.5 (Δ-0.25)\n",
            "Episode time: 7.0s, average: 7.7s (±202.08), ETA: 08:21:31 (01:05:15 to 09::12:06:58)\n",
            "Steps: 189. Time per step: 3.7e-02s. Reward per step: -0.76.\n",
            "It has been 85 episode(s) since the model was last saved, with a score of -108 (Δ-13.55).\n",
            "\n",
            "11:53:44 \n",
            "Episode: **87**/4000, Score: -122 (Δ 21.0)\n",
            "Average score: -121.5 (Δ-0.00)\n",
            "Episode time: 5.0s, average: 7.7s (±199.84), ETA: 08:19:22 (01:05:14 to 09::09:35:17)\n",
            "Steps: 120. Time per step: 4.2e-02s. Reward per step: -1.02.\n",
            "It has been 86 episode(s) since the model was last saved, with a score of -108 (Δ-13.56).\n",
            "\n",
            "11:53:47 \n",
            "Episode: **88**/4000, Score: -120 (Δ  2.0)\n",
            "Average score: -121.5 (Δ 0.02)\n",
            "Episode time: 3.0s, average: 7.6s (±197.81), ETA: 08:15:48 (01:05:13 to 09::07:16:09)\n",
            "Steps: 78. Time per step: 3.8e-02s. Reward per step: -1.54.\n",
            "It has been 87 episode(s) since the model was last saved, with a score of -108 (Δ-13.54).\n",
            "\n",
            "11:53:50 \n",
            "Episode: **89**/4000, Score: -130 (Δ-10.2)\n",
            "Average score: -121.6 (Δ-0.10)\n",
            "Episode time: 3.0s, average: 7.6s (±195.82), ETA: 08:12:18 (01:05:12 to 09::04:59:48)\n",
            "Steps: 94. Time per step: 3.2e-02s. Reward per step: -1.38.\n",
            "It has been 88 episode(s) since the model was last saved, with a score of -108 (Δ-13.64).\n",
            "\n",
            "11:53:54 \n",
            "Episode: **90**/4000, Score: -133 (Δ -2.7)\n",
            "Average score: -121.7 (Δ-0.12)\n",
            "Episode time: 4.0s, average: 7.5s (±193.78), ETA: 08:09:36 (01:05:11 to 09::02:41:02)\n",
            "Steps: 111. Time per step: 3.6e-02s. Reward per step: -1.20.\n",
            "It has been 89 episode(s) since the model was last saved, with a score of -108 (Δ-13.76).\n",
            "\n",
            "11:53:57 \n",
            "Episode: **91**/4000, Score: -119 (Δ 14.1)\n",
            "Average score: -121.7 (Δ 0.03)\n",
            "Episode time: 3.0s, average: 7.5s (±191.87), ETA: 08:06:15 (01:05:10 to 09::00:30:05)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.67.\n",
            "It has been 90 episode(s) since the model was last saved, with a score of -108 (Δ-13.73).\n",
            "\n",
            "11:54:01 \n",
            "Episode: **92**/4000, Score: -134 (Δ-15.2)\n",
            "Average score: -121.8 (Δ-0.13)\n",
            "Episode time: 4.0s, average: 7.4s (±189.92), ETA: 08:03:40 (01:05:09 to 08::22:16:50)\n",
            "Steps: 108. Time per step: 3.7e-02s. Reward per step: -1.24.\n",
            "It has been 91 episode(s) since the model was last saved, with a score of -108 (Δ-13.86).\n",
            "\n",
            "11:54:10 \n",
            "Episode: **93**/4000, Score: -125 (Δ  8.8)\n",
            "Average score: -121.8 (Δ-0.03)\n",
            "Episode time: 9.0s, average: 7.4s (±187.90), ETA: 08:04:39 (01:05:08 to 08::20:03:22)\n",
            "Steps: 219. Time per step: 4.1e-02s. Reward per step: -0.57.\n",
            "It has been 92 episode(s) since the model was last saved, with a score of -108 (Δ-13.90).\n",
            "\n",
            "11:54:15 \n",
            "Episode: **94**/4000, Score: -107 (Δ 18.1)\n",
            "Average score: -121.7 (Δ 0.16)\n",
            "Episode time: 5.0s, average: 7.4s (±185.97), ETA: 08:02:50 (01:05:07 to 08::17:52:20)\n",
            "Steps: 146. Time per step: 3.4e-02s. Reward per step: -0.73.\n",
            "It has been 93 episode(s) since the model was last saved, with a score of -108 (Δ-13.74).\n",
            "\n",
            "11:54:22 \n",
            "Episode: **95**/4000, Score: -120 (Δ-12.8)\n",
            "Average score: -121.7 (Δ 0.02)\n",
            "Episode time: 7.0s, average: 7.4s (±184.01), ETA: 08:02:26 (01:05:06 to 08::15:41:30)\n",
            "Steps: 192. Time per step: 3.6e-02s. Reward per step: -0.62.\n",
            "It has been 94 episode(s) since the model was last saved, with a score of -108 (Δ-13.72).\n",
            "\n",
            "11:54:29 \n",
            "Episode: **96**/4000, Score: -135 (Δ-15.6)\n",
            "Average score: -121.8 (Δ-0.14)\n",
            "Episode time: 7.0s, average: 7.4s (±182.10), ETA: 08:02:01 (01:05:05 to 08::13:33:24)\n",
            "Steps: 189. Time per step: 3.7e-02s. Reward per step: -0.72.\n",
            "It has been 95 episode(s) since the model was last saved, with a score of -108 (Δ-13.86).\n",
            "\n",
            "11:54:32 \n",
            "Episode: **97**/4000, Score: -116 (Δ 19.6)\n",
            "Average score: -121.7 (Δ 0.06)\n",
            "Episode time: 3.0s, average: 7.4s (±180.42), ETA: 07:58:57 (01:05:04 to 08::11:38:01)\n",
            "Steps: 74. Time per step: 4.1e-02s. Reward per step: -1.56.\n",
            "It has been 96 episode(s) since the model was last saved, with a score of -108 (Δ-13.80).\n",
            "\n",
            "11:54:35 \n",
            "Episode: **98**/4000, Score: -112 (Δ  3.6)\n",
            "Average score: -121.6 (Δ 0.10)\n",
            "Episode time: 3.0s, average: 7.3s (±178.77), ETA: 07:55:56 (01:05:03 to 08::09:44:44)\n",
            "Steps: 78. Time per step: 3.8e-02s. Reward per step: -1.44.\n",
            "It has been 97 episode(s) since the model was last saved, with a score of -108 (Δ-13.70).\n",
            "\n",
            "11:54:37 \n",
            "Episode: **99**/4000, Score: -112 (Δ  0.2)\n",
            "Average score: -121.5 (Δ 0.10)\n",
            "Episode time: 2.0s, average: 7.3s (±177.24), ETA: 07:52:19 (01:05:02 to 08::07:59:05)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -2.04.\n",
            "It has been 98 episode(s) since the model was last saved, with a score of -108 (Δ-13.60).\n",
            "\n",
            "11:54:40 \n",
            "Episode: **100**/4000, Score: -109 (Δ  2.8)\n",
            "Average score: -121.4 (Δ 0.12)\n",
            "Episode time: 3.0s, average: 7.2s (±175.65), ETA: 07:49:25 (01:05:01 to 08::06:09:42)\n",
            "Steps: 64. Time per step: 4.7e-02s. Reward per step: -1.71.\n",
            "It has been 99 episode(s) since the model was last saved, with a score of -108 (Δ-13.48).\n",
            "\n",
            "11:54:43 \n",
            "Episode: **101**/4000, Score: -116 (Δ -6.4)\n",
            "Average score: -121.5 (Δ-0.08)\n",
            "Episode time: 3.0s, average: 7.2s (±174.09), ETA: 07:46:35 (01:05:00 to 08::04:22:15)\n",
            "Steps: 79. Time per step: 3.8e-02s. Reward per step: -1.46.\n",
            "It has been 100 episode(s) since the model was last saved, with a score of -108 (Δ-13.56).\n",
            "\n",
            "11:54:45 \n",
            "Episode: **102**/4000, Score: -115 (Δ  0.8)\n",
            "Average score: -121.5 (Δ-0.05)\n",
            "Episode time: 2.0s, average: 7.1s (±172.64), ETA: 07:43:10 (01:04:59 to 08::02:41:56)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.85.\n",
            "It has been 101 episode(s) since the model was last saved, with a score of -108 (Δ-13.60).\n",
            "\n",
            "11:54:48 \n",
            "Episode: **103**/4000, Score: -116 (Δ -1.1)\n",
            "Average score: -121.5 (Δ 0.02)\n",
            "Episode time: 3.0s, average: 7.1s (±171.13), ETA: 07:40:27 (01:04:58 to 08::00:58:05)\n",
            "Steps: 68. Time per step: 4.4e-02s. Reward per step: -1.70.\n",
            "It has been 102 episode(s) since the model was last saved, with a score of -108 (Δ-13.58).\n",
            "\n",
            "11:54:50 \n",
            "Episode: **104**/4000, Score: -117 (Δ -1.0)\n",
            "Average score: -121.5 (Δ-0.01)\n",
            "Episode time: 2.0s, average: 7.0s (±169.73), ETA: 07:37:09 (01:04:57 to 07::23:21:04)\n",
            "Steps: 74. Time per step: 2.7e-02s. Reward per step: -1.58.\n",
            "It has been 103 episode(s) since the model was last saved, with a score of -108 (Δ-13.59).\n",
            "\n",
            "11:54:54 \n",
            "Episode: **105**/4000, Score: -123 (Δ -6.3)\n",
            "Average score: -121.6 (Δ-0.02)\n",
            "Episode time: 4.0s, average: 7.0s (±168.20), ETA: 07:35:09 (01:04:56 to 07::21:36:56)\n",
            "Steps: 92. Time per step: 4.3e-02s. Reward per step: -1.34.\n",
            "It has been 104 episode(s) since the model was last saved, with a score of -108 (Δ-13.61).\n",
            "\n",
            "11:55:08 \n",
            "Episode: **106**/4000, Score: -151 (Δ-27.9)\n",
            "Average score: -122.0 (Δ-0.48)\n",
            "Episode time: 14.0s, average: 7.1s (±167.07), ETA: 07:39:19 (01:04:55 to 07::20:24:56)\n",
            "Steps: 371. Time per step: 3.8e-02s. Reward per step: -0.41.\n",
            "It has been 105 episode(s) since the model was last saved, with a score of -108 (Δ-14.09).\n",
            "\n",
            "11:55:11 \n",
            "Episode: **107**/4000, Score: -127 (Δ 24.3)\n",
            "Average score: -122.3 (Δ-0.25)\n",
            "Episode time: 3.0s, average: 7.0s (±165.66), ETA: 07:36:44 (01:04:54 to 07::18:48:12)\n",
            "Steps: 87. Time per step: 3.4e-02s. Reward per step: -1.46.\n",
            "It has been 106 episode(s) since the model was last saved, with a score of -108 (Δ-14.34).\n",
            "\n",
            "11:55:14 \n",
            "Episode: **108**/4000, Score: -123 (Δ  3.6)\n",
            "Average score: -122.4 (Δ-0.12)\n",
            "Episode time: 3.0s, average: 7.0s (±164.28), ETA: 07:34:11 (01:04:53 to 07::17:13:04)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.47.\n",
            "It has been 107 episode(s) since the model was last saved, with a score of -108 (Δ-14.46).\n",
            "\n",
            "11:55:19 \n",
            "Episode: **109**/4000, Score: -123 (Δ  0.1)\n",
            "Average score: -122.4 (Δ 0.00)\n",
            "Episode time: 5.0s, average: 7.0s (±162.81), ETA: 07:32:53 (01:04:52 to 07::15:33:37)\n",
            "Steps: 123. Time per step: 4.1e-02s. Reward per step: -1.00.\n",
            "It has been 108 episode(s) since the model was last saved, with a score of -108 (Δ-14.46).\n",
            "\n",
            "11:55:22 \n",
            "Episode: **110**/4000, Score: -119 (Δ  4.0)\n",
            "Average score: -122.5 (Δ-0.06)\n",
            "Episode time: 3.0s, average: 6.9s (±161.47), ETA: 07:30:25 (01:04:51 to 07::14:01:44)\n",
            "Steps: 97. Time per step: 3.1e-02s. Reward per step: -1.23.\n",
            "It has been 109 episode(s) since the model was last saved, with a score of -108 (Δ-14.52).\n",
            "\n",
            "11:55:28 \n",
            "Episode: **111**/4000, Score: -141 (Δ-22.1)\n",
            "Average score: -122.6 (Δ-0.18)\n",
            "Episode time: 6.0s, average: 6.9s (±160.02), ETA: 07:29:45 (01:04:50 to 07::12:24:34)\n",
            "Steps: 139. Time per step: 4.3e-02s. Reward per step: -1.02.\n",
            "It has been 110 episode(s) since the model was last saved, with a score of -108 (Δ-14.70).\n",
            "\n",
            "11:55:34 \n",
            "Episode: **112**/4000, Score: -136 (Δ  5.8)\n",
            "Average score: -122.8 (Δ-0.13)\n",
            "Episode time: 6.0s, average: 6.9s (±158.60), ETA: 07:29:05 (01:04:49 to 07::10:49:09)\n",
            "Steps: 159. Time per step: 3.8e-02s. Reward per step: -0.85.\n",
            "It has been 111 episode(s) since the model was last saved, with a score of -108 (Δ-14.83).\n",
            "\n",
            "11:55:38 \n",
            "Episode: **113**/4000, Score: -128 (Δ  7.4)\n",
            "Average score: -122.8 (Δ-0.06)\n",
            "Episode time: 4.0s, average: 6.9s (±157.27), ETA: 07:27:18 (01:04:48 to 07::09:18:38)\n",
            "Steps: 119. Time per step: 3.4e-02s. Reward per step: -1.08.\n",
            "It has been 112 episode(s) since the model was last saved, with a score of -108 (Δ-14.89).\n",
            "\n",
            "11:55:43 \n",
            "Episode: **114**/4000, Score: -117 (Δ 11.3)\n",
            "Average score: -122.8 (Δ 0.03)\n",
            "Episode time: 5.0s, average: 6.9s (±155.93), ETA: 07:26:06 (01:04:47 to 07::07:47:29)\n",
            "Steps: 135. Time per step: 3.7e-02s. Reward per step: -0.87.\n",
            "It has been 113 episode(s) since the model was last saved, with a score of -108 (Δ-14.86).\n",
            "\n",
            "11:55:49 \n",
            "Episode: **115**/4000, Score: -141 (Δ-24.5)\n",
            "Average score: -123.0 (Δ-0.19)\n",
            "Episode time: 6.0s, average: 6.9s (±154.58), ETA: 07:25:29 (01:04:46 to 07::06:16:53)\n",
            "Steps: 161. Time per step: 3.7e-02s. Reward per step: -0.88.\n",
            "It has been 114 episode(s) since the model was last saved, with a score of -108 (Δ-15.04).\n",
            "\n",
            "11:55:55 \n",
            "Episode: **116**/4000, Score: -141 (Δ  0.8)\n",
            "Average score: -123.2 (Δ-0.17)\n",
            "Episode time: 6.0s, average: 6.9s (±153.25), ETA: 07:24:53 (01:04:45 to 07::04:47:51)\n",
            "Steps: 164. Time per step: 3.7e-02s. Reward per step: -0.86.\n",
            "It has been 115 episode(s) since the model was last saved, with a score of -108 (Δ-15.21).\n",
            "\n",
            "11:56:06 \n",
            "Episode: **117**/4000, Score: -143 (Δ -2.8)\n",
            "Average score: -123.4 (Δ-0.21)\n",
            "Episode time: 11.0s, average: 6.9s (±152.09), ETA: 07:27:03 (01:04:44 to 07::03:32:02)\n",
            "Steps: 281. Time per step: 3.9e-02s. Reward per step: -0.51.\n",
            "It has been 116 episode(s) since the model was last saved, with a score of -108 (Δ-15.42).\n",
            "\n",
            "11:56:12 \n",
            "Episode: **118**/4000, Score: -140 (Δ  3.6)\n",
            "Average score: -123.5 (Δ-0.17)\n",
            "Episode time: 6.0s, average: 6.9s (±150.80), ETA: 07:26:26 (01:04:43 to 07::02:05:55)\n",
            "Steps: 157. Time per step: 3.8e-02s. Reward per step: -0.89.\n",
            "It has been 117 episode(s) since the model was last saved, with a score of -108 (Δ-15.59).\n",
            "\n",
            "11:56:18 \n",
            "Episode: **119**/4000, Score: -141 (Δ -1.1)\n",
            "Average score: -123.7 (Δ-0.18)\n",
            "Episode time: 6.0s, average: 6.9s (±149.54), ETA: 07:25:50 (01:04:42 to 07::00:41:15)\n",
            "Steps: 169. Time per step: 3.6e-02s. Reward per step: -0.83.\n",
            "It has been 118 episode(s) since the model was last saved, with a score of -108 (Δ-15.77).\n",
            "\n",
            "11:56:28 \n",
            "Episode: **120**/4000, Score: -140 (Δ  0.6)\n",
            "Average score: -123.9 (Δ-0.18)\n",
            "Episode time: 10.0s, average: 6.9s (±148.38), ETA: 07:27:24 (01:04:41 to 06::23:24:52)\n",
            "Steps: 260. Time per step: 3.8e-02s. Reward per step: -0.54.\n",
            "It has been 119 episode(s) since the model was last saved, with a score of -108 (Δ-15.95).\n",
            "\n",
            "11:56:30 \n",
            "Episode: **121**/4000, Score: -110 (Δ 30.4)\n",
            "Average score: -123.8 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 6.9s (±147.35), ETA: 07:24:39 (01:04:40 to 06::22:13:10)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.29.\n",
            "It has been 120 episode(s) since the model was last saved, with a score of -108 (Δ-15.83).\n",
            "\n",
            "11:56:40 \n",
            "Episode: **122**/4000, Score: -144 (Δ-34.0)\n",
            "Average score: -124.0 (Δ-0.22)\n",
            "Episode time: 10.0s, average: 6.9s (±146.22), ETA: 07:26:11 (01:04:39 to 06::20:59:18)\n",
            "Steps: 268. Time per step: 3.7e-02s. Reward per step: -0.54.\n",
            "It has been 121 episode(s) since the model was last saved, with a score of -108 (Δ-16.05).\n",
            "\n",
            "11:56:54 \n",
            "Episode: **123**/4000, Score: -157 (Δ-13.4)\n",
            "Average score: -124.3 (Δ-0.35)\n",
            "Episode time: 14.0s, average: 7.0s (±145.44), ETA: 07:29:48 (01:04:38 to 06::20:09:54)\n",
            "Steps: 385. Time per step: 3.6e-02s. Reward per step: -0.41.\n",
            "It has been 122 episode(s) since the model was last saved, with a score of -108 (Δ-16.40).\n",
            "\n",
            "11:56:56 \n",
            "Episode: **124**/4000, Score: -112 (Δ 45.6)\n",
            "Average score: -124.2 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 6.9s (±144.46), ETA: 07:27:06 (01:04:37 to 06::19:01:43)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -2.15.\n",
            "It has been 123 episode(s) since the model was last saved, with a score of -108 (Δ-16.29).\n",
            "\n",
            "11:56:58 \n",
            "Episode: **125**/4000, Score: -115 (Δ -3.2)\n",
            "Average score: -124.2 (Δ 0.07)\n",
            "Episode time: 2.0s, average: 6.9s (±143.50), ETA: 07:24:27 (01:04:36 to 06::17:54:24)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -2.13.\n",
            "It has been 124 episode(s) since the model was last saved, with a score of -108 (Δ-16.21).\n",
            "\n",
            "11:57:06 \n",
            "Episode: **126**/4000, Score: -118 (Δ -2.7)\n",
            "Average score: -124.1 (Δ 0.04)\n",
            "Episode time: 8.0s, average: 6.9s (±142.37), ETA: 07:24:54 (01:04:35 to 06::16:39:33)\n",
            "Steps: 217. Time per step: 3.7e-02s. Reward per step: -0.54.\n",
            "It has been 125 episode(s) since the model was last saved, with a score of -108 (Δ-16.18).\n",
            "\n",
            "11:57:08 \n",
            "Episode: **127**/4000, Score: -111 (Δ  6.6)\n",
            "Average score: -123.9 (Δ 0.24)\n",
            "Episode time: 2.0s, average: 6.9s (±141.43), ETA: 07:22:18 (01:04:34 to 06::15:34:15)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.36.\n",
            "It has been 126 episode(s) since the model was last saved, with a score of -108 (Δ-15.94).\n",
            "\n",
            "11:57:10 \n",
            "Episode: **128**/4000, Score: -115 (Δ -4.0)\n",
            "Average score: -123.6 (Δ 0.25)\n",
            "Episode time: 2.0s, average: 6.8s (±140.51), ETA: 07:19:45 (01:04:33 to 06::14:29:47)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -2.17.\n",
            "It has been 127 episode(s) since the model was last saved, with a score of -108 (Δ-15.69).\n",
            "\n",
            "11:57:15 \n",
            "Episode: **129**/4000, Score: -132 (Δ-16.6)\n",
            "Average score: -123.9 (Δ-0.24)\n",
            "Episode time: 5.0s, average: 6.8s (±139.45), ETA: 07:18:44 (01:04:32 to 06::13:17:45)\n",
            "Steps: 135. Time per step: 3.7e-02s. Reward per step: -0.97.\n",
            "It has been 128 episode(s) since the model was last saved, with a score of -108 (Δ-15.93).\n",
            "\n",
            "11:58:15 \n",
            "Episode: **130**/4000, Score: -123 (Δ  8.9)\n",
            "Average score: -124.0 (Δ-0.16)\n",
            "Episode time: 60.0s, average: 7.2s (±159.98), ETA: 07:45:01 (01:04:31 to 07::11:46:23)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.08.\n",
            "It has been 129 episode(s) since the model was last saved, with a score of -108 (Δ-16.09).\n",
            "\n",
            "11:58:39 \n",
            "Episode: **131**/4000, Score: -176 (Δ-53.0)\n",
            "Average score: -123.8 (Δ 0.22)\n",
            "Episode time: 24.0s, average: 7.3s (±160.89), ETA: 07:53:10 (01:04:30 to 07::12:50:53)\n",
            "Steps: 661. Time per step: 3.6e-02s. Reward per step: -0.27.\n",
            "It has been 130 episode(s) since the model was last saved, with a score of -108 (Δ-15.87).\n",
            "\n",
            "11:59:11 \n",
            "Episode: **132**/4000, Score: -181 (Δ -5.5)\n",
            "Average score: -124.5 (Δ-0.71)\n",
            "Episode time: 32.0s, average: 7.5s (±164.25), ETA: 08:05:05 (01:04:29 to 07::16:36:27)\n",
            "Steps: 832. Time per step: 3.8e-02s. Reward per step: -0.22.\n",
            "It has been 131 episode(s) since the model was last saved, with a score of -108 (Δ-16.58).\n",
            "\n",
            "11:59:13 \n",
            "Episode: **133**/4000, Score: -122 (Δ 59.3)\n",
            "Average score: -124.6 (Δ-0.13)\n",
            "Episode time: 2.0s, average: 7.5s (±163.24), ETA: 08:02:17 (01:04:28 to 07::15:25:58)\n",
            "Steps: 70. Time per step: 2.9e-02s. Reward per step: -1.74.\n",
            "It has been 132 episode(s) since the model was last saved, with a score of -108 (Δ-16.70).\n",
            "\n",
            "11:59:16 \n",
            "Episode: **134**/4000, Score: -117 (Δ  4.6)\n",
            "Average score: -124.7 (Δ-0.06)\n",
            "Episode time: 3.0s, average: 7.4s (±162.17), ETA: 08:00:00 (01:04:27 to 07::14:12:02)\n",
            "Steps: 57. Time per step: 5.3e-02s. Reward per step: -2.05.\n",
            "It has been 133 episode(s) since the model was last saved, with a score of -108 (Δ-16.76).\n",
            "\n",
            "11:59:17 \n",
            "Episode: **135**/4000, Score: -118 (Δ -0.5)\n",
            "Average score: -124.8 (Δ-0.11)\n",
            "Episode time: 1.0s, average: 7.4s (±161.28), ETA: 07:56:48 (01:04:26 to 07::13:08:25)\n",
            "Steps: 44. Time per step: 2.3e-02s. Reward per step: -2.67.\n",
            "It has been 134 episode(s) since the model was last saved, with a score of -108 (Δ-16.87).\n",
            "\n",
            "11:59:19 \n",
            "Episode: **136**/4000, Score: -116 (Δ  1.9)\n",
            "Average score: -124.9 (Δ-0.05)\n",
            "Episode time: 2.0s, average: 7.4s (±160.30), ETA: 07:54:08 (01:04:25 to 07::12:00:23)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.46.\n",
            "It has been 135 episode(s) since the model was last saved, with a score of -108 (Δ-16.92).\n",
            "\n",
            "11:59:21 \n",
            "Episode: **137**/4000, Score: -122 (Δ -6.1)\n",
            "Average score: -124.9 (Δ-0.08)\n",
            "Episode time: 2.0s, average: 7.3s (±159.34), ETA: 07:51:29 (01:04:24 to 07::10:53:07)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.96.\n",
            "It has been 136 episode(s) since the model was last saved, with a score of -108 (Δ-16.99).\n",
            "\n",
            "11:59:23 \n",
            "Episode: **138**/4000, Score: -121 (Δ  1.2)\n",
            "Average score: -125.1 (Δ-0.11)\n",
            "Episode time: 2.0s, average: 7.3s (±158.39), ETA: 07:48:53 (01:04:23 to 07::09:46:38)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.74.\n",
            "It has been 137 episode(s) since the model was last saved, with a score of -108 (Δ-17.11).\n",
            "\n",
            "11:59:28 \n",
            "Episode: **139**/4000, Score: -115 (Δ  5.2)\n",
            "Average score: -125.1 (Δ-0.01)\n",
            "Episode time: 5.0s, average: 7.3s (±157.29), ETA: 07:47:42 (01:04:22 to 07::08:31:52)\n",
            "Steps: 141. Time per step: 3.5e-02s. Reward per step: -0.82.\n",
            "It has been 138 episode(s) since the model was last saved, with a score of -108 (Δ-17.12).\n",
            "\n",
            "11:59:30 \n",
            "Episode: **140**/4000, Score: -122 (Δ -7.0)\n",
            "Average score: -125.1 (Δ-0.08)\n",
            "Episode time: 2.0s, average: 7.2s (±156.36), ETA: 07:45:10 (01:04:21 to 07::07:27:03)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.85.\n",
            "It has been 139 episode(s) since the model was last saved, with a score of -108 (Δ-17.19).\n",
            "\n",
            "11:59:35 \n",
            "Episode: **141**/4000, Score: -115 (Δ  7.0)\n",
            "Average score: -125.2 (Δ-0.08)\n",
            "Episode time: 5.0s, average: 7.2s (±155.29), ETA: 07:44:01 (01:04:20 to 07::06:14:13)\n",
            "Steps: 135. Time per step: 3.7e-02s. Reward per step: -0.85.\n",
            "It has been 140 episode(s) since the model was last saved, with a score of -108 (Δ-17.27).\n",
            "\n",
            "11:59:37 \n",
            "Episode: **142**/4000, Score: -116 (Δ -0.2)\n",
            "Average score: -125.2 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 7.2s (±154.38), ETA: 07:41:32 (01:04:19 to 07::05:11:02)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.31.\n",
            "It has been 141 episode(s) since the model was last saved, with a score of -108 (Δ-17.24).\n",
            "\n",
            "11:59:41 \n",
            "Episode: **143**/4000, Score: -114 (Δ  1.2)\n",
            "Average score: -125.3 (Δ-0.14)\n",
            "Episode time: 4.0s, average: 7.2s (±153.37), ETA: 07:40:00 (01:04:18 to 07::04:02:00)\n",
            "Steps: 117. Time per step: 3.4e-02s. Reward per step: -0.98.\n",
            "It has been 142 episode(s) since the model was last saved, with a score of -108 (Δ-17.37).\n",
            "\n",
            "11:59:46 \n",
            "Episode: **144**/4000, Score: -117 (Δ -2.1)\n",
            "Average score: -125.4 (Δ-0.10)\n",
            "Episode time: 5.0s, average: 7.1s (±152.34), ETA: 07:38:55 (01:04:17 to 07::02:51:57)\n",
            "Steps: 133. Time per step: 3.8e-02s. Reward per step: -0.88.\n",
            "It has been 143 episode(s) since the model was last saved, with a score of -108 (Δ-17.47).\n",
            "\n",
            "11:59:50 \n",
            "Episode: **145**/4000, Score: -113 (Δ  4.0)\n",
            "Average score: -125.5 (Δ-0.05)\n",
            "Episode time: 4.0s, average: 7.1s (±151.36), ETA: 07:37:24 (01:04:16 to 07::01:44:43)\n",
            "Steps: 107. Time per step: 3.7e-02s. Reward per step: -1.05.\n",
            "It has been 144 episode(s) since the model was last saved, with a score of -108 (Δ-17.53).\n",
            "\n",
            "11:59:57 \n",
            "Episode: **146**/4000, Score: -118 (Δ -5.6)\n",
            "Average score: -125.6 (Δ-0.11)\n",
            "Episode time: 7.0s, average: 7.1s (±150.32), ETA: 07:37:14 (01:04:15 to 07::00:35:25)\n",
            "Steps: 173. Time per step: 4.0e-02s. Reward per step: -0.68.\n",
            "It has been 145 episode(s) since the model was last saved, with a score of -108 (Δ-17.63).\n",
            "\n",
            "11:59:58 \n",
            "Episode: **147**/4000, Score: -122 (Δ -4.2)\n",
            "Average score: -125.8 (Δ-0.21)\n",
            "Episode time: 1.0s, average: 7.1s (±149.55), ETA: 07:34:26 (01:04:14 to 06::23:40:41)\n",
            "Steps: 43. Time per step: 2.3e-02s. Reward per step: -2.85.\n",
            "It has been 146 episode(s) since the model was last saved, with a score of -108 (Δ-17.84).\n",
            "\n",
            "12:00:00 \n",
            "Episode: **148**/4000, Score: -119 (Δ  3.2)\n",
            "Average score: -125.9 (Δ-0.12)\n",
            "Episode time: 2.0s, average: 7.0s (±148.71), ETA: 07:32:07 (01:04:13 to 06::22:42:04)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.59.\n",
            "It has been 147 episode(s) since the model was last saved, with a score of -108 (Δ-17.96).\n",
            "\n",
            "12:00:06 \n",
            "Episode: **149**/4000, Score: -118 (Δ  1.4)\n",
            "Average score: -125.9 (Δ-0.04)\n",
            "Episode time: 6.0s, average: 7.0s (±147.72), ETA: 07:31:33 (01:04:12 to 06::21:35:25)\n",
            "Steps: 157. Time per step: 3.8e-02s. Reward per step: -0.75.\n",
            "It has been 148 episode(s) since the model was last saved, with a score of -108 (Δ-18.00).\n",
            "\n",
            "12:00:09 \n",
            "Episode: **150**/4000, Score: -118 (Δ  0.2)\n",
            "Average score: -126.0 (Δ-0.09)\n",
            "Episode time: 3.0s, average: 7.0s (±146.85), ETA: 07:29:43 (01:04:11 to 06::20:34:49)\n",
            "Steps: 74. Time per step: 4.1e-02s. Reward per step: -1.59.\n",
            "It has been 149 episode(s) since the model was last saved, with a score of -108 (Δ-18.10).\n",
            "\n",
            "12:00:12 \n",
            "Episode: **151**/4000, Score: -121 (Δ -3.5)\n",
            "Average score: -126.1 (Δ-0.06)\n",
            "Episode time: 3.0s, average: 7.0s (±145.98), ETA: 07:27:54 (01:04:10 to 06::19:34:56)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.44.\n",
            "It has been 150 episode(s) since the model was last saved, with a score of -108 (Δ-18.15).\n",
            "\n",
            "12:00:14 \n",
            "Episode: **152**/4000, Score: -123 (Δ -1.7)\n",
            "Average score: -126.2 (Δ-0.12)\n",
            "Episode time: 2.0s, average: 6.9s (±145.18), ETA: 07:25:40 (01:04:09 to 06::18:39:04)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.79.\n",
            "It has been 151 episode(s) since the model was last saved, with a score of -108 (Δ-18.28).\n",
            "\n",
            "12:00:16 \n",
            "Episode: **153**/4000, Score: -111 (Δ 11.7)\n",
            "Average score: -126.2 (Δ 0.03)\n",
            "Episode time: 2.0s, average: 6.9s (±144.39), ETA: 07:23:29 (01:04:08 to 06::17:43:47)\n",
            "Steps: 67. Time per step: 3.0e-02s. Reward per step: -1.66.\n",
            "It has been 152 episode(s) since the model was last saved, with a score of -108 (Δ-18.25).\n",
            "\n",
            "12:00:18 \n",
            "Episode: **154**/4000, Score: -117 (Δ -5.7)\n",
            "Average score: -126.2 (Δ 0.03)\n",
            "Episode time: 2.0s, average: 6.9s (±143.61), ETA: 07:21:19 (01:04:07 to 06::16:49:06)\n",
            "Steps: 40. Time per step: 5.0e-02s. Reward per step: -2.92.\n",
            "It has been 153 episode(s) since the model was last saved, with a score of -108 (Δ-18.22).\n",
            "\n",
            "12:00:19 \n",
            "Episode: **155**/4000, Score: -122 (Δ -5.2)\n",
            "Average score: -126.3 (Δ-0.12)\n",
            "Episode time: 1.0s, average: 6.8s (±142.91), ETA: 07:18:46 (01:04:06 to 06::15:58:59)\n",
            "Steps: 45. Time per step: 2.2e-02s. Reward per step: -2.71.\n",
            "It has been 154 episode(s) since the model was last saved, with a score of -108 (Δ-18.34).\n",
            "\n",
            "12:00:22 \n",
            "Episode: **156**/4000, Score: -119 (Δ  3.4)\n",
            "Average score: -126.2 (Δ 0.05)\n",
            "Episode time: 3.0s, average: 6.8s (±142.08), ETA: 07:17:05 (01:04:05 to 06::15:02:15)\n",
            "Steps: 75. Time per step: 4.0e-02s. Reward per step: -1.58.\n",
            "It has been 155 episode(s) since the model was last saved, with a score of -108 (Δ-18.29).\n",
            "\n",
            "12:00:24 \n",
            "Episode: **157**/4000, Score: -122 (Δ -2.9)\n",
            "Average score: -126.2 (Δ 0.01)\n",
            "Episode time: 2.0s, average: 6.8s (±141.33), ETA: 07:15:00 (01:04:04 to 06::14:09:14)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.89.\n",
            "It has been 156 episode(s) since the model was last saved, with a score of -108 (Δ-18.28).\n",
            "\n",
            "12:00:29 \n",
            "Episode: **158**/4000, Score: -114 (Δ  7.1)\n",
            "Average score: -126.1 (Δ 0.08)\n",
            "Episode time: 5.0s, average: 6.8s (±140.45), ETA: 07:14:10 (01:04:03 to 06::13:10:03)\n",
            "Steps: 148. Time per step: 3.4e-02s. Reward per step: -0.77.\n",
            "It has been 157 episode(s) since the model was last saved, with a score of -108 (Δ-18.20).\n",
            "\n",
            "12:00:31 \n",
            "Episode: **159**/4000, Score: -116 (Δ -1.3)\n",
            "Average score: -126.2 (Δ-0.06)\n",
            "Episode time: 2.0s, average: 6.7s (±139.71), ETA: 07:12:07 (01:04:02 to 06::12:18:14)\n",
            "Steps: 40. Time per step: 5.0e-02s. Reward per step: -2.89.\n",
            "It has been 158 episode(s) since the model was last saved, with a score of -108 (Δ-18.26).\n",
            "\n",
            "12:00:36 \n",
            "Episode: **160**/4000, Score: -113 (Δ  2.4)\n",
            "Average score: -126.1 (Δ 0.09)\n",
            "Episode time: 5.0s, average: 6.7s (±138.86), ETA: 07:11:19 (01:04:01 to 06::11:20:25)\n",
            "Steps: 129. Time per step: 3.9e-02s. Reward per step: -0.88.\n",
            "It has been 159 episode(s) since the model was last saved, with a score of -108 (Δ-18.17).\n",
            "\n",
            "12:00:37 \n",
            "Episode: **161**/4000, Score: -120 (Δ -6.2)\n",
            "Average score: -126.1 (Δ 0.06)\n",
            "Episode time: 1.0s, average: 6.7s (±138.20), ETA: 07:08:55 (01:04:00 to 06::10:33:31)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.85.\n",
            "It has been 160 episode(s) since the model was last saved, with a score of -108 (Δ-18.11).\n",
            "\n",
            "12:00:40 \n",
            "Episode: **162**/4000, Score: -111 (Δ  8.1)\n",
            "Average score: -125.9 (Δ 0.14)\n",
            "Episode time: 3.0s, average: 6.7s (±137.43), ETA: 07:07:21 (01:03:59 to 06::09:40:26)\n",
            "Steps: 72. Time per step: 4.2e-02s. Reward per step: -1.55.\n",
            "It has been 161 episode(s) since the model was last saved, with a score of -108 (Δ-17.97).\n",
            "\n",
            "12:00:42 \n",
            "Episode: **163**/4000, Score: -118 (Δ -7.0)\n",
            "Average score: -126.0 (Δ-0.06)\n",
            "Episode time: 2.0s, average: 6.7s (±136.72), ETA: 07:05:24 (01:03:58 to 06::08:50:48)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.69.\n",
            "It has been 162 episode(s) since the model was last saved, with a score of -108 (Δ-18.03).\n",
            "\n",
            "12:00:43 \n",
            "Episode: **164**/4000, Score: -120 (Δ -1.6)\n",
            "Average score: -125.9 (Δ 0.09)\n",
            "Episode time: 1.0s, average: 6.6s (±136.08), ETA: 07:03:05 (01:03:57 to 06::08:05:16)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.86.\n",
            "It has been 163 episode(s) since the model was last saved, with a score of -108 (Δ-17.94).\n",
            "\n",
            "12:00:45 \n",
            "Episode: **165**/4000, Score: -120 (Δ -0.2)\n",
            "Average score: -125.7 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 6.6s (±135.38), ETA: 07:01:11 (01:03:56 to 06::07:16:35)\n",
            "Steps: 41. Time per step: 4.9e-02s. Reward per step: -2.93.\n",
            "It has been 164 episode(s) since the model was last saved, with a score of -108 (Δ-17.78).\n",
            "\n",
            "12:00:46 \n",
            "Episode: **166**/4000, Score: -115 (Δ  5.0)\n",
            "Average score: -125.6 (Δ 0.15)\n",
            "Episode time: 1.0s, average: 6.6s (±134.75), ETA: 06:58:55 (01:03:55 to 06::06:31:54)\n",
            "Steps: 37. Time per step: 2.7e-02s. Reward per step: -3.11.\n",
            "It has been 165 episode(s) since the model was last saved, with a score of -108 (Δ-17.63).\n",
            "\n",
            "12:00:48 \n",
            "Episode: **167**/4000, Score: -121 (Δ -5.3)\n",
            "Average score: -125.4 (Δ 0.14)\n",
            "Episode time: 2.0s, average: 6.5s (±134.07), ETA: 06:57:04 (01:03:54 to 06::05:44:07)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.80.\n",
            "It has been 166 episode(s) since the model was last saved, with a score of -108 (Δ-17.49).\n",
            "\n",
            "12:00:49 \n",
            "Episode: **168**/4000, Score: -122 (Δ -1.8)\n",
            "Average score: -125.6 (Δ-0.12)\n",
            "Episode time: 1.0s, average: 6.5s (±133.45), ETA: 06:54:52 (01:03:53 to 06::05:00:15)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.91.\n",
            "It has been 167 episode(s) since the model was last saved, with a score of -108 (Δ-17.61).\n",
            "\n",
            "12:00:52 \n",
            "Episode: **169**/4000, Score: -118 (Δ  4.7)\n",
            "Average score: -125.6 (Δ-0.01)\n",
            "Episode time: 3.0s, average: 6.5s (±132.73), ETA: 06:53:26 (01:03:52 to 06::04:10:45)\n",
            "Steps: 69. Time per step: 4.3e-02s. Reward per step: -1.70.\n",
            "It has been 168 episode(s) since the model was last saved, with a score of -108 (Δ-17.62).\n",
            "\n",
            "12:00:56 \n",
            "Episode: **170**/4000, Score: -112 (Δ  6.1)\n",
            "Average score: -125.4 (Δ 0.21)\n",
            "Episode time: 4.0s, average: 6.5s (±131.99), ETA: 06:52:24 (01:03:51 to 06::03:19:55)\n",
            "Steps: 96. Time per step: 4.2e-02s. Reward per step: -1.16.\n",
            "It has been 169 episode(s) since the model was last saved, with a score of -108 (Δ-17.42).\n",
            "\n",
            "12:00:59 \n",
            "Episode: **171**/4000, Score: -123 (Δ-11.1)\n",
            "Average score: -125.3 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 6.4s (±131.29), ETA: 06:51:00 (01:03:50 to 06::02:31:30)\n",
            "Steps: 85. Time per step: 3.5e-02s. Reward per step: -1.44.\n",
            "It has been 170 episode(s) since the model was last saved, with a score of -108 (Δ-17.34).\n",
            "\n",
            "12:01:03 \n",
            "Episode: **172**/4000, Score: -126 (Δ -3.6)\n",
            "Average score: -125.2 (Δ 0.06)\n",
            "Episode time: 4.0s, average: 6.4s (±130.56), ETA: 06:49:59 (01:03:49 to 06::01:41:47)\n",
            "Steps: 110. Time per step: 3.6e-02s. Reward per step: -1.15.\n",
            "It has been 171 episode(s) since the model was last saved, with a score of -108 (Δ-17.28).\n",
            "\n",
            "12:01:05 \n",
            "Episode: **173**/4000, Score: -120 (Δ  5.7)\n",
            "Average score: -125.1 (Δ 0.11)\n",
            "Episode time: 2.0s, average: 6.4s (±129.92), ETA: 06:48:15 (01:03:48 to 06::00:56:53)\n",
            "Steps: 45. Time per step: 4.4e-02s. Reward per step: -2.68.\n",
            "It has been 172 episode(s) since the model was last saved, with a score of -108 (Δ-17.17).\n",
            "\n",
            "12:01:06 \n",
            "Episode: **174**/4000, Score: -120 (Δ  0.5)\n",
            "Average score: -125.0 (Δ 0.09)\n",
            "Episode time: 1.0s, average: 6.4s (±129.34), ETA: 06:46:10 (01:03:47 to 06::00:15:38)\n",
            "Steps: 44. Time per step: 2.3e-02s. Reward per step: -2.73.\n",
            "It has been 173 episode(s) since the model was last saved, with a score of -108 (Δ-17.08).\n",
            "\n",
            "12:01:08 \n",
            "Episode: **175**/4000, Score: -120 (Δ  0.2)\n",
            "Average score: -125.0 (Δ-0.02)\n",
            "Episode time: 2.0s, average: 6.3s (±128.71), ETA: 06:44:28 (01:03:46 to 05::23:31:34)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.85.\n",
            "It has been 174 episode(s) since the model was last saved, with a score of -108 (Δ-17.10).\n",
            "\n",
            "12:01:12 \n",
            "Episode: **176**/4000, Score: -112 (Δ  8.1)\n",
            "Average score: -124.8 (Δ 0.28)\n",
            "Episode time: 4.0s, average: 6.3s (±128.01), ETA: 06:43:31 (01:03:45 to 05::22:43:50)\n",
            "Steps: 116. Time per step: 3.4e-02s. Reward per step: -0.96.\n",
            "It has been 175 episode(s) since the model was last saved, with a score of -108 (Δ-16.83).\n",
            "\n",
            "12:01:14 \n",
            "Episode: **177**/4000, Score: -120 (Δ -8.0)\n",
            "Average score: -124.8 (Δ 0.01)\n",
            "Episode time: 2.0s, average: 6.3s (±127.39), ETA: 06:41:51 (01:03:44 to 05::22:00:39)\n",
            "Steps: 40. Time per step: 5.0e-02s. Reward per step: -2.99.\n",
            "It has been 176 episode(s) since the model was last saved, with a score of -108 (Δ-16.82).\n",
            "\n",
            "12:01:19 \n",
            "Episode: **178**/4000, Score: -114 (Δ  5.3)\n",
            "Average score: -124.7 (Δ 0.03)\n",
            "Episode time: 5.0s, average: 6.3s (±126.68), ETA: 06:41:16 (01:03:43 to 05::21:12:58)\n",
            "Steps: 127. Time per step: 3.9e-02s. Reward per step: -0.90.\n",
            "It has been 177 episode(s) since the model was last saved, with a score of -108 (Δ-16.79).\n",
            "\n",
            "12:01:20 \n",
            "Episode: **179**/4000, Score: -121 (Δ -7.2)\n",
            "Average score: -124.8 (Δ-0.05)\n",
            "Episode time: 1.0s, average: 6.3s (±126.13), ETA: 06:39:17 (01:03:42 to 05::20:33:43)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.89.\n",
            "It has been 178 episode(s) since the model was last saved, with a score of -108 (Δ-16.84).\n",
            "\n",
            "12:01:22 \n",
            "Episode: **180**/4000, Score: -118 (Δ  3.9)\n",
            "Average score: -124.0 (Δ 0.73)\n",
            "Episode time: 2.0s, average: 6.2s (±125.53), ETA: 06:37:40 (01:03:41 to 05::19:51:47)\n",
            "Steps: 45. Time per step: 4.4e-02s. Reward per step: -2.61.\n",
            "It has been 179 episode(s) since the model was last saved, with a score of -108 (Δ-16.11).\n",
            "\n",
            "12:01:24 \n",
            "Episode: **181**/4000, Score: -122 (Δ -4.2)\n",
            "Average score: -123.7 (Δ 0.33)\n",
            "Episode time: 2.0s, average: 6.2s (±124.93), ETA: 06:36:04 (01:03:40 to 05::19:10:14)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.77.\n",
            "It has been 180 episode(s) since the model was last saved, with a score of -108 (Δ-15.78).\n",
            "\n",
            "12:01:27 \n",
            "Episode: **182**/4000, Score: -111 (Δ 10.6)\n",
            "Average score: -123.2 (Δ 0.50)\n",
            "Episode time: 3.0s, average: 6.2s (±124.30), ETA: 06:34:50 (01:03:39 to 05::18:26:51)\n",
            "Steps: 77. Time per step: 3.9e-02s. Reward per step: -1.44.\n",
            "It has been 181 episode(s) since the model was last saved, with a score of -108 (Δ-15.28).\n",
            "\n",
            "12:01:28 \n",
            "Episode: **183**/4000, Score: -123 (Δ-11.9)\n",
            "Average score: -123.0 (Δ 0.24)\n",
            "Episode time: 1.0s, average: 6.2s (±123.77), ETA: 06:32:56 (01:03:38 to 05::17:49:00)\n",
            "Steps: 43. Time per step: 2.3e-02s. Reward per step: -2.86.\n",
            "It has been 182 episode(s) since the model was last saved, with a score of -108 (Δ-15.04).\n",
            "\n",
            "12:01:30 \n",
            "Episode: **184**/4000, Score: -119 (Δ  3.7)\n",
            "Average score: -122.9 (Δ 0.11)\n",
            "Episode time: 2.0s, average: 6.2s (±123.19), ETA: 06:31:23 (01:03:37 to 05::17:08:35)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.59.\n",
            "It has been 183 episode(s) since the model was last saved, with a score of -108 (Δ-14.92).\n",
            "\n",
            "12:01:36 \n",
            "Episode: **185**/4000, Score: -117 (Δ  2.7)\n",
            "Average score: -122.9 (Δ-0.02)\n",
            "Episode time: 6.0s, average: 6.2s (±122.53), ETA: 06:31:14 (01:03:36 to 05::16:24:02)\n",
            "Steps: 168. Time per step: 3.6e-02s. Reward per step: -0.69.\n",
            "It has been 184 episode(s) since the model was last saved, with a score of -108 (Δ-14.95).\n",
            "\n",
            "12:01:38 \n",
            "Episode: **186**/4000, Score: -122 (Δ -5.8)\n",
            "Average score: -122.7 (Δ 0.20)\n",
            "Episode time: 2.0s, average: 6.1s (±121.96), ETA: 06:29:42 (01:03:35 to 05::15:44:27)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.85.\n",
            "It has been 185 episode(s) since the model was last saved, with a score of -108 (Δ-14.74).\n",
            "\n",
            "12:01:42 \n",
            "Episode: **187**/4000, Score: -113 (Δ  9.4)\n",
            "Average score: -122.6 (Δ 0.09)\n",
            "Episode time: 4.0s, average: 6.1s (±121.33), ETA: 06:28:53 (01:03:34 to 05::15:01:40)\n",
            "Steps: 112. Time per step: 3.6e-02s. Reward per step: -1.01.\n",
            "It has been 186 episode(s) since the model was last saved, with a score of -108 (Δ-14.65).\n",
            "\n",
            "12:01:44 \n",
            "Episode: **188**/4000, Score: -122 (Δ -9.1)\n",
            "Average score: -122.6 (Δ-0.02)\n",
            "Episode time: 2.0s, average: 6.1s (±120.78), ETA: 06:27:23 (01:03:33 to 05::14:22:50)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.77.\n",
            "It has been 187 episode(s) since the model was last saved, with a score of -108 (Δ-14.67).\n",
            "\n",
            "12:01:45 \n",
            "Episode: **189**/4000, Score: -118 (Δ  4.4)\n",
            "Average score: -122.5 (Δ 0.12)\n",
            "Episode time: 1.0s, average: 6.1s (±120.28), ETA: 06:25:34 (01:03:32 to 05::13:47:05)\n",
            "Steps: 41. Time per step: 2.4e-02s. Reward per step: -2.87.\n",
            "It has been 188 episode(s) since the model was last saved, with a score of -108 (Δ-14.55).\n",
            "\n",
            "12:01:48 \n",
            "Episode: **190**/4000, Score: -112 (Δ  5.4)\n",
            "Average score: -122.3 (Δ 0.21)\n",
            "Episode time: 3.0s, average: 6.1s (±119.69), ETA: 06:24:27 (01:03:31 to 05::13:06:53)\n",
            "Steps: 79. Time per step: 3.8e-02s. Reward per step: -1.42.\n",
            "It has been 189 episode(s) since the model was last saved, with a score of -108 (Δ-14.34).\n",
            "\n",
            "12:01:50 \n",
            "Episode: **191**/4000, Score: -122 (Δ -9.4)\n",
            "Average score: -122.3 (Δ-0.03)\n",
            "Episode time: 2.0s, average: 6.0s (±119.15), ETA: 06:23:00 (01:03:30 to 05::12:29:04)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.83.\n",
            "It has been 190 episode(s) since the model was last saved, with a score of -108 (Δ-14.37).\n",
            "\n",
            "12:01:52 \n",
            "Episode: **192**/4000, Score: -118 (Δ  4.1)\n",
            "Average score: -122.2 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 6.0s (±118.61), ETA: 06:21:34 (01:03:29 to 05::11:51:36)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.67.\n",
            "It has been 191 episode(s) since the model was last saved, with a score of -108 (Δ-14.21).\n",
            "\n",
            "12:01:55 \n",
            "Episode: **193**/4000, Score: -121 (Δ -3.1)\n",
            "Average score: -122.1 (Δ 0.04)\n",
            "Episode time: 3.0s, average: 6.0s (±118.05), ETA: 06:20:28 (01:03:28 to 05::11:12:30)\n",
            "Steps: 79. Time per step: 3.8e-02s. Reward per step: -1.53.\n",
            "It has been 192 episode(s) since the model was last saved, with a score of -108 (Δ-14.17).\n",
            "\n",
            "12:01:56 \n",
            "Episode: **194**/4000, Score: -122 (Δ -1.0)\n",
            "Average score: -122.3 (Δ-0.15)\n",
            "Episode time: 1.0s, average: 6.0s (±117.57), ETA: 06:18:44 (01:03:27 to 05::10:38:18)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.90.\n",
            "It has been 193 episode(s) since the model was last saved, with a score of -108 (Δ-14.32).\n",
            "\n",
            "12:01:58 \n",
            "Episode: **195**/4000, Score: -120 (Δ  1.5)\n",
            "Average score: -122.3 (Δ-0.00)\n",
            "Episode time: 2.0s, average: 5.9s (±117.04), ETA: 06:17:21 (01:03:26 to 05::10:01:48)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.80.\n",
            "It has been 194 episode(s) since the model was last saved, with a score of -108 (Δ-14.32).\n",
            "\n",
            "12:01:59 \n",
            "Episode: **196**/4000, Score: -122 (Δ -2.1)\n",
            "Average score: -122.1 (Δ 0.13)\n",
            "Episode time: 1.0s, average: 5.9s (±116.57), ETA: 06:15:39 (01:03:25 to 05::09:28:10)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.91.\n",
            "It has been 195 episode(s) since the model was last saved, with a score of -108 (Δ-14.19).\n",
            "\n",
            "12:02:03 \n",
            "Episode: **197**/4000, Score: -112 (Δ 10.6)\n",
            "Average score: -122.1 (Δ 0.04)\n",
            "Episode time: 4.0s, average: 5.9s (±116.00), ETA: 06:14:56 (01:03:24 to 05::08:49:11)\n",
            "Steps: 93. Time per step: 4.3e-02s. Reward per step: -1.20.\n",
            "It has been 196 episode(s) since the model was last saved, with a score of -108 (Δ-14.15).\n",
            "\n",
            "12:02:05 \n",
            "Episode: **198**/4000, Score: -114 (Δ -2.0)\n",
            "Average score: -122.1 (Δ-0.01)\n",
            "Episode time: 2.0s, average: 5.9s (±115.49), ETA: 06:13:35 (01:03:23 to 05::08:13:38)\n",
            "Steps: 61. Time per step: 3.3e-02s. Reward per step: -1.86.\n",
            "It has been 197 episode(s) since the model was last saved, with a score of -108 (Δ-14.16).\n",
            "\n",
            "12:02:07 \n",
            "Episode: **199**/4000, Score: -122 (Δ -8.7)\n",
            "Average score: -122.2 (Δ-0.10)\n",
            "Episode time: 2.0s, average: 5.9s (±114.98), ETA: 06:12:14 (01:03:22 to 05::07:38:24)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.91.\n",
            "It has been 198 episode(s) since the model was last saved, with a score of -108 (Δ-14.27).\n",
            "\n",
            "12:02:08 \n",
            "Episode: **200**/4000, Score: -123 (Δ -0.4)\n",
            "Average score: -122.3 (Δ-0.14)\n",
            "Episode time: 1.0s, average: 5.8s (±114.53), ETA: 06:10:36 (01:03:21 to 05::07:05:55)\n",
            "Steps: 43. Time per step: 2.3e-02s. Reward per step: -2.86.\n",
            "It has been 199 episode(s) since the model was last saved, with a score of -108 (Δ-14.40).\n",
            "\n",
            "12:02:11 \n",
            "Episode: **201**/4000, Score: -112 (Δ 10.3)\n",
            "Average score: -122.3 (Δ 0.03)\n",
            "Episode time: 3.0s, average: 5.8s (±114.00), ETA: 06:09:36 (01:03:20 to 05::06:29:28)\n",
            "Steps: 74. Time per step: 4.1e-02s. Reward per step: -1.52.\n",
            "It has been 200 episode(s) since the model was last saved, with a score of -108 (Δ-14.37).\n",
            "\n",
            "12:02:13 \n",
            "Episode: **202**/4000, Score: -121 (Δ -8.8)\n",
            "Average score: -122.4 (Δ-0.06)\n",
            "Episode time: 2.0s, average: 5.8s (±113.51), ETA: 06:08:18 (01:03:19 to 05::05:55:08)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.82.\n",
            "It has been 201 episode(s) since the model was last saved, with a score of -108 (Δ-14.44).\n",
            "\n",
            "12:02:15 \n",
            "Episode: **203**/4000, Score: -122 (Δ -0.7)\n",
            "Average score: -122.4 (Δ-0.06)\n",
            "Episode time: 2.0s, average: 5.8s (±113.02), ETA: 06:07:01 (01:03:18 to 05::05:21:05)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.90.\n",
            "It has been 202 episode(s) since the model was last saved, with a score of -108 (Δ-14.50).\n",
            "\n",
            "12:02:16 \n",
            "Episode: **204**/4000, Score: -120 (Δ  1.9)\n",
            "Average score: -122.5 (Δ-0.03)\n",
            "Episode time: 1.0s, average: 5.8s (±112.58), ETA: 06:05:26 (01:03:17 to 05::04:49:39)\n",
            "Steps: 44. Time per step: 2.3e-02s. Reward per step: -2.73.\n",
            "It has been 203 episode(s) since the model was last saved, with a score of -108 (Δ-14.53).\n",
            "\n",
            "12:02:18 \n",
            "Episode: **205**/4000, Score: -120 (Δ -0.2)\n",
            "Average score: -122.4 (Δ 0.03)\n",
            "Episode time: 2.0s, average: 5.8s (±112.10), ETA: 06:04:10 (01:03:16 to 05::04:16:09)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.80.\n",
            "It has been 204 episode(s) since the model was last saved, with a score of -108 (Δ-14.50).\n",
            "\n",
            "12:02:20 \n",
            "Episode: **206**/4000, Score: -123 (Δ -2.3)\n",
            "Average score: -122.2 (Δ 0.29)\n",
            "Episode time: 2.0s, average: 5.7s (±111.62), ETA: 06:02:55 (01:03:15 to 05::03:42:55)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.85.\n",
            "It has been 205 episode(s) since the model was last saved, with a score of -108 (Δ-14.22).\n",
            "\n",
            "12:02:21 \n",
            "Episode: **207**/4000, Score: -117 (Δ  5.2)\n",
            "Average score: -122.1 (Δ 0.10)\n",
            "Episode time: 1.0s, average: 5.7s (±111.19), ETA: 06:01:23 (01:03:14 to 05::03:12:15)\n",
            "Steps: 48. Time per step: 2.1e-02s. Reward per step: -2.45.\n",
            "It has been 206 episode(s) since the model was last saved, with a score of -108 (Δ-14.12).\n",
            "\n",
            "12:02:23 \n",
            "Episode: **208**/4000, Score: -122 (Δ -4.6)\n",
            "Average score: -122.0 (Δ 0.01)\n",
            "Episode time: 2.0s, average: 5.7s (±110.72), ETA: 06:00:09 (01:03:13 to 05::02:39:33)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.84.\n",
            "It has been 207 episode(s) since the model was last saved, with a score of -108 (Δ-14.11).\n",
            "\n",
            "12:02:25 \n",
            "Episode: **209**/4000, Score: -118 (Δ  3.9)\n",
            "Average score: -122.0 (Δ 0.05)\n",
            "Episode time: 2.0s, average: 5.7s (±110.26), ETA: 05:58:56 (01:03:12 to 05::02:07:07)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.75.\n",
            "It has been 208 episode(s) since the model was last saved, with a score of -108 (Δ-14.06).\n",
            "\n",
            "12:02:26 \n",
            "Episode: **210**/4000, Score: -121 (Δ -2.9)\n",
            "Average score: -122.0 (Δ-0.02)\n",
            "Episode time: 1.0s, average: 5.7s (±109.83), ETA: 05:57:26 (01:03:11 to 05::01:37:10)\n",
            "Steps: 43. Time per step: 2.3e-02s. Reward per step: -2.81.\n",
            "It has been 209 episode(s) since the model was last saved, with a score of -108 (Δ-14.07).\n",
            "\n",
            "12:02:28 \n",
            "Episode: **211**/4000, Score: -121 (Δ  0.5)\n",
            "Average score: -121.8 (Δ 0.21)\n",
            "Episode time: 2.0s, average: 5.6s (±109.38), ETA: 05:56:15 (01:03:10 to 05::01:05:15)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.87.\n",
            "It has been 210 episode(s) since the model was last saved, with a score of -108 (Δ-13.86).\n",
            "\n",
            "12:02:30 \n",
            "Episode: **212**/4000, Score: -122 (Δ -1.4)\n",
            "Average score: -121.7 (Δ 0.14)\n",
            "Episode time: 2.0s, average: 5.6s (±108.92), ETA: 05:55:04 (01:03:09 to 05::00:33:36)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.77.\n",
            "It has been 211 episode(s) since the model was last saved, with a score of -108 (Δ-13.73).\n",
            "\n",
            "12:02:31 \n",
            "Episode: **213**/4000, Score: -123 (Δ -0.7)\n",
            "Average score: -121.6 (Δ 0.05)\n",
            "Episode time: 1.0s, average: 5.6s (±108.51), ETA: 05:53:36 (01:03:08 to 05::00:04:20)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.92.\n",
            "It has been 212 episode(s) since the model was last saved, with a score of -108 (Δ-13.67).\n",
            "\n",
            "12:02:33 \n",
            "Episode: **214**/4000, Score: -119 (Δ  3.8)\n",
            "Average score: -121.6 (Δ-0.02)\n",
            "Episode time: 2.0s, average: 5.6s (±108.07), ETA: 05:52:27 (01:03:07 to 04::23:33:11)\n",
            "Steps: 40. Time per step: 5.0e-02s. Reward per step: -2.97.\n",
            "It has been 213 episode(s) since the model was last saved, with a score of -108 (Δ-13.69).\n",
            "\n",
            "12:02:36 \n",
            "Episode: **215**/4000, Score: -114 (Δ  4.8)\n",
            "Average score: -121.4 (Δ 0.27)\n",
            "Episode time: 3.0s, average: 5.6s (±107.59), ETA: 05:51:36 (01:03:06 to 04::23:00:45)\n",
            "Steps: 97. Time per step: 3.1e-02s. Reward per step: -1.18.\n",
            "It has been 214 episode(s) since the model was last saved, with a score of -108 (Δ-13.42).\n",
            "\n",
            "12:02:41 \n",
            "Episode: **216**/4000, Score: -116 (Δ -1.6)\n",
            "Average score: -121.1 (Δ 0.25)\n",
            "Episode time: 5.0s, average: 5.6s (±107.10), ETA: 05:51:20 (01:03:05 to 04::22:27:23)\n",
            "Steps: 120. Time per step: 4.2e-02s. Reward per step: -0.96.\n",
            "It has been 215 episode(s) since the model was last saved, with a score of -108 (Δ-13.17).\n",
            "\n",
            "12:02:46 \n",
            "Episode: **217**/4000, Score: -133 (Δ-17.3)\n",
            "Average score: -121.0 (Δ 0.10)\n",
            "Episode time: 5.0s, average: 5.6s (±106.60), ETA: 05:51:05 (01:03:04 to 04::21:54:18)\n",
            "Steps: 151. Time per step: 3.3e-02s. Reward per step: -0.88.\n",
            "It has been 216 episode(s) since the model was last saved, with a score of -108 (Δ-13.07).\n",
            "\n",
            "12:02:48 \n",
            "Episode: **218**/4000, Score: -121 (Δ 11.7)\n",
            "Average score: -120.8 (Δ 0.19)\n",
            "Episode time: 2.0s, average: 5.6s (±106.17), ETA: 05:49:57 (01:03:03 to 04::21:24:14)\n",
            "Steps: 44. Time per step: 4.5e-02s. Reward per step: -2.76.\n",
            "It has been 217 episode(s) since the model was last saved, with a score of -108 (Δ-12.88).\n",
            "\n",
            "12:02:50 \n",
            "Episode: **219**/4000, Score: -120 (Δ  1.7)\n",
            "Average score: -120.6 (Δ 0.21)\n",
            "Episode time: 2.0s, average: 5.5s (±105.75), ETA: 05:48:51 (01:03:02 to 04::20:54:24)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.78.\n",
            "It has been 218 episode(s) since the model was last saved, with a score of -108 (Δ-12.67).\n",
            "\n",
            "12:02:51 \n",
            "Episode: **220**/4000, Score: -122 (Δ -2.5)\n",
            "Average score: -120.4 (Δ 0.18)\n",
            "Episode time: 1.0s, average: 5.5s (±105.36), ETA: 05:47:27 (01:03:01 to 04::20:26:49)\n",
            "Steps: 44. Time per step: 2.3e-02s. Reward per step: -2.77.\n",
            "It has been 219 episode(s) since the model was last saved, with a score of -108 (Δ-12.49).\n",
            "\n",
            "12:02:53 \n",
            "Episode: **221**/4000, Score: -122 (Δ -0.1)\n",
            "Average score: -120.6 (Δ-0.12)\n",
            "Episode time: 2.0s, average: 5.5s (±104.94), ETA: 05:46:21 (01:03:00 to 04::19:57:26)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.91.\n",
            "It has been 220 episode(s) since the model was last saved, with a score of -108 (Δ-12.61).\n",
            "\n",
            "12:02:55 \n",
            "Episode: **222**/4000, Score: -119 (Δ  2.7)\n",
            "Average score: -120.3 (Δ 0.24)\n",
            "Episode time: 2.0s, average: 5.5s (±104.52), ETA: 05:45:16 (01:02:59 to 04::19:28:17)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.78.\n",
            "It has been 221 episode(s) since the model was last saved, with a score of -108 (Δ-12.36).\n",
            "\n",
            "12:02:56 \n",
            "Episode: **223**/4000, Score: -122 (Δ -2.6)\n",
            "Average score: -120.0 (Δ 0.35)\n",
            "Episode time: 1.0s, average: 5.5s (±104.14), ETA: 05:43:55 (01:02:58 to 04::19:01:19)\n",
            "Steps: 42. Time per step: 2.4e-02s. Reward per step: -2.90.\n",
            "It has been 222 episode(s) since the model was last saved, with a score of -108 (Δ-12.01).\n",
            "\n",
            "12:02:58 \n",
            "Episode: **224**/4000, Score: -122 (Δ -0.3)\n",
            "Average score: -120.1 (Δ-0.11)\n",
            "Episode time: 2.0s, average: 5.4s (±103.73), ETA: 05:42:51 (01:02:57 to 04::18:32:37)\n",
            "Steps: 42. Time per step: 4.8e-02s. Reward per step: -2.91.\n",
            "It has been 223 episode(s) since the model was last saved, with a score of -108 (Δ-12.12).\n",
            "\n",
            "12:03:00 \n",
            "Episode: **225**/4000, Score: -122 (Δ  0.5)\n",
            "Average score: -120.1 (Δ-0.07)\n",
            "Episode time: 2.0s, average: 5.4s (±103.32), ETA: 05:41:48 (01:02:56 to 04::18:04:07)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.83.\n",
            "It has been 224 episode(s) since the model was last saved, with a score of -108 (Δ-12.19).\n",
            "\n",
            "12:03:06 \n",
            "Episode: **226**/4000, Score: -119 (Δ  2.3)\n",
            "Average score: -120.1 (Δ-0.02)\n",
            "Episode time: 6.0s, average: 5.4s (±102.87), ETA: 05:41:52 (01:02:55 to 04::17:33:48)\n",
            "Steps: 157. Time per step: 3.8e-02s. Reward per step: -0.76.\n",
            "It has been 225 episode(s) since the model was last saved, with a score of -108 (Δ-12.20).\n",
            "\n",
            "12:03:07 \n",
            "Episode: **227**/4000, Score: -116 (Δ  3.8)\n",
            "Average score: -120.2 (Δ-0.05)\n",
            "Episode time: 1.0s, average: 5.4s (±102.50), ETA: 05:40:33 (01:02:54 to 04::17:07:41)\n",
            "Steps: 44. Time per step: 2.3e-02s. Reward per step: -2.63.\n",
            "It has been 226 episode(s) since the model was last saved, with a score of -108 (Δ-12.25).\n",
            "\n",
            "12:03:12 \n",
            "Episode: **228**/4000, Score: -131 (Δ-15.7)\n",
            "Average score: -120.4 (Δ-0.16)\n",
            "Episode time: 5.0s, average: 5.4s (±102.05), ETA: 05:40:21 (01:02:53 to 04::16:37:33)\n",
            "Steps: 123. Time per step: 4.1e-02s. Reward per step: -1.07.\n",
            "It has been 227 episode(s) since the model was last saved, with a score of -108 (Δ-12.42).\n",
            "\n",
            "12:03:15 \n",
            "Episode: **229**/4000, Score: -113 (Δ 18.0)\n",
            "Average score: -120.2 (Δ 0.18)\n",
            "Episode time: 3.0s, average: 5.4s (±101.63), ETA: 05:39:35 (01:02:52 to 04::16:08:40)\n",
            "Steps: 92. Time per step: 3.3e-02s. Reward per step: -1.23.\n",
            "It has been 228 episode(s) since the model was last saved, with a score of -108 (Δ-12.23).\n",
            "\n",
            "12:03:21 \n",
            "Episode: **230**/4000, Score: -115 (Δ -2.0)\n",
            "Average score: -120.1 (Δ 0.07)\n",
            "Episode time: 6.0s, average: 5.4s (±101.19), ETA: 05:39:40 (01:02:51 to 04::15:39:22)\n",
            "Steps: 142. Time per step: 4.2e-02s. Reward per step: -0.81.\n",
            "It has been 229 episode(s) since the model was last saved, with a score of -108 (Δ-12.16).\n",
            "\n",
            "12:03:23 \n",
            "Episode: **231**/4000, Score: -116 (Δ -0.8)\n",
            "Average score: -119.5 (Δ 0.59)\n",
            "Episode time: 2.0s, average: 5.4s (±100.80), ETA: 05:38:39 (01:02:50 to 04::15:12:17)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.47.\n",
            "It has been 230 episode(s) since the model was last saved, with a score of -108 (Δ-11.57).\n",
            "\n",
            "12:03:24 \n",
            "Episode: **232**/4000, Score: -117 (Δ -0.6)\n",
            "Average score: -118.9 (Δ 0.64)\n",
            "Episode time: 1.0s, average: 5.4s (±100.45), ETA: 05:37:22 (01:02:49 to 04::14:47:14)\n",
            "Steps: 50. Time per step: 2.0e-02s. Reward per step: -2.33.\n",
            "It has been 231 episode(s) since the model was last saved, with a score of -108 (Δ-10.92).\n",
            "\n",
            "12:03:27 \n",
            "Episode: **233**/4000, Score: -115 (Δ  1.6)\n",
            "Average score: -118.8 (Δ 0.07)\n",
            "Episode time: 3.0s, average: 5.4s (±100.04), ETA: 05:36:38 (01:02:48 to 04::14:19:16)\n",
            "Steps: 61. Time per step: 4.9e-02s. Reward per step: -1.89.\n",
            "It has been 232 episode(s) since the model was last saved, with a score of -108 (Δ-10.86).\n",
            "\n",
            "12:03:31 \n",
            "Episode: **234**/4000, Score: -129 (Δ-13.5)\n",
            "Average score: -118.9 (Δ-0.11)\n",
            "Episode time: 4.0s, average: 5.4s (±99.62), ETA: 05:36:11 (01:02:47 to 04::13:50:47)\n",
            "Steps: 110. Time per step: 3.6e-02s. Reward per step: -1.17.\n",
            "It has been 233 episode(s) since the model was last saved, with a score of -108 (Δ-10.97).\n",
            "\n",
            "12:03:34 \n",
            "Episode: **235**/4000, Score: -114 (Δ 15.0)\n",
            "Average score: -118.9 (Δ 0.04)\n",
            "Episode time: 3.0s, average: 5.3s (±99.22), ETA: 05:35:28 (01:02:46 to 04::13:23:17)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.60.\n",
            "It has been 234 episode(s) since the model was last saved, with a score of -108 (Δ-10.93).\n",
            "\n",
            "12:03:37 \n",
            "Episode: **236**/4000, Score: -126 (Δ-12.8)\n",
            "Average score: -119.0 (Δ-0.11)\n",
            "Episode time: 3.0s, average: 5.3s (±98.82), ETA: 05:34:45 (01:02:45 to 04::12:55:59)\n",
            "Steps: 96. Time per step: 3.1e-02s. Reward per step: -1.32.\n",
            "It has been 235 episode(s) since the model was last saved, with a score of -108 (Δ-11.04).\n",
            "\n",
            "12:03:43 \n",
            "Episode: **237**/4000, Score: -133 (Δ -6.8)\n",
            "Average score: -119.1 (Δ-0.11)\n",
            "Episode time: 6.0s, average: 5.3s (±98.41), ETA: 05:34:51 (01:02:44 to 04::12:28:23)\n",
            "Steps: 148. Time per step: 4.1e-02s. Reward per step: -0.90.\n",
            "It has been 236 episode(s) since the model was last saved, with a score of -108 (Δ-11.15).\n",
            "\n",
            "12:03:45 \n",
            "Episode: **238**/4000, Score: -119 (Δ 14.2)\n",
            "Average score: -119.1 (Δ 0.02)\n",
            "Episode time: 2.0s, average: 5.3s (±98.04), ETA: 05:33:52 (01:02:43 to 04::12:02:46)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.53.\n",
            "It has been 237 episode(s) since the model was last saved, with a score of -108 (Δ-11.13).\n",
            "\n",
            "12:03:50 \n",
            "Episode: **239**/4000, Score: -117 (Δ  1.5)\n",
            "Average score: -119.1 (Δ-0.02)\n",
            "Episode time: 5.0s, average: 5.3s (±97.63), ETA: 05:33:42 (01:02:42 to 04::11:35:16)\n",
            "Steps: 155. Time per step: 3.2e-02s. Reward per step: -0.76.\n",
            "It has been 238 episode(s) since the model was last saved, with a score of -108 (Δ-11.15).\n",
            "\n",
            "12:03:52 \n",
            "Episode: **240**/4000, Score: -117 (Δ  0.6)\n",
            "Average score: -119.0 (Δ 0.06)\n",
            "Episode time: 2.0s, average: 5.3s (±97.27), ETA: 05:32:45 (01:02:41 to 04::11:10:03)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.43.\n",
            "It has been 239 episode(s) since the model was last saved, with a score of -108 (Δ-11.10).\n",
            "\n",
            "12:04:48 \n",
            "Episode: **241**/4000, Score: -245 (Δ-128.5)\n",
            "Average score: -120.3 (Δ-1.30)\n",
            "Episode time: 56.0s, average: 5.5s (±107.49), ETA: 05:45:50 (01:02:40 to 04::22:01:38)\n",
            "Steps: 1476. Time per step: 3.8e-02s. Reward per step: -0.17.\n",
            "It has been 240 episode(s) since the model was last saved, with a score of -108 (Δ-12.40).\n",
            "\n",
            "12:04:50 \n",
            "Episode: **242**/4000, Score: -115 (Δ130.4)\n",
            "Average score: -120.3 (Δ 0.01)\n",
            "Episode time: 2.0s, average: 5.5s (±107.09), ETA: 05:44:50 (01:02:39 to 04::21:34:12)\n",
            "Steps: 74. Time per step: 2.7e-02s. Reward per step: -1.55.\n",
            "It has been 241 episode(s) since the model was last saved, with a score of -108 (Δ-12.39).\n",
            "\n",
            "12:04:52 \n",
            "Episode: **243**/4000, Score: -116 (Δ -0.8)\n",
            "Average score: -120.3 (Δ-0.01)\n",
            "Episode time: 2.0s, average: 5.5s (±106.70), ETA: 05:43:50 (01:02:38 to 04::21:06:59)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.69.\n",
            "It has been 242 episode(s) since the model was last saved, with a score of -108 (Δ-12.40).\n",
            "\n",
            "12:04:54 \n",
            "Episode: **244**/4000, Score: -117 (Δ -1.8)\n",
            "Average score: -120.4 (Δ-0.01)\n",
            "Episode time: 2.0s, average: 5.5s (±106.31), ETA: 05:42:51 (01:02:37 to 04::20:39:56)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -2.03.\n",
            "It has been 243 episode(s) since the model was last saved, with a score of -108 (Δ-12.41).\n",
            "\n",
            "12:04:59 \n",
            "Episode: **245**/4000, Score: -127 (Δ -9.9)\n",
            "Average score: -120.5 (Δ-0.15)\n",
            "Episode time: 5.0s, average: 5.5s (±105.88), ETA: 05:42:38 (01:02:36 to 04::20:10:51)\n",
            "Steps: 126. Time per step: 4.0e-02s. Reward per step: -1.01.\n",
            "It has been 244 episode(s) since the model was last saved, with a score of -108 (Δ-12.56).\n",
            "\n",
            "12:05:01 \n",
            "Episode: **246**/4000, Score: -116 (Δ 11.0)\n",
            "Average score: -120.5 (Δ 0.02)\n",
            "Episode time: 2.0s, average: 5.5s (±105.50), ETA: 05:41:40 (01:02:35 to 04::19:44:14)\n",
            "Steps: 61. Time per step: 3.3e-02s. Reward per step: -1.91.\n",
            "It has been 245 episode(s) since the model was last saved, with a score of -108 (Δ-12.54).\n",
            "\n",
            "12:05:03 \n",
            "Episode: **247**/4000, Score: -113 (Δ  3.1)\n",
            "Average score: -120.4 (Δ 0.09)\n",
            "Episode time: 2.0s, average: 5.4s (±105.12), ETA: 05:40:42 (01:02:34 to 04::19:17:48)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -2.06.\n",
            "It has been 246 episode(s) since the model was last saved, with a score of -108 (Δ-12.45).\n",
            "\n",
            "12:05:06 \n",
            "Episode: **248**/4000, Score: -117 (Δ -4.2)\n",
            "Average score: -120.4 (Δ 0.02)\n",
            "Episode time: 3.0s, average: 5.4s (±104.72), ETA: 05:39:59 (01:02:33 to 04::18:50:20)\n",
            "Steps: 60. Time per step: 5.0e-02s. Reward per step: -1.96.\n",
            "It has been 247 episode(s) since the model was last saved, with a score of -108 (Δ-12.43).\n",
            "\n",
            "12:05:08 \n",
            "Episode: **249**/4000, Score: -113 (Δ  4.5)\n",
            "Average score: -120.3 (Δ 0.05)\n",
            "Episode time: 2.0s, average: 5.4s (±104.35), ETA: 05:39:02 (01:02:32 to 04::18:24:17)\n",
            "Steps: 59. Time per step: 3.4e-02s. Reward per step: -1.91.\n",
            "It has been 248 episode(s) since the model was last saved, with a score of -108 (Δ-12.38).\n",
            "\n",
            "12:05:10 \n",
            "Episode: **250**/4000, Score: -109 (Δ  4.2)\n",
            "Average score: -120.2 (Δ 0.09)\n",
            "Episode time: 2.0s, average: 5.4s (±103.98), ETA: 05:38:05 (01:02:31 to 04::17:58:25)\n",
            "Steps: 70. Time per step: 2.9e-02s. Reward per step: -1.55.\n",
            "It has been 249 episode(s) since the model was last saved, with a score of -108 (Δ-12.30).\n",
            "\n",
            "12:05:12 \n",
            "Episode: **251**/4000, Score: -115 (Δ -5.8)\n",
            "Average score: -120.2 (Δ 0.07)\n",
            "Episode time: 2.0s, average: 5.4s (±103.61), ETA: 05:37:09 (01:02:30 to 04::17:32:44)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.66.\n",
            "It has been 250 episode(s) since the model was last saved, with a score of -108 (Δ-12.23).\n",
            "\n",
            "12:05:14 \n",
            "Episode: **252**/4000, Score: -117 (Δ -2.0)\n",
            "Average score: -120.1 (Δ 0.06)\n",
            "Episode time: 2.0s, average: 5.4s (±103.24), ETA: 05:36:13 (01:02:29 to 04::17:07:14)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -2.01.\n",
            "It has been 251 episode(s) since the model was last saved, with a score of -108 (Δ-12.17).\n",
            "\n",
            "12:05:16 \n",
            "Episode: **253**/4000, Score: -116 (Δ  0.1)\n",
            "Average score: -120.2 (Δ-0.05)\n",
            "Episode time: 2.0s, average: 5.4s (±102.88), ETA: 05:35:18 (01:02:28 to 04::16:41:55)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -2.20.\n",
            "It has been 252 episode(s) since the model was last saved, with a score of -108 (Δ-12.22).\n",
            "\n",
            "12:05:19 \n",
            "Episode: **254**/4000, Score: -119 (Δ -2.2)\n",
            "Average score: -120.2 (Δ-0.02)\n",
            "Episode time: 3.0s, average: 5.4s (±102.50), ETA: 05:34:37 (01:02:27 to 04::16:15:36)\n",
            "Steps: 66. Time per step: 4.5e-02s. Reward per step: -1.80.\n",
            "It has been 253 episode(s) since the model was last saved, with a score of -108 (Δ-12.24).\n",
            "\n",
            "12:05:21 \n",
            "Episode: **255**/4000, Score: -111 (Δ  7.1)\n",
            "Average score: -120.1 (Δ 0.11)\n",
            "Episode time: 2.0s, average: 5.3s (±102.14), ETA: 05:33:43 (01:02:26 to 04::15:50:38)\n",
            "Steps: 64. Time per step: 3.1e-02s. Reward per step: -1.74.\n",
            "It has been 254 episode(s) since the model was last saved, with a score of -108 (Δ-12.13).\n",
            "\n",
            "12:05:24 \n",
            "Episode: **256**/4000, Score: -114 (Δ -2.8)\n",
            "Average score: -120.0 (Δ 0.04)\n",
            "Episode time: 3.0s, average: 5.3s (±101.76), ETA: 05:33:03 (01:02:25 to 04::15:24:42)\n",
            "Steps: 66. Time per step: 4.5e-02s. Reward per step: -1.73.\n",
            "It has been 255 episode(s) since the model was last saved, with a score of -108 (Δ-12.09).\n",
            "\n",
            "12:05:26 \n",
            "Episode: **257**/4000, Score: -118 (Δ -3.7)\n",
            "Average score: -120.0 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 5.3s (±101.41), ETA: 05:32:09 (01:02:24 to 04::15:00:06)\n",
            "Steps: 57. Time per step: 3.5e-02s. Reward per step: -2.07.\n",
            "It has been 256 episode(s) since the model was last saved, with a score of -108 (Δ-12.05).\n",
            "\n",
            "12:05:29 \n",
            "Episode: **258**/4000, Score: -117 (Δ  1.4)\n",
            "Average score: -120.0 (Δ-0.02)\n",
            "Episode time: 3.0s, average: 5.3s (±101.04), ETA: 05:31:30 (01:02:23 to 04::14:34:32)\n",
            "Steps: 73. Time per step: 4.1e-02s. Reward per step: -1.60.\n",
            "It has been 257 episode(s) since the model was last saved, with a score of -108 (Δ-12.07).\n",
            "\n",
            "12:05:31 \n",
            "Episode: **259**/4000, Score: -114 (Δ  3.1)\n",
            "Average score: -120.0 (Δ 0.02)\n",
            "Episode time: 2.0s, average: 5.3s (±100.69), ETA: 05:30:37 (01:02:22 to 04::14:10:16)\n",
            "Steps: 63. Time per step: 3.2e-02s. Reward per step: -1.80.\n",
            "It has been 258 episode(s) since the model was last saved, with a score of -108 (Δ-12.05).\n",
            "\n",
            "12:05:38 \n",
            "Episode: **260**/4000, Score: -123 (Δ -9.3)\n",
            "Average score: -120.1 (Δ-0.09)\n",
            "Episode time: 7.0s, average: 5.3s (±100.31), ETA: 05:30:56 (01:02:21 to 04::13:45:27)\n",
            "Steps: 190. Time per step: 3.7e-02s. Reward per step: -0.65.\n",
            "It has been 259 episode(s) since the model was last saved, with a score of -108 (Δ-12.14).\n",
            "\n",
            "12:05:43 \n",
            "Episode: **261**/4000, Score: -134 (Δ-11.3)\n",
            "Average score: -120.2 (Δ-0.15)\n",
            "Episode time: 5.0s, average: 5.3s (±99.93), ETA: 05:30:46 (01:02:20 to 04::13:19:41)\n",
            "Steps: 130. Time per step: 3.8e-02s. Reward per step: -1.03.\n",
            "It has been 260 episode(s) since the model was last saved, with a score of -108 (Δ-12.29).\n",
            "\n",
            "12:05:49 \n",
            "Episode: **262**/4000, Score: -118 (Δ 16.5)\n",
            "Average score: -120.3 (Δ-0.06)\n",
            "Episode time: 6.0s, average: 5.3s (±99.55), ETA: 05:30:51 (01:02:19 to 04::12:54:26)\n",
            "Steps: 170. Time per step: 3.5e-02s. Reward per step: -0.69.\n",
            "It has been 261 episode(s) since the model was last saved, with a score of -108 (Δ-12.35).\n",
            "\n",
            "12:05:54 \n",
            "Episode: **263**/4000, Score: -115 (Δ  2.4)\n",
            "Average score: -120.3 (Δ 0.03)\n",
            "Episode time: 5.0s, average: 5.3s (±99.17), ETA: 05:30:41 (01:02:18 to 04::12:29:04)\n",
            "Steps: 126. Time per step: 4.0e-02s. Reward per step: -0.91.\n",
            "It has been 262 episode(s) since the model was last saved, with a score of -108 (Δ-12.32).\n",
            "\n",
            "12:05:57 \n",
            "Episode: **264**/4000, Score: -121 (Δ -6.0)\n",
            "Average score: -120.3 (Δ-0.01)\n",
            "Episode time: 3.0s, average: 5.3s (±98.82), ETA: 05:30:03 (01:02:17 to 04::12:04:38)\n",
            "Steps: 73. Time per step: 4.1e-02s. Reward per step: -1.66.\n",
            "It has been 263 episode(s) since the model was last saved, with a score of -108 (Δ-12.33).\n",
            "\n",
            "12:05:59 \n",
            "Episode: **265**/4000, Score: -121 (Δ  0.7)\n",
            "Average score: -120.3 (Δ-0.00)\n",
            "Episode time: 2.0s, average: 5.3s (±98.48), ETA: 05:29:11 (01:02:16 to 04::11:41:27)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.95.\n",
            "It has been 264 episode(s) since the model was last saved, with a score of -108 (Δ-12.34).\n",
            "\n",
            "12:06:02 \n",
            "Episode: **266**/4000, Score: -122 (Δ -1.4)\n",
            "Average score: -120.3 (Δ-0.07)\n",
            "Episode time: 3.0s, average: 5.3s (±98.13), ETA: 05:28:34 (01:02:15 to 04::11:17:21)\n",
            "Steps: 74. Time per step: 4.1e-02s. Reward per step: -1.65.\n",
            "It has been 265 episode(s) since the model was last saved, with a score of -108 (Δ-12.40).\n",
            "\n",
            "12:06:04 \n",
            "Episode: **267**/4000, Score: -113 (Δ  8.7)\n",
            "Average score: -120.3 (Δ 0.07)\n",
            "Episode time: 2.0s, average: 5.3s (±97.81), ETA: 05:27:43 (01:02:14 to 04::10:54:29)\n",
            "Steps: 74. Time per step: 2.7e-02s. Reward per step: -1.53.\n",
            "It has been 266 episode(s) since the model was last saved, with a score of -108 (Δ-12.33).\n",
            "\n",
            "12:06:12 \n",
            "Episode: **268**/4000, Score: -122 (Δ -8.8)\n",
            "Average score: -120.3 (Δ 0.00)\n",
            "Episode time: 8.0s, average: 5.3s (±97.47), ETA: 05:28:16 (01:02:13 to 04::10:32:26)\n",
            "Steps: 203. Time per step: 3.9e-02s. Reward per step: -0.60.\n",
            "It has been 267 episode(s) since the model was last saved, with a score of -108 (Δ-12.33).\n",
            "\n",
            "12:06:16 \n",
            "Episode: **269**/4000, Score: -126 (Δ -4.2)\n",
            "Average score: -120.4 (Δ-0.09)\n",
            "Episode time: 4.0s, average: 5.3s (±97.11), ETA: 05:27:53 (01:02:12 to 04::10:08:16)\n",
            "Steps: 93. Time per step: 4.3e-02s. Reward per step: -1.36.\n",
            "It has been 268 episode(s) since the model was last saved, with a score of -108 (Δ-12.42).\n",
            "\n",
            "12:06:18 \n",
            "Episode: **270**/4000, Score: -121 (Δ  5.2)\n",
            "Average score: -120.5 (Δ-0.10)\n",
            "Episode time: 2.0s, average: 5.3s (±96.79), ETA: 05:27:02 (01:02:11 to 04::09:45:53)\n",
            "Steps: 70. Time per step: 2.9e-02s. Reward per step: -1.73.\n",
            "It has been 269 episode(s) since the model was last saved, with a score of -108 (Δ-12.51).\n",
            "\n",
            "12:06:21 \n",
            "Episode: **271**/4000, Score: -124 (Δ -3.1)\n",
            "Average score: -120.5 (Δ-0.02)\n",
            "Episode time: 3.0s, average: 5.3s (±96.45), ETA: 05:26:26 (01:02:10 to 04::09:22:38)\n",
            "Steps: 85. Time per step: 3.5e-02s. Reward per step: -1.46.\n",
            "It has been 270 episode(s) since the model was last saved, with a score of -108 (Δ-12.53).\n",
            "\n",
            "12:06:24 \n",
            "Episode: **272**/4000, Score: -118 (Δ  6.1)\n",
            "Average score: -120.4 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 5.2s (±96.12), ETA: 05:25:50 (01:02:09 to 04::08:59:32)\n",
            "Steps: 69. Time per step: 4.3e-02s. Reward per step: -1.71.\n",
            "It has been 271 episode(s) since the model was last saved, with a score of -108 (Δ-12.45).\n",
            "\n",
            "12:06:26 \n",
            "Episode: **273**/4000, Score: -115 (Δ  3.5)\n",
            "Average score: -120.3 (Δ 0.06)\n",
            "Episode time: 2.0s, average: 5.2s (±95.80), ETA: 05:25:00 (01:02:08 to 04::08:37:37)\n",
            "Steps: 63. Time per step: 3.2e-02s. Reward per step: -1.82.\n",
            "It has been 272 episode(s) since the model was last saved, with a score of -108 (Δ-12.39).\n",
            "\n",
            "12:06:30 \n",
            "Episode: **274**/4000, Score: -127 (Δ-13.0)\n",
            "Average score: -120.4 (Δ-0.08)\n",
            "Episode time: 4.0s, average: 5.2s (±95.46), ETA: 05:24:38 (01:02:07 to 04::08:14:17)\n",
            "Steps: 93. Time per step: 4.3e-02s. Reward per step: -1.37.\n",
            "It has been 273 episode(s) since the model was last saved, with a score of -108 (Δ-12.46).\n",
            "\n",
            "12:06:33 \n",
            "Episode: **275**/4000, Score: -125 (Δ  2.2)\n",
            "Average score: -120.5 (Δ-0.06)\n",
            "Episode time: 3.0s, average: 5.2s (±95.13), ETA: 05:24:03 (01:02:06 to 04::07:51:39)\n",
            "Steps: 88. Time per step: 3.4e-02s. Reward per step: -1.42.\n",
            "It has been 274 episode(s) since the model was last saved, with a score of -108 (Δ-12.52).\n",
            "\n",
            "12:06:36 \n",
            "Episode: **276**/4000, Score: -127 (Δ -1.9)\n",
            "Average score: -120.6 (Δ-0.16)\n",
            "Episode time: 3.0s, average: 5.2s (±94.80), ETA: 05:23:28 (01:02:05 to 04::07:29:11)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.53.\n",
            "It has been 275 episode(s) since the model was last saved, with a score of -108 (Δ-12.67).\n",
            "\n",
            "12:06:39 \n",
            "Episode: **277**/4000, Score: -119 (Δ  8.2)\n",
            "Average score: -120.6 (Δ 0.01)\n",
            "Episode time: 3.0s, average: 5.2s (±94.48), ETA: 05:22:53 (01:02:04 to 04::07:06:53)\n",
            "Steps: 75. Time per step: 4.0e-02s. Reward per step: -1.59.\n",
            "It has been 276 episode(s) since the model was last saved, with a score of -108 (Δ-12.67).\n",
            "\n",
            "12:06:42 \n",
            "Episode: **278**/4000, Score: -118 (Δ  0.9)\n",
            "Average score: -120.7 (Δ-0.04)\n",
            "Episode time: 3.0s, average: 5.2s (±94.16), ETA: 05:22:18 (01:02:03 to 04::06:44:43)\n",
            "Steps: 65. Time per step: 4.6e-02s. Reward per step: -1.82.\n",
            "It has been 277 episode(s) since the model was last saved, with a score of -108 (Δ-12.71).\n",
            "\n",
            "12:06:44 \n",
            "Episode: **279**/4000, Score: -117 (Δ  1.0)\n",
            "Average score: -120.6 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 5.2s (±93.86), ETA: 05:21:30 (01:02:02 to 04::06:23:40)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -2.09.\n",
            "It has been 278 episode(s) since the model was last saved, with a score of -108 (Δ-12.67).\n",
            "\n",
            "12:06:48 \n",
            "Episode: **280**/4000, Score: -134 (Δ-17.1)\n",
            "Average score: -120.8 (Δ-0.17)\n",
            "Episode time: 4.0s, average: 5.2s (±93.53), ETA: 05:21:09 (01:02:01 to 04::06:01:17)\n",
            "Steps: 103. Time per step: 3.9e-02s. Reward per step: -1.30.\n",
            "It has been 279 episode(s) since the model was last saved, with a score of -108 (Δ-12.83).\n",
            "\n",
            "12:06:52 \n",
            "Episode: **281**/4000, Score: -114 (Δ 20.5)\n",
            "Average score: -120.7 (Δ 0.08)\n",
            "Episode time: 4.0s, average: 5.2s (±93.20), ETA: 05:20:49 (01:02:00 to 04::05:39:03)\n",
            "Steps: 115. Time per step: 3.5e-02s. Reward per step: -0.99.\n",
            "It has been 280 episode(s) since the model was last saved, with a score of -108 (Δ-12.75).\n",
            "\n",
            "12:06:56 \n",
            "Episode: **282**/4000, Score: -132 (Δ-18.6)\n",
            "Average score: -120.9 (Δ-0.21)\n",
            "Episode time: 4.0s, average: 5.2s (±92.87), ETA: 05:20:28 (01:01:59 to 04::05:16:58)\n",
            "Steps: 98. Time per step: 4.1e-02s. Reward per step: -1.35.\n",
            "It has been 281 episode(s) since the model was last saved, with a score of -108 (Δ-12.97).\n",
            "\n",
            "12:06:59 \n",
            "Episode: **283**/4000, Score: -120 (Δ 12.3)\n",
            "Average score: -120.9 (Δ 0.03)\n",
            "Episode time: 3.0s, average: 5.2s (±92.56), ETA: 05:19:54 (01:01:58 to 04::04:55:33)\n",
            "Steps: 75. Time per step: 4.0e-02s. Reward per step: -1.60.\n",
            "It has been 282 episode(s) since the model was last saved, with a score of -108 (Δ-12.94).\n",
            "\n",
            "12:07:02 \n",
            "Episode: **284**/4000, Score: -127 (Δ -7.2)\n",
            "Average score: -121.0 (Δ-0.08)\n",
            "Episode time: 3.0s, average: 5.2s (±92.25), ETA: 05:19:21 (01:01:57 to 04::04:34:17)\n",
            "Steps: 91. Time per step: 3.3e-02s. Reward per step: -1.40.\n",
            "It has been 283 episode(s) since the model was last saved, with a score of -108 (Δ-13.02).\n",
            "\n",
            "12:07:06 \n",
            "Episode: **285**/4000, Score: -130 (Δ -2.3)\n",
            "Average score: -121.1 (Δ-0.13)\n",
            "Episode time: 4.0s, average: 5.2s (±91.93), ETA: 05:19:01 (01:01:56 to 04::04:12:39)\n",
            "Steps: 108. Time per step: 3.7e-02s. Reward per step: -1.20.\n",
            "It has been 284 episode(s) since the model was last saved, with a score of -108 (Δ-13.15).\n",
            "\n",
            "12:07:10 \n",
            "Episode: **286**/4000, Score: -129 (Δ  0.5)\n",
            "Average score: -121.2 (Δ-0.07)\n",
            "Episode time: 4.0s, average: 5.1s (±91.61), ETA: 05:18:41 (01:01:55 to 04::03:51:10)\n",
            "Steps: 92. Time per step: 4.3e-02s. Reward per step: -1.40.\n",
            "It has been 285 episode(s) since the model was last saved, with a score of -108 (Δ-13.21).\n",
            "\n",
            "12:07:13 \n",
            "Episode: **287**/4000, Score: -127 (Δ  2.2)\n",
            "Average score: -121.3 (Δ-0.14)\n",
            "Episode time: 3.0s, average: 5.1s (±91.31), ETA: 05:18:08 (01:01:54 to 04::03:30:19)\n",
            "Steps: 93. Time per step: 3.2e-02s. Reward per step: -1.36.\n",
            "It has been 286 episode(s) since the model was last saved, with a score of -108 (Δ-13.35).\n",
            "\n",
            "12:07:18 \n",
            "Episode: **288**/4000, Score: -132 (Δ -5.6)\n",
            "Average score: -121.4 (Δ-0.10)\n",
            "Episode time: 5.0s, average: 5.1s (±90.99), ETA: 05:18:01 (01:01:53 to 04::03:09:04)\n",
            "Steps: 125. Time per step: 4.0e-02s. Reward per step: -1.06.\n",
            "It has been 287 episode(s) since the model was last saved, with a score of -108 (Δ-13.46).\n",
            "\n",
            "12:07:22 \n",
            "Episode: **289**/4000, Score: -133 (Δ -0.9)\n",
            "Average score: -121.6 (Δ-0.16)\n",
            "Episode time: 4.0s, average: 5.1s (±90.68), ETA: 05:17:41 (01:01:52 to 04::02:48:01)\n",
            "Steps: 105. Time per step: 3.8e-02s. Reward per step: -1.27.\n",
            "It has been 288 episode(s) since the model was last saved, with a score of -108 (Δ-13.61).\n",
            "\n",
            "12:07:25 \n",
            "Episode: **290**/4000, Score: -125 (Δ  8.5)\n",
            "Average score: -121.7 (Δ-0.13)\n",
            "Episode time: 3.0s, average: 5.1s (±90.39), ETA: 05:17:08 (01:01:51 to 04::02:27:35)\n",
            "Steps: 76. Time per step: 3.9e-02s. Reward per step: -1.64.\n",
            "It has been 289 episode(s) since the model was last saved, with a score of -108 (Δ-13.74).\n",
            "\n",
            "12:07:30 \n",
            "Episode: **291**/4000, Score: -116 (Δ  9.0)\n",
            "Average score: -121.6 (Δ 0.06)\n",
            "Episode time: 5.0s, average: 5.1s (±90.08), ETA: 05:17:02 (01:01:50 to 04::02:06:46)\n",
            "Steps: 132. Time per step: 3.8e-02s. Reward per step: -0.88.\n",
            "It has been 290 episode(s) since the model was last saved, with a score of -108 (Δ-13.68).\n",
            "\n",
            "12:07:33 \n",
            "Episode: **292**/4000, Score: -121 (Δ -5.6)\n",
            "Average score: -121.7 (Δ-0.04)\n",
            "Episode time: 3.0s, average: 5.1s (±89.78), ETA: 05:16:30 (01:01:49 to 04::01:46:37)\n",
            "Steps: 89. Time per step: 3.4e-02s. Reward per step: -1.36.\n",
            "It has been 291 episode(s) since the model was last saved, with a score of -108 (Δ-13.72).\n",
            "\n",
            "12:07:36 \n",
            "Episode: **293**/4000, Score: -121 (Δ  0.4)\n",
            "Average score: -121.7 (Δ-0.00)\n",
            "Episode time: 3.0s, average: 5.1s (±89.49), ETA: 05:15:58 (01:01:48 to 04::01:26:36)\n",
            "Steps: 72. Time per step: 4.2e-02s. Reward per step: -1.68.\n",
            "It has been 292 episode(s) since the model was last saved, with a score of -108 (Δ-13.72).\n",
            "\n",
            "12:07:41 \n",
            "Episode: **294**/4000, Score: -124 (Δ -3.2)\n",
            "Average score: -121.7 (Δ-0.02)\n",
            "Episode time: 5.0s, average: 5.1s (±89.19), ETA: 05:15:51 (01:01:47 to 04::01:06:11)\n",
            "Steps: 132. Time per step: 3.8e-02s. Reward per step: -0.94.\n",
            "It has been 293 episode(s) since the model was last saved, with a score of -108 (Δ-13.74).\n",
            "\n",
            "12:07:45 \n",
            "Episode: **295**/4000, Score: -117 (Δ  7.2)\n",
            "Average score: -121.7 (Δ 0.03)\n",
            "Episode time: 4.0s, average: 5.1s (±88.89), ETA: 05:15:32 (01:01:46 to 04::00:45:58)\n",
            "Steps: 105. Time per step: 3.8e-02s. Reward per step: -1.11.\n",
            "It has been 294 episode(s) since the model was last saved, with a score of -108 (Δ-13.71).\n",
            "\n",
            "12:07:50 \n",
            "Episode: **296**/4000, Score: -134 (Δ-17.3)\n",
            "Average score: -121.8 (Δ-0.12)\n",
            "Episode time: 5.0s, average: 5.1s (±88.59), ETA: 05:15:26 (01:01:45 to 04::00:25:50)\n",
            "Steps: 131. Time per step: 3.8e-02s. Reward per step: -1.02.\n",
            "It has been 295 episode(s) since the model was last saved, with a score of -108 (Δ-13.83).\n",
            "\n",
            "12:07:53 \n",
            "Episode: **297**/4000, Score: -120 (Δ 14.0)\n",
            "Average score: -121.9 (Δ-0.08)\n",
            "Episode time: 3.0s, average: 5.1s (±88.31), ETA: 05:14:54 (01:01:44 to 04::00:06:21)\n",
            "Steps: 98. Time per step: 3.1e-02s. Reward per step: -1.23.\n",
            "It has been 296 episode(s) since the model was last saved, with a score of -108 (Δ-13.91).\n",
            "\n",
            "12:07:59 \n",
            "Episode: **298**/4000, Score: -136 (Δ-15.4)\n",
            "Average score: -122.1 (Δ-0.22)\n",
            "Episode time: 6.0s, average: 5.1s (±88.01), ETA: 05:15:00 (01:01:43 to 03::23:46:51)\n",
            "Steps: 156. Time per step: 3.8e-02s. Reward per step: -0.87.\n",
            "It has been 297 episode(s) since the model was last saved, with a score of -108 (Δ-14.13).\n",
            "\n",
            "12:08:04 \n",
            "Episode: **299**/4000, Score: -131 (Δ  4.4)\n",
            "Average score: -122.2 (Δ-0.09)\n",
            "Episode time: 5.0s, average: 5.1s (±87.72), ETA: 05:14:54 (01:01:42 to 03::23:27:07)\n",
            "Steps: 124. Time per step: 4.0e-02s. Reward per step: -1.06.\n",
            "It has been 298 episode(s) since the model was last saved, with a score of -108 (Δ-14.22).\n",
            "\n",
            "12:08:10 \n",
            "Episode: **300**/4000, Score: -136 (Δ -4.6)\n",
            "Average score: -122.3 (Δ-0.13)\n",
            "Episode time: 6.0s, average: 5.1s (±87.43), ETA: 05:15:00 (01:01:41 to 03::23:07:53)\n",
            "Steps: 162. Time per step: 3.7e-02s. Reward per step: -0.84.\n",
            "It has been 299 episode(s) since the model was last saved, with a score of -108 (Δ-14.35).\n",
            "\n",
            "12:08:15 \n",
            "Episode: **301**/4000, Score: -135 (Δ  1.0)\n",
            "Average score: -122.5 (Δ-0.22)\n",
            "Episode time: 5.0s, average: 5.1s (±87.14), ETA: 05:14:53 (01:01:40 to 03::22:48:25)\n",
            "Steps: 136. Time per step: 3.7e-02s. Reward per step: -0.99.\n",
            "It has been 300 episode(s) since the model was last saved, with a score of -108 (Δ-14.57).\n",
            "\n",
            "12:08:19 \n",
            "Episode: **302**/4000, Score: -120 (Δ 14.7)\n",
            "Average score: -122.5 (Δ 0.01)\n",
            "Episode time: 4.0s, average: 5.1s (±86.85), ETA: 05:14:35 (01:01:39 to 03::22:29:07)\n",
            "Steps: 90. Time per step: 4.4e-02s. Reward per step: -1.33.\n",
            "It has been 301 episode(s) since the model was last saved, with a score of -108 (Δ-14.56).\n",
            "\n",
            "12:08:24 \n",
            "Episode: **303**/4000, Score: -123 (Δ -2.5)\n",
            "Average score: -122.5 (Δ-0.01)\n",
            "Episode time: 5.0s, average: 5.1s (±86.57), ETA: 05:14:28 (01:01:38 to 03::22:09:53)\n",
            "Steps: 143. Time per step: 3.5e-02s. Reward per step: -0.86.\n",
            "It has been 302 episode(s) since the model was last saved, with a score of -108 (Δ-14.56).\n",
            "\n",
            "12:08:27 \n",
            "Episode: **304**/4000, Score: -113 (Δ  9.6)\n",
            "Average score: -122.4 (Δ 0.07)\n",
            "Episode time: 3.0s, average: 5.1s (±86.30), ETA: 05:13:58 (01:01:37 to 03::21:51:17)\n",
            "Steps: 91. Time per step: 3.3e-02s. Reward per step: -1.24.\n",
            "It has been 303 episode(s) since the model was last saved, with a score of -108 (Δ-14.49).\n",
            "\n",
            "12:08:31 \n",
            "Episode: **305**/4000, Score: -121 (Δ -8.2)\n",
            "Average score: -122.4 (Δ-0.01)\n",
            "Episode time: 4.0s, average: 5.1s (±86.02), ETA: 05:13:39 (01:01:36 to 03::21:32:21)\n",
            "Steps: 89. Time per step: 4.5e-02s. Reward per step: -1.36.\n",
            "It has been 304 episode(s) since the model was last saved, with a score of -108 (Δ-14.50).\n",
            "\n",
            "12:08:34 \n",
            "Episode: **306**/4000, Score: -115 (Δ  6.3)\n",
            "Average score: -122.4 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 5.1s (±85.75), ETA: 05:13:09 (01:01:35 to 03::21:13:59)\n",
            "Steps: 76. Time per step: 3.9e-02s. Reward per step: -1.51.\n",
            "It has been 305 episode(s) since the model was last saved, with a score of -108 (Δ-14.42).\n",
            "\n",
            "12:08:36 \n",
            "Episode: **307**/4000, Score: -116 (Δ -1.6)\n",
            "Average score: -122.4 (Δ 0.01)\n",
            "Episode time: 2.0s, average: 5.1s (±85.50), ETA: 05:12:27 (01:01:34 to 03::20:56:33)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -2.08.\n",
            "It has been 306 episode(s) since the model was last saved, with a score of -108 (Δ-14.41).\n",
            "\n",
            "12:08:38 \n",
            "Episode: **308**/4000, Score: -116 (Δ  0.7)\n",
            "Average score: -122.3 (Δ 0.06)\n",
            "Episode time: 2.0s, average: 5.1s (±85.26), ETA: 05:11:45 (01:01:33 to 03::20:39:13)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.99.\n",
            "It has been 307 episode(s) since the model was last saved, with a score of -108 (Δ-14.35).\n",
            "\n",
            "12:08:42 \n",
            "Episode: **309**/4000, Score: -121 (Δ -5.4)\n",
            "Average score: -122.3 (Δ-0.03)\n",
            "Episode time: 4.0s, average: 5.1s (±84.98), ETA: 05:11:27 (01:01:32 to 03::20:20:45)\n",
            "Steps: 100. Time per step: 4.0e-02s. Reward per step: -1.21.\n",
            "It has been 308 episode(s) since the model was last saved, with a score of -108 (Δ-14.38).\n",
            "\n",
            "12:08:45 \n",
            "Episode: **310**/4000, Score: -119 (Δ  2.4)\n",
            "Average score: -122.3 (Δ 0.02)\n",
            "Episode time: 3.0s, average: 5.1s (±84.72), ETA: 05:10:57 (01:01:31 to 03::20:02:49)\n",
            "Steps: 72. Time per step: 4.2e-02s. Reward per step: -1.65.\n",
            "It has been 309 episode(s) since the model was last saved, with a score of -108 (Δ-14.35).\n",
            "\n",
            "12:08:48 \n",
            "Episode: **311**/4000, Score: -122 (Δ -3.1)\n",
            "Average score: -122.3 (Δ-0.01)\n",
            "Episode time: 3.0s, average: 5.0s (±84.46), ETA: 05:10:28 (01:01:30 to 03::19:45:00)\n",
            "Steps: 87. Time per step: 3.4e-02s. Reward per step: -1.40.\n",
            "It has been 310 episode(s) since the model was last saved, with a score of -108 (Δ-14.37).\n",
            "\n",
            "12:08:51 \n",
            "Episode: **312**/4000, Score: -116 (Δ  5.7)\n",
            "Average score: -122.2 (Δ 0.06)\n",
            "Episode time: 3.0s, average: 5.0s (±84.21), ETA: 05:09:59 (01:01:29 to 03::19:27:17)\n",
            "Steps: 76. Time per step: 3.9e-02s. Reward per step: -1.53.\n",
            "It has been 311 episode(s) since the model was last saved, with a score of -108 (Δ-14.31).\n",
            "\n",
            "12:08:54 \n",
            "Episode: **313**/4000, Score: -120 (Δ -4.1)\n",
            "Average score: -122.2 (Δ 0.03)\n",
            "Episode time: 3.0s, average: 5.0s (±83.95), ETA: 05:09:30 (01:01:28 to 03::19:09:40)\n",
            "Steps: 93. Time per step: 3.2e-02s. Reward per step: -1.29.\n",
            "It has been 312 episode(s) since the model was last saved, with a score of -108 (Δ-14.28).\n",
            "\n",
            "12:08:59 \n",
            "Episode: **314**/4000, Score: -133 (Δ-12.8)\n",
            "Average score: -122.4 (Δ-0.14)\n",
            "Episode time: 5.0s, average: 5.0s (±83.68), ETA: 05:09:24 (01:01:27 to 03::18:51:45)\n",
            "Steps: 125. Time per step: 4.0e-02s. Reward per step: -1.06.\n",
            "It has been 313 episode(s) since the model was last saved, with a score of -108 (Δ-14.42).\n",
            "\n",
            "12:09:02 \n",
            "Episode: **315**/4000, Score: -113 (Δ 20.2)\n",
            "Average score: -122.4 (Δ 0.01)\n",
            "Episode time: 3.0s, average: 5.0s (±83.43), ETA: 05:08:55 (01:01:26 to 03::18:34:22)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.59.\n",
            "It has been 314 episode(s) since the model was last saved, with a score of -108 (Δ-14.41).\n",
            "\n",
            "12:09:05 \n",
            "Episode: **316**/4000, Score: -120 (Δ -7.0)\n",
            "Average score: -122.4 (Δ-0.04)\n",
            "Episode time: 3.0s, average: 5.0s (±83.18), ETA: 05:08:27 (01:01:25 to 03::18:17:05)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.42.\n",
            "It has been 315 episode(s) since the model was last saved, with a score of -108 (Δ-14.45).\n",
            "\n",
            "12:09:10 \n",
            "Episode: **317**/4000, Score: -134 (Δ-14.1)\n",
            "Average score: -122.4 (Δ-0.01)\n",
            "Episode time: 5.0s, average: 5.0s (±82.92), ETA: 05:08:21 (01:01:24 to 03::17:59:29)\n",
            "Steps: 137. Time per step: 3.6e-02s. Reward per step: -0.98.\n",
            "It has been 316 episode(s) since the model was last saved, with a score of -108 (Δ-14.46).\n",
            "\n",
            "12:09:15 \n",
            "Episode: **318**/4000, Score: -118 (Δ 16.2)\n",
            "Average score: -122.4 (Δ 0.04)\n",
            "Episode time: 5.0s, average: 5.0s (±82.66), ETA: 05:08:16 (01:01:23 to 03::17:42:01)\n",
            "Steps: 141. Time per step: 3.5e-02s. Reward per step: -0.83.\n",
            "It has been 317 episode(s) since the model was last saved, with a score of -108 (Δ-14.42).\n",
            "\n",
            "12:09:21 \n",
            "Episode: **319**/4000, Score: -139 (Δ-21.6)\n",
            "Average score: -122.6 (Δ-0.20)\n",
            "Episode time: 6.0s, average: 5.0s (±82.40), ETA: 05:08:22 (01:01:22 to 03::17:25:01)\n",
            "Steps: 159. Time per step: 3.8e-02s. Reward per step: -0.88.\n",
            "It has been 318 episode(s) since the model was last saved, with a score of -108 (Δ-14.62).\n",
            "\n",
            "12:09:24 \n",
            "Episode: **320**/4000, Score: -118 (Δ 21.0)\n",
            "Average score: -122.5 (Δ 0.04)\n",
            "Episode time: 3.0s, average: 5.0s (±82.16), ETA: 05:07:54 (01:01:21 to 03::17:08:10)\n",
            "Steps: 75. Time per step: 4.0e-02s. Reward per step: -1.58.\n",
            "It has been 319 episode(s) since the model was last saved, with a score of -108 (Δ-14.58).\n",
            "\n",
            "12:09:29 \n",
            "Episode: **321**/4000, Score: -114 (Δ  4.5)\n",
            "Average score: -122.4 (Δ 0.08)\n",
            "Episode time: 5.0s, average: 5.0s (±81.90), ETA: 05:07:49 (01:01:20 to 03::16:51:01)\n",
            "Steps: 125. Time per step: 4.0e-02s. Reward per step: -0.91.\n",
            "It has been 320 episode(s) since the model was last saved, with a score of -108 (Δ-14.49).\n",
            "\n",
            "12:09:32 \n",
            "Episode: **322**/4000, Score: -112 (Δ  1.6)\n",
            "Average score: -122.4 (Δ 0.07)\n",
            "Episode time: 3.0s, average: 5.0s (±81.66), ETA: 05:07:21 (01:01:19 to 03::16:34:21)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.33.\n",
            "It has been 321 episode(s) since the model was last saved, with a score of -108 (Δ-14.42).\n",
            "\n",
            "12:09:34 \n",
            "Episode: **323**/4000, Score: -108 (Δ  3.8)\n",
            "Average score: -122.2 (Δ 0.14)\n",
            "Episode time: 2.0s, average: 5.0s (±81.43), ETA: 05:06:41 (01:01:18 to 03::16:18:34)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -2.04.\n",
            "It has been 322 episode(s) since the model was last saved, with a score of -108 (Δ-14.28).\n",
            "\n",
            "12:09:37 \n",
            "Episode: **324**/4000, Score: -106 (Δ  1.9)\n",
            "Average score: -122.1 (Δ 0.16)\n",
            "Episode time: 3.0s, average: 5.0s (±81.19), ETA: 05:06:14 (01:01:17 to 03::16:02:06)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.28.\n",
            "It has been 323 episode(s) since the model was last saved, with a score of -108 (Δ-14.12).\n",
            "\n",
            "12:09:40 \n",
            "Episode: **325**/4000, Score: -114 (Δ -8.0)\n",
            "Average score: -122.0 (Δ 0.07)\n",
            "Episode time: 3.0s, average: 5.0s (±80.96), ETA: 05:05:46 (01:01:16 to 03::15:45:43)\n",
            "Steps: 68. Time per step: 4.4e-02s. Reward per step: -1.68.\n",
            "It has been 324 episode(s) since the model was last saved, with a score of -108 (Δ-14.05).\n",
            "\n",
            "12:09:44 \n",
            "Episode: **326**/4000, Score: -123 (Δ -9.0)\n",
            "Average score: -122.0 (Δ-0.04)\n",
            "Episode time: 4.0s, average: 5.0s (±80.71), ETA: 05:05:30 (01:01:15 to 03::15:29:05)\n",
            "Steps: 106. Time per step: 3.8e-02s. Reward per step: -1.16.\n",
            "It has been 325 episode(s) since the model was last saved, with a score of -108 (Δ-14.09).\n",
            "\n",
            "12:09:46 \n",
            "Episode: **327**/4000, Score: -111 (Δ 12.8)\n",
            "Average score: -122.0 (Δ 0.05)\n",
            "Episode time: 2.0s, average: 5.0s (±80.49), ETA: 05:04:51 (01:01:14 to 03::15:13:39)\n",
            "Steps: 61. Time per step: 3.3e-02s. Reward per step: -1.81.\n",
            "It has been 326 episode(s) since the model was last saved, with a score of -108 (Δ-14.04).\n",
            "\n",
            "12:09:49 \n",
            "Episode: **328**/4000, Score: -114 (Δ -3.5)\n",
            "Average score: -121.8 (Δ 0.17)\n",
            "Episode time: 3.0s, average: 5.0s (±80.26), ETA: 05:04:24 (01:01:13 to 03::14:57:33)\n",
            "Steps: 66. Time per step: 4.5e-02s. Reward per step: -1.73.\n",
            "It has been 327 episode(s) since the model was last saved, with a score of -108 (Δ-13.86).\n",
            "\n",
            "12:09:52 \n",
            "Episode: **329**/4000, Score: -128 (Δ-13.8)\n",
            "Average score: -122.0 (Δ-0.15)\n",
            "Episode time: 3.0s, average: 5.0s (±80.03), ETA: 05:03:57 (01:01:12 to 03::14:41:34)\n",
            "Steps: 94. Time per step: 3.2e-02s. Reward per step: -1.36.\n",
            "It has been 328 episode(s) since the model was last saved, with a score of -108 (Δ-14.01).\n",
            "\n",
            "12:09:56 \n",
            "Episode: **330**/4000, Score: -125 (Δ  2.7)\n",
            "Average score: -122.1 (Δ-0.10)\n",
            "Episode time: 4.0s, average: 5.0s (±79.79), ETA: 05:03:42 (01:01:11 to 03::14:25:18)\n",
            "Steps: 97. Time per step: 4.1e-02s. Reward per step: -1.29.\n",
            "It has been 329 episode(s) since the model was last saved, with a score of -108 (Δ-14.11).\n",
            "\n",
            "12:10:02 \n",
            "Episode: **331**/4000, Score: -119 (Δ  6.4)\n",
            "Average score: -122.1 (Δ-0.03)\n",
            "Episode time: 6.0s, average: 5.0s (±79.55), ETA: 05:03:48 (01:01:10 to 03::14:09:32)\n",
            "Steps: 164. Time per step: 3.7e-02s. Reward per step: -0.72.\n",
            "It has been 330 episode(s) since the model was last saved, with a score of -108 (Δ-14.13).\n",
            "\n",
            "12:10:06 \n",
            "Episode: **332**/4000, Score: -114 (Δ  5.1)\n",
            "Average score: -122.0 (Δ 0.03)\n",
            "Episode time: 4.0s, average: 5.0s (±79.31), ETA: 05:03:32 (01:01:09 to 03::13:53:28)\n",
            "Steps: 94. Time per step: 4.3e-02s. Reward per step: -1.21.\n",
            "It has been 331 episode(s) since the model was last saved, with a score of -108 (Δ-14.11).\n",
            "\n",
            "12:10:10 \n",
            "Episode: **333**/4000, Score: -112 (Δ  1.4)\n",
            "Average score: -122.0 (Δ 0.03)\n",
            "Episode time: 4.0s, average: 5.0s (±79.08), ETA: 05:03:17 (01:01:08 to 03::13:37:30)\n",
            "Steps: 117. Time per step: 3.4e-02s. Reward per step: -0.96.\n",
            "It has been 332 episode(s) since the model was last saved, with a score of -108 (Δ-14.08).\n",
            "\n",
            "12:10:15 \n",
            "Episode: **334**/4000, Score: -121 (Δ -9.0)\n",
            "Average score: -122.0 (Δ 0.07)\n",
            "Episode time: 5.0s, average: 5.0s (±78.84), ETA: 05:03:12 (01:01:07 to 03::13:21:38)\n",
            "Steps: 132. Time per step: 3.8e-02s. Reward per step: -0.92.\n",
            "It has been 333 episode(s) since the model was last saved, with a score of -108 (Δ-14.01).\n",
            "\n",
            "12:10:18 \n",
            "Episode: **335**/4000, Score: -113 (Δ  8.3)\n",
            "Average score: -121.9 (Δ 0.00)\n",
            "Episode time: 3.0s, average: 5.0s (±78.62), ETA: 05:02:46 (01:01:06 to 03::13:06:12)\n",
            "Steps: 66. Time per step: 4.5e-02s. Reward per step: -1.71.\n",
            "It has been 334 episode(s) since the model was last saved, with a score of -108 (Δ-14.00).\n",
            "\n",
            "12:10:22 \n",
            "Episode: **336**/4000, Score: -126 (Δ-13.4)\n",
            "Average score: -121.9 (Δ-0.00)\n",
            "Episode time: 4.0s, average: 5.0s (±78.38), ETA: 05:02:30 (01:01:05 to 03::12:50:30)\n",
            "Steps: 103. Time per step: 3.9e-02s. Reward per step: -1.23.\n",
            "It has been 335 episode(s) since the model was last saved, with a score of -108 (Δ-14.00).\n",
            "\n",
            "12:10:24 \n",
            "Episode: **337**/4000, Score: -108 (Δ 18.4)\n",
            "Average score: -121.7 (Δ 0.25)\n",
            "Episode time: 2.0s, average: 4.9s (±78.18), ETA: 05:01:53 (01:01:04 to 03::12:35:57)\n",
            "Steps: 60. Time per step: 3.3e-02s. Reward per step: -1.80.\n",
            "It has been 336 episode(s) since the model was last saved, with a score of -108 (Δ-13.75).\n",
            "\n",
            "12:10:27 \n",
            "Episode: **338**/4000, Score: -115 (Δ -7.2)\n",
            "Average score: -121.7 (Δ 0.04)\n",
            "Episode time: 3.0s, average: 4.9s (±77.96), ETA: 05:01:27 (01:01:03 to 03::12:20:46)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.39.\n",
            "It has been 337 episode(s) since the model was last saved, with a score of -108 (Δ-13.72).\n",
            "\n",
            "12:10:29 \n",
            "Episode: **339**/4000, Score: -114 (Δ  1.3)\n",
            "Average score: -121.6 (Δ 0.03)\n",
            "Episode time: 2.0s, average: 4.9s (±77.75), ETA: 05:00:51 (01:01:02 to 03::12:06:23)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -2.04.\n",
            "It has been 338 episode(s) since the model was last saved, with a score of -108 (Δ-13.68).\n",
            "\n",
            "12:10:34 \n",
            "Episode: **340**/4000, Score: -116 (Δ -1.7)\n",
            "Average score: -121.6 (Δ 0.01)\n",
            "Episode time: 5.0s, average: 4.9s (±77.52), ETA: 05:00:47 (01:01:01 to 03::11:51:04)\n",
            "Steps: 127. Time per step: 3.9e-02s. Reward per step: -0.91.\n",
            "It has been 339 episode(s) since the model was last saved, with a score of -108 (Δ-13.67).\n",
            "\n",
            "12:10:38 \n",
            "Episode: **341**/4000, Score: -131 (Δ-15.3)\n",
            "Average score: -120.5 (Δ 1.14)\n",
            "Episode time: 4.0s, average: 4.9s (±77.30), ETA: 05:00:32 (01:01:00 to 03::11:35:48)\n",
            "Steps: 118. Time per step: 3.4e-02s. Reward per step: -1.11.\n",
            "It has been 340 episode(s) since the model was last saved, with a score of -108 (Δ-12.53).\n",
            "\n",
            "12:10:41 \n",
            "Episode: **342**/4000, Score: -112 (Δ 18.9)\n",
            "Average score: -120.4 (Δ 0.03)\n",
            "Episode time: 3.0s, average: 4.9s (±77.08), ETA: 05:00:06 (01:00:59 to 03::11:20:58)\n",
            "Steps: 69. Time per step: 4.3e-02s. Reward per step: -1.63.\n",
            "It has been 341 episode(s) since the model was last saved, with a score of -108 (Δ-12.50).\n",
            "\n",
            "12:10:44 \n",
            "Episode: **343**/4000, Score: -116 (Δ -3.6)\n",
            "Average score: -120.4 (Δ-0.00)\n",
            "Episode time: 3.0s, average: 4.9s (±76.87), ETA: 04:59:41 (01:00:58 to 03::11:06:13)\n",
            "Steps: 86. Time per step: 3.5e-02s. Reward per step: -1.35.\n",
            "It has been 342 episode(s) since the model was last saved, with a score of -108 (Δ-12.50).\n",
            "\n",
            "12:10:47 \n",
            "Episode: **344**/4000, Score: -118 (Δ -2.5)\n",
            "Average score: -120.5 (Δ-0.01)\n",
            "Episode time: 3.0s, average: 4.9s (±76.66), ETA: 04:59:15 (01:00:57 to 03::10:51:32)\n",
            "Steps: 91. Time per step: 3.3e-02s. Reward per step: -1.30.\n",
            "It has been 343 episode(s) since the model was last saved, with a score of -108 (Δ-12.51).\n",
            "\n",
            "12:10:52 \n",
            "Episode: **345**/4000, Score: -138 (Δ-19.5)\n",
            "Average score: -120.6 (Δ-0.10)\n",
            "Episode time: 5.0s, average: 4.9s (±76.44), ETA: 04:59:11 (01:00:56 to 03::10:36:39)\n",
            "Steps: 119. Time per step: 4.2e-02s. Reward per step: -1.16.\n",
            "It has been 344 episode(s) since the model was last saved, with a score of -108 (Δ-12.61).\n",
            "\n",
            "12:10:57 \n",
            "Episode: **346**/4000, Score: -112 (Δ 25.8)\n",
            "Average score: -120.5 (Δ 0.04)\n",
            "Episode time: 5.0s, average: 4.9s (±76.21), ETA: 04:59:08 (01:00:55 to 03::10:21:52)\n",
            "Steps: 144. Time per step: 3.5e-02s. Reward per step: -0.78.\n",
            "It has been 345 episode(s) since the model was last saved, with a score of -108 (Δ-12.57).\n",
            "\n",
            "12:11:02 \n",
            "Episode: **347**/4000, Score: -135 (Δ-23.3)\n",
            "Average score: -120.7 (Δ-0.22)\n",
            "Episode time: 5.0s, average: 4.9s (±75.99), ETA: 04:59:04 (01:00:54 to 03::10:07:09)\n",
            "Steps: 111. Time per step: 4.5e-02s. Reward per step: -1.22.\n",
            "It has been 346 episode(s) since the model was last saved, with a score of -108 (Δ-12.79).\n",
            "\n",
            "12:11:04 \n",
            "Episode: **348**/4000, Score: -113 (Δ 22.6)\n",
            "Average score: -120.7 (Δ 0.05)\n",
            "Episode time: 2.0s, average: 4.9s (±75.80), ETA: 04:58:28 (01:00:53 to 03::09:53:28)\n",
            "Steps: 74. Time per step: 2.7e-02s. Reward per step: -1.52.\n",
            "It has been 347 episode(s) since the model was last saved, with a score of -108 (Δ-12.74).\n",
            "\n",
            "12:11:08 \n",
            "Episode: **349**/4000, Score: -110 (Δ  2.3)\n",
            "Average score: -120.7 (Δ 0.03)\n",
            "Episode time: 4.0s, average: 4.9s (±75.59), ETA: 04:58:14 (01:00:52 to 03::09:38:54)\n",
            "Steps: 94. Time per step: 4.3e-02s. Reward per step: -1.17.\n",
            "It has been 348 episode(s) since the model was last saved, with a score of -108 (Δ-12.71).\n",
            "\n",
            "12:11:13 \n",
            "Episode: **350**/4000, Score: -133 (Δ-23.1)\n",
            "Average score: -120.9 (Δ-0.25)\n",
            "Episode time: 5.0s, average: 4.9s (±75.37), ETA: 04:58:10 (01:00:51 to 03::09:24:26)\n",
            "Steps: 133. Time per step: 3.8e-02s. Reward per step: -1.00.\n",
            "It has been 349 episode(s) since the model was last saved, with a score of -108 (Δ-12.96).\n",
            "\n",
            "12:11:18 \n",
            "Episode: **351**/4000, Score: -118 (Δ 15.2)\n",
            "Average score: -120.9 (Δ-0.04)\n",
            "Episode time: 5.0s, average: 4.9s (±75.16), ETA: 04:58:06 (01:00:50 to 03::09:10:03)\n",
            "Steps: 119. Time per step: 4.2e-02s. Reward per step: -0.99.\n",
            "It has been 350 episode(s) since the model was last saved, with a score of -108 (Δ-13.00).\n",
            "\n",
            "12:11:22 \n",
            "Episode: **352**/4000, Score: -118 (Δ  0.5)\n",
            "Average score: -121.0 (Δ-0.01)\n",
            "Episode time: 4.0s, average: 4.9s (±74.94), ETA: 04:57:52 (01:00:49 to 03::08:55:43)\n",
            "Steps: 109. Time per step: 3.7e-02s. Reward per step: -1.08.\n",
            "It has been 351 episode(s) since the model was last saved, with a score of -108 (Δ-13.01).\n",
            "\n",
            "12:11:25 \n",
            "Episode: **353**/4000, Score: -115 (Δ  2.3)\n",
            "Average score: -120.9 (Δ 0.01)\n",
            "Episode time: 3.0s, average: 4.9s (±74.74), ETA: 04:57:27 (01:00:48 to 03::08:41:46)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.42.\n",
            "It has been 352 episode(s) since the model was last saved, with a score of -108 (Δ-13.00).\n",
            "\n",
            "12:11:29 \n",
            "Episode: **354**/4000, Score: -117 (Δ -2.0)\n",
            "Average score: -120.9 (Δ 0.01)\n",
            "Episode time: 4.0s, average: 4.9s (±74.53), ETA: 04:57:13 (01:00:47 to 03::08:27:35)\n",
            "Steps: 110. Time per step: 3.6e-02s. Reward per step: -1.07.\n",
            "It has been 353 episode(s) since the model was last saved, with a score of -108 (Δ-12.99).\n",
            "\n",
            "12:11:32 \n",
            "Episode: **355**/4000, Score: -119 (Δ -1.3)\n",
            "Average score: -121.0 (Δ-0.07)\n",
            "Episode time: 3.0s, average: 4.9s (±74.33), ETA: 04:56:49 (01:00:46 to 03::08:13:48)\n",
            "Steps: 93. Time per step: 3.2e-02s. Reward per step: -1.28.\n",
            "It has been 354 episode(s) since the model was last saved, with a score of -108 (Δ-13.06).\n",
            "\n",
            "12:11:35 \n",
            "Episode: **356**/4000, Score: -109 (Δ  9.2)\n",
            "Average score: -121.0 (Δ 0.05)\n",
            "Episode time: 3.0s, average: 4.9s (±74.13), ETA: 04:56:25 (01:00:45 to 03::08:00:04)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.35.\n",
            "It has been 355 episode(s) since the model was last saved, with a score of -108 (Δ-13.01).\n",
            "\n",
            "12:11:38 \n",
            "Episode: **357**/4000, Score: -109 (Δ  0.2)\n",
            "Average score: -120.9 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 4.9s (±73.94), ETA: 04:56:01 (01:00:44 to 03::07:46:25)\n",
            "Steps: 57. Time per step: 5.3e-02s. Reward per step: -1.92.\n",
            "It has been 356 episode(s) since the model was last saved, with a score of -108 (Δ-12.92).\n",
            "\n",
            "12:11:40 \n",
            "Episode: **358**/4000, Score: -105 (Δ  3.9)\n",
            "Average score: -120.7 (Δ 0.11)\n",
            "Episode time: 2.0s, average: 4.9s (±73.75), ETA: 04:55:27 (01:00:43 to 03::07:33:29)\n",
            "Steps: 71. Time per step: 2.8e-02s. Reward per step: -1.48.\n",
            "It has been 357 episode(s) since the model was last saved, with a score of -108 (Δ-12.81).\n",
            "\n",
            "12:11:43 \n",
            "Episode: **359**/4000, Score: -115 (Δ-10.0)\n",
            "Average score: -120.8 (Δ-0.02)\n",
            "Episode time: 3.0s, average: 4.9s (±73.56), ETA: 04:55:03 (01:00:42 to 03::07:19:58)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.44.\n",
            "It has been 358 episode(s) since the model was last saved, with a score of -108 (Δ-12.82).\n",
            "\n",
            "12:11:48 \n",
            "Episode: **360**/4000, Score: -136 (Δ-20.9)\n",
            "Average score: -120.9 (Δ-0.13)\n",
            "Episode time: 5.0s, average: 4.9s (±73.35), ETA: 04:54:59 (01:00:41 to 03::07:06:17)\n",
            "Steps: 125. Time per step: 4.0e-02s. Reward per step: -1.09.\n",
            "It has been 359 episode(s) since the model was last saved, with a score of -108 (Δ-12.96).\n",
            "\n",
            "12:11:51 \n",
            "Episode: **361**/4000, Score: -112 (Δ 24.5)\n",
            "Average score: -120.7 (Δ 0.23)\n",
            "Episode time: 3.0s, average: 4.9s (±73.16), ETA: 04:54:36 (01:00:40 to 03::06:52:56)\n",
            "Steps: 59. Time per step: 5.1e-02s. Reward per step: -1.89.\n",
            "It has been 360 episode(s) since the model was last saved, with a score of -108 (Δ-12.73).\n",
            "\n",
            "12:11:54 \n",
            "Episode: **362**/4000, Score: -109 (Δ  2.8)\n",
            "Average score: -120.6 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 4.9s (±72.97), ETA: 04:54:12 (01:00:39 to 03::06:39:38)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.29.\n",
            "It has been 361 episode(s) since the model was last saved, with a score of -108 (Δ-12.64).\n",
            "\n",
            "12:11:56 \n",
            "Episode: **363**/4000, Score: -109 (Δ -0.6)\n",
            "Average score: -120.5 (Δ 0.06)\n",
            "Episode time: 2.0s, average: 4.8s (±72.79), ETA: 04:53:39 (01:00:38 to 03::06:27:02)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -2.06.\n",
            "It has been 362 episode(s) since the model was last saved, with a score of -108 (Δ-12.58).\n",
            "\n",
            "12:11:59 \n",
            "Episode: **364**/4000, Score: -114 (Δ -4.4)\n",
            "Average score: -120.4 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 4.8s (±72.60), ETA: 04:53:15 (01:00:37 to 03::06:13:52)\n",
            "Steps: 95. Time per step: 3.2e-02s. Reward per step: -1.20.\n",
            "It has been 363 episode(s) since the model was last saved, with a score of -108 (Δ-12.51).\n",
            "\n",
            "12:12:02 \n",
            "Episode: **365**/4000, Score: -108 (Δ  5.7)\n",
            "Average score: -120.3 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 4.8s (±72.41), ETA: 04:52:52 (01:00:36 to 03::06:00:47)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.35.\n",
            "It has been 364 episode(s) since the model was last saved, with a score of -108 (Δ-12.38).\n",
            "\n",
            "12:12:06 \n",
            "Episode: **366**/4000, Score: -110 (Δ -2.0)\n",
            "Average score: -120.2 (Δ 0.12)\n",
            "Episode time: 4.0s, average: 4.8s (±72.21), ETA: 04:52:39 (01:00:35 to 03::05:47:29)\n",
            "Steps: 88. Time per step: 4.5e-02s. Reward per step: -1.25.\n",
            "It has been 365 episode(s) since the model was last saved, with a score of -108 (Δ-12.26).\n",
            "\n",
            "12:12:12 \n",
            "Episode: **367**/4000, Score: -115 (Δ -4.5)\n",
            "Average score: -120.2 (Δ-0.01)\n",
            "Episode time: 6.0s, average: 4.8s (±72.02), ETA: 04:52:46 (01:00:34 to 03::05:34:42)\n",
            "Steps: 154. Time per step: 3.9e-02s. Reward per step: -0.74.\n",
            "It has been 366 episode(s) since the model was last saved, with a score of -108 (Δ-12.27).\n",
            "\n",
            "12:12:15 \n",
            "Episode: **368**/4000, Score: -114 (Δ  0.3)\n",
            "Average score: -120.1 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 4.8s (±71.83), ETA: 04:52:23 (01:00:33 to 03::05:21:49)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.36.\n",
            "It has been 367 episode(s) since the model was last saved, with a score of -108 (Δ-12.20).\n",
            "\n",
            "12:12:18 \n",
            "Episode: **369**/4000, Score: -114 (Δ  0.0)\n",
            "Average score: -120.0 (Δ 0.12)\n",
            "Episode time: 3.0s, average: 4.8s (±71.65), ETA: 04:52:00 (01:00:32 to 03::05:09:00)\n",
            "Steps: 84. Time per step: 3.6e-02s. Reward per step: -1.36.\n",
            "It has been 368 episode(s) since the model was last saved, with a score of -108 (Δ-12.07).\n",
            "\n",
            "12:12:20 \n",
            "Episode: **370**/4000, Score: -109 (Δ  5.2)\n",
            "Average score: -119.9 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 4.8s (±71.47), ETA: 04:51:28 (01:00:31 to 03::04:56:51)\n",
            "Steps: 64. Time per step: 3.1e-02s. Reward per step: -1.70.\n",
            "It has been 369 episode(s) since the model was last saved, with a score of -108 (Δ-11.95).\n",
            "\n",
            "12:12:24 \n",
            "Episode: **371**/4000, Score: -111 (Δ -2.1)\n",
            "Average score: -119.8 (Δ 0.13)\n",
            "Episode time: 4.0s, average: 4.8s (±71.28), ETA: 04:51:15 (01:00:30 to 03::04:43:54)\n",
            "Steps: 106. Time per step: 3.8e-02s. Reward per step: -1.05.\n",
            "It has been 370 episode(s) since the model was last saved, with a score of -108 (Δ-11.82).\n",
            "\n",
            "12:12:28 \n",
            "Episode: **372**/4000, Score: -112 (Δ -0.8)\n",
            "Average score: -119.7 (Δ 0.06)\n",
            "Episode time: 4.0s, average: 4.8s (±71.09), ETA: 04:51:02 (01:00:29 to 03::04:31:01)\n",
            "Steps: 84. Time per step: 4.8e-02s. Reward per step: -1.33.\n",
            "It has been 371 episode(s) since the model was last saved, with a score of -108 (Δ-11.76).\n",
            "\n",
            "12:12:36 \n",
            "Episode: **373**/4000, Score: -134 (Δ-22.0)\n",
            "Average score: -119.9 (Δ-0.19)\n",
            "Episode time: 8.0s, average: 4.8s (±70.93), ETA: 04:51:28 (01:00:28 to 03::04:20:23)\n",
            "Steps: 224. Time per step: 3.6e-02s. Reward per step: -0.60.\n",
            "It has been 372 episode(s) since the model was last saved, with a score of -108 (Δ-11.96).\n",
            "\n",
            "12:12:40 \n",
            "Episode: **374**/4000, Score: -119 (Δ 14.6)\n",
            "Average score: -119.8 (Δ 0.08)\n",
            "Episode time: 4.0s, average: 4.8s (±70.74), ETA: 04:51:16 (01:00:27 to 03::04:07:38)\n",
            "Steps: 98. Time per step: 4.1e-02s. Reward per step: -1.22.\n",
            "It has been 373 episode(s) since the model was last saved, with a score of -108 (Δ-11.87).\n",
            "\n",
            "12:12:43 \n",
            "Episode: **375**/4000, Score: -114 (Δ  5.5)\n",
            "Average score: -119.7 (Δ 0.11)\n",
            "Episode time: 3.0s, average: 4.8s (±70.56), ETA: 04:50:53 (01:00:26 to 03::03:55:13)\n",
            "Steps: 86. Time per step: 3.5e-02s. Reward per step: -1.32.\n",
            "It has been 374 episode(s) since the model was last saved, with a score of -108 (Δ-11.76).\n",
            "\n",
            "12:12:47 \n",
            "Episode: **376**/4000, Score: -105 (Δ  9.2)\n",
            "Average score: -119.5 (Δ 0.23)\n",
            "Episode time: 4.0s, average: 4.8s (±70.38), ETA: 04:50:40 (01:00:25 to 03::03:42:36)\n",
            "Steps: 110. Time per step: 3.6e-02s. Reward per step: -0.95.\n",
            "It has been 375 episode(s) since the model was last saved, with a score of -108 (Δ-11.53).\n",
            "\n",
            "12:12:52 \n",
            "Episode: **377**/4000, Score: -112 (Δ -7.8)\n",
            "Average score: -119.4 (Δ 0.07)\n",
            "Episode time: 5.0s, average: 4.8s (±70.19), ETA: 04:50:37 (01:00:24 to 03::03:30:06)\n",
            "Steps: 128. Time per step: 3.9e-02s. Reward per step: -0.88.\n",
            "It has been 376 episode(s) since the model was last saved, with a score of -108 (Δ-11.47).\n",
            "\n",
            "12:12:55 \n",
            "Episode: **378**/4000, Score: -109 (Δ  3.0)\n",
            "Average score: -119.3 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 4.8s (±70.01), ETA: 04:50:15 (01:00:23 to 03::03:17:52)\n",
            "Steps: 67. Time per step: 4.5e-02s. Reward per step: -1.63.\n",
            "It has been 377 episode(s) since the model was last saved, with a score of -108 (Δ-11.38).\n",
            "\n",
            "12:12:58 \n",
            "Episode: **379**/4000, Score: -117 (Δ -8.0)\n",
            "Average score: -119.3 (Δ-0.00)\n",
            "Episode time: 3.0s, average: 4.8s (±69.84), ETA: 04:49:53 (01:00:22 to 03::03:05:42)\n",
            "Steps: 89. Time per step: 3.4e-02s. Reward per step: -1.32.\n",
            "It has been 378 episode(s) since the model was last saved, with a score of -108 (Δ-11.38).\n",
            "\n",
            "12:13:03 \n",
            "Episode: **380**/4000, Score: -102 (Δ 15.0)\n",
            "Average score: -119.0 (Δ 0.32)\n",
            "Episode time: 5.0s, average: 4.8s (±69.65), ETA: 04:49:50 (01:00:21 to 03::02:53:24)\n",
            "Steps: 127. Time per step: 3.9e-02s. Reward per step: -0.81.\n",
            "It has been 379 episode(s) since the model was last saved, with a score of -108 (Δ-11.06).\n",
            "\n",
            "12:13:05 \n",
            "Episode: **381**/4000, Score: -110 (Δ -7.6)\n",
            "Average score: -119.0 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 4.8s (±69.49), ETA: 04:49:19 (01:00:20 to 03::02:41:56)\n",
            "Steps: 66. Time per step: 3.0e-02s. Reward per step: -1.67.\n",
            "It has been 380 episode(s) since the model was last saved, with a score of -108 (Δ-11.02).\n",
            "\n",
            "12:14:06 \n",
            "Episode: **382**/4000, Score: -148 (Δ-38.0)\n",
            "Average score: -119.1 (Δ-0.16)\n",
            "Episode time: 61.0s, average: 4.9s (±77.56), ETA: 04:58:07 (01:00:19 to 03::10:56:05)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.09.\n",
            "It has been 381 episode(s) since the model was last saved, with a score of -108 (Δ-11.18).\n",
            "\n",
            "12:14:09 \n",
            "Episode: **383**/4000, Score: -111 (Δ 36.6)\n",
            "Average score: -119.0 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 4.9s (±77.36), ETA: 04:57:43 (01:00:18 to 03::10:42:47)\n",
            "Steps: 77. Time per step: 3.9e-02s. Reward per step: -1.45.\n",
            "It has been 382 episode(s) since the model was last saved, with a score of -108 (Δ-11.09).\n",
            "\n",
            "12:14:12 \n",
            "Episode: **384**/4000, Score: -118 (Δ -7.0)\n",
            "Average score: -118.9 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 4.9s (±77.17), ETA: 04:57:20 (01:00:17 to 03::10:29:33)\n",
            "Steps: 76. Time per step: 3.9e-02s. Reward per step: -1.56.\n",
            "It has been 383 episode(s) since the model was last saved, with a score of -108 (Δ-11.00).\n",
            "\n",
            "12:14:20 \n",
            "Episode: **385**/4000, Score: -107 (Δ 11.2)\n",
            "Average score: -118.7 (Δ 0.22)\n",
            "Episode time: 8.0s, average: 4.9s (±77.00), ETA: 04:57:44 (01:00:16 to 03::10:18:03)\n",
            "Steps: 209. Time per step: 3.8e-02s. Reward per step: -0.51.\n",
            "It has been 384 episode(s) since the model was last saved, with a score of -108 (Δ-10.78).\n",
            "\n",
            "12:14:23 \n",
            "Episode: **386**/4000, Score: -116 (Δ -8.6)\n",
            "Average score: -118.6 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 4.9s (±76.81), ETA: 04:57:21 (01:00:15 to 03::10:04:57)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.43.\n",
            "It has been 385 episode(s) since the model was last saved, with a score of -108 (Δ-10.64).\n",
            "\n",
            "12:14:26 \n",
            "Episode: **387**/4000, Score: -116 (Δ -0.7)\n",
            "Average score: -118.5 (Δ 0.10)\n",
            "Episode time: 3.0s, average: 4.9s (±76.62), ETA: 04:56:58 (01:00:14 to 03::09:51:55)\n",
            "Steps: 85. Time per step: 3.5e-02s. Reward per step: -1.37.\n",
            "It has been 386 episode(s) since the model was last saved, with a score of -108 (Δ-10.54).\n",
            "\n",
            "12:14:31 \n",
            "Episode: **388**/4000, Score: -132 (Δ-15.3)\n",
            "Average score: -118.5 (Δ 0.01)\n",
            "Episode time: 5.0s, average: 4.9s (±76.42), ETA: 04:56:54 (01:00:13 to 03::09:38:41)\n",
            "Steps: 130. Time per step: 3.8e-02s. Reward per step: -1.01.\n",
            "It has been 387 episode(s) since the model was last saved, with a score of -108 (Δ-10.53).\n",
            "\n",
            "12:14:34 \n",
            "Episode: **389**/4000, Score: -121 (Δ 10.3)\n",
            "Average score: -118.4 (Δ 0.12)\n",
            "Episode time: 3.0s, average: 4.9s (±76.23), ETA: 04:56:31 (01:00:12 to 03::09:25:46)\n",
            "Steps: 98. Time per step: 3.1e-02s. Reward per step: -1.24.\n",
            "It has been 388 episode(s) since the model was last saved, with a score of -108 (Δ-10.41).\n",
            "\n",
            "12:14:37 \n",
            "Episode: **390**/4000, Score: -111 (Δ 10.2)\n",
            "Average score: -118.2 (Δ 0.14)\n",
            "Episode time: 3.0s, average: 4.9s (±76.05), ETA: 04:56:08 (01:00:11 to 03::09:12:56)\n",
            "Steps: 65. Time per step: 4.6e-02s. Reward per step: -1.71.\n",
            "It has been 389 episode(s) since the model was last saved, with a score of -108 (Δ-10.28).\n",
            "\n",
            "12:14:50 \n",
            "Episode: **391**/4000, Score: -152 (Δ-40.3)\n",
            "Average score: -118.6 (Δ-0.36)\n",
            "Episode time: 13.0s, average: 4.9s (±76.02), ETA: 04:57:18 (01:00:10 to 03::09:11:08)\n",
            "Steps: 348. Time per step: 3.7e-02s. Reward per step: -0.44.\n",
            "It has been 390 episode(s) since the model was last saved, with a score of -108 (Δ-10.63).\n",
            "\n",
            "12:15:51 \n",
            "Episode: **392**/4000, Score: -150 (Δ  1.3)\n",
            "Average score: -118.9 (Δ-0.29)\n",
            "Episode time: 61.0s, average: 5.1s (±83.82), ETA: 05:05:49 (01:00:09 to 03::17:07:42)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.09.\n",
            "It has been 391 episode(s) since the model was last saved, with a score of -108 (Δ-10.92).\n",
            "\n",
            "12:15:52 \n",
            "Episode: **393**/4000, Score: -114 (Δ 36.2)\n",
            "Average score: -118.8 (Δ 0.07)\n",
            "Episode time: 1.0s, average: 5.1s (±83.65), ETA: 05:05:06 (01:00:08 to 03::16:55:19)\n",
            "Steps: 40. Time per step: 2.5e-02s. Reward per step: -2.85.\n",
            "It has been 392 episode(s) since the model was last saved, with a score of -108 (Δ-10.85).\n",
            "\n",
            "12:15:55 \n",
            "Episode: **394**/4000, Score: -128 (Δ-14.2)\n",
            "Average score: -118.8 (Δ-0.04)\n",
            "Episode time: 3.0s, average: 5.1s (±83.45), ETA: 05:04:42 (01:00:07 to 03::16:41:25)\n",
            "Steps: 74. Time per step: 4.1e-02s. Reward per step: -1.73.\n",
            "It has been 393 episode(s) since the model was last saved, with a score of -108 (Δ-10.89).\n",
            "\n",
            "12:16:02 \n",
            "Episode: **395**/4000, Score: -147 (Δ-18.4)\n",
            "Average score: -119.1 (Δ-0.30)\n",
            "Episode time: 7.0s, average: 5.1s (±83.25), ETA: 05:04:55 (01:00:06 to 03::16:28:06)\n",
            "Steps: 172. Time per step: 4.1e-02s. Reward per step: -0.85.\n",
            "It has been 394 episode(s) since the model was last saved, with a score of -108 (Δ-11.19).\n",
            "\n",
            "12:16:04 \n",
            "Episode: **396**/4000, Score: -127 (Δ 20.0)\n",
            "Average score: -119.1 (Δ 0.08)\n",
            "Episode time: 2.0s, average: 5.1s (±83.06), ETA: 05:04:22 (01:00:05 to 03::16:14:58)\n",
            "Steps: 64. Time per step: 3.1e-02s. Reward per step: -1.98.\n",
            "It has been 395 episode(s) since the model was last saved, with a score of -108 (Δ-11.11).\n",
            "\n",
            "12:16:07 \n",
            "Episode: **397**/4000, Score: -127 (Δ -0.3)\n",
            "Average score: -119.1 (Δ-0.07)\n",
            "Episode time: 3.0s, average: 5.1s (±82.86), ETA: 05:03:58 (01:00:04 to 03::16:01:16)\n",
            "Steps: 75. Time per step: 4.0e-02s. Reward per step: -1.69.\n",
            "It has been 396 episode(s) since the model was last saved, with a score of -108 (Δ-11.18).\n",
            "\n",
            "12:16:08 \n",
            "Episode: **398**/4000, Score: -111 (Δ 16.2)\n",
            "Average score: -118.9 (Δ 0.25)\n",
            "Episode time: 1.0s, average: 5.1s (±82.70), ETA: 05:03:16 (01:00:03 to 03::15:49:10)\n",
            "Steps: 36. Time per step: 2.8e-02s. Reward per step: -3.08.\n",
            "It has been 397 episode(s) since the model was last saved, with a score of -108 (Δ-10.93).\n",
            "\n",
            "12:16:11 \n",
            "Episode: **399**/4000, Score: -122 (Δ-11.0)\n",
            "Average score: -118.8 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 5.0s (±82.50), ETA: 05:02:52 (01:00:02 to 03::15:35:35)\n",
            "Steps: 69. Time per step: 4.3e-02s. Reward per step: -1.76.\n",
            "It has been 398 episode(s) since the model was last saved, with a score of -108 (Δ-10.84).\n",
            "\n",
            "12:16:14 \n",
            "Episode: **400**/4000, Score: -127 (Δ -5.4)\n",
            "Average score: -118.7 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 5.0s (±82.30), ETA: 05:02:29 (01:00:01 to 03::15:22:04)\n",
            "Steps: 70. Time per step: 4.3e-02s. Reward per step: -1.82.\n",
            "It has been 399 episode(s) since the model was last saved, with a score of -108 (Δ-10.75).\n",
            "\n",
            "12:16:17 \n",
            "Episode: **401**/4000, Score: -124 (Δ  2.9)\n",
            "Average score: -118.6 (Δ 0.11)\n",
            "Episode time: 3.0s, average: 5.0s (±82.11), ETA: 05:02:06 (01:00:00 to 03::15:08:36)\n",
            "Steps: 92. Time per step: 3.3e-02s. Reward per step: -1.35.\n",
            "It has been 400 episode(s) since the model was last saved, with a score of -108 (Δ-10.64).\n",
            "\n",
            "12:16:20 \n",
            "Episode: **402**/4000, Score: -109 (Δ 15.3)\n",
            "Average score: -118.5 (Δ 0.11)\n",
            "Episode time: 3.0s, average: 5.0s (±81.91), ETA: 05:01:42 (00:59:59 to 03::14:55:13)\n",
            "Steps: 74. Time per step: 4.1e-02s. Reward per step: -1.47.\n",
            "It has been 401 episode(s) since the model was last saved, with a score of -108 (Δ-10.53).\n",
            "\n",
            "12:16:23 \n",
            "Episode: **403**/4000, Score: -107 (Δ  1.5)\n",
            "Average score: -118.3 (Δ 0.15)\n",
            "Episode time: 3.0s, average: 5.0s (±81.72), ETA: 05:01:19 (00:59:58 to 03::14:41:53)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.33.\n",
            "It has been 402 episode(s) since the model was last saved, with a score of -108 (Δ-10.38).\n",
            "\n",
            "12:16:27 \n",
            "Episode: **404**/4000, Score: -128 (Δ-21.0)\n",
            "Average score: -118.5 (Δ-0.15)\n",
            "Episode time: 4.0s, average: 5.0s (±81.52), ETA: 05:01:05 (00:59:57 to 03::14:28:19)\n",
            "Steps: 114. Time per step: 3.5e-02s. Reward per step: -1.13.\n",
            "It has been 403 episode(s) since the model was last saved, with a score of -108 (Δ-10.54).\n",
            "\n",
            "12:16:29 \n",
            "Episode: **405**/4000, Score: -109 (Δ 19.2)\n",
            "Average score: -118.4 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 5.0s (±81.34), ETA: 05:00:33 (00:59:56 to 03::14:15:43)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -2.02.\n",
            "It has been 404 episode(s) since the model was last saved, with a score of -108 (Δ-10.42).\n",
            "\n",
            "12:17:01 \n",
            "Episode: **406**/4000, Score: -142 (Δ-33.1)\n",
            "Average score: -118.6 (Δ-0.28)\n",
            "Episode time: 32.0s, average: 5.1s (±82.93), ETA: 05:04:27 (00:59:55 to 03::15:53:27)\n",
            "Steps: 842. Time per step: 3.8e-02s. Reward per step: -0.17.\n",
            "It has been 405 episode(s) since the model was last saved, with a score of -108 (Δ-10.69).\n",
            "\n",
            "12:18:01 \n",
            "Episode: **407**/4000, Score: -155 (Δ-12.5)\n",
            "Average score: -119.0 (Δ-0.38)\n",
            "Episode time: 60.0s, average: 5.2s (±90.12), ETA: 05:12:27 (00:59:54 to 03::23:10:40)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 406 episode(s) since the model was last saved, with a score of -108 (Δ-11.08).\n",
            "\n",
            "12:19:02 \n",
            "Episode: **408**/4000, Score: -127 (Δ 27.4)\n",
            "Average score: -119.1 (Δ-0.12)\n",
            "Episode time: 61.0s, average: 5.4s (±97.51), ETA: 05:20:33 (00:59:53 to 04::06:39:39)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.08.\n",
            "It has been 407 episode(s) since the model was last saved, with a score of -108 (Δ-11.19).\n",
            "\n",
            "12:19:05 \n",
            "Episode: **409**/4000, Score: -101 (Δ 26.6)\n",
            "Average score: -118.9 (Δ 0.20)\n",
            "Episode time: 3.0s, average: 5.3s (±97.28), ETA: 05:20:07 (00:59:52 to 04::06:24:07)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.26.\n",
            "It has been 408 episode(s) since the model was last saved, with a score of -108 (Δ-10.99).\n",
            "\n",
            "12:19:59 \n",
            "Episode: **410**/4000, Score: -218 (Δ-117.0)\n",
            "Average score: -119.9 (Δ-0.99)\n",
            "Episode time: 54.0s, average: 5.5s (±102.80), ETA: 05:27:08 (00:59:51 to 04::12:00:00)\n",
            "Steps: 1427. Time per step: 3.8e-02s. Reward per step: -0.15.\n",
            "It has been 409 episode(s) since the model was last saved, with a score of -108 (Δ-11.98).\n",
            "\n",
            "12:20:59 \n",
            "Episode: **411**/4000, Score: -165 (Δ 52.8)\n",
            "Average score: -120.4 (Δ-0.43)\n",
            "Episode time: 60.0s, average: 5.6s (±109.77), ETA: 05:34:59 (00:59:50 to 04::19:03:04)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 410 episode(s) since the model was last saved, with a score of -108 (Δ-12.42).\n",
            "\n",
            "12:21:01 \n",
            "Episode: **412**/4000, Score: -110 (Δ 55.5)\n",
            "Average score: -120.3 (Δ 0.06)\n",
            "Episode time: 2.0s, average: 5.6s (±109.54), ETA: 05:34:22 (00:59:49 to 04::18:46:34)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -2.03.\n",
            "It has been 411 episode(s) since the model was last saved, with a score of -108 (Δ-12.35).\n",
            "\n",
            "12:21:04 \n",
            "Episode: **413**/4000, Score: -110 (Δ  0.0)\n",
            "Average score: -120.2 (Δ 0.11)\n",
            "Episode time: 3.0s, average: 5.6s (±109.29), ETA: 05:33:54 (00:59:48 to 04::18:29:23)\n",
            "Steps: 59. Time per step: 5.1e-02s. Reward per step: -1.86.\n",
            "It has been 412 episode(s) since the model was last saved, with a score of -108 (Δ-12.24).\n",
            "\n",
            "12:21:06 \n",
            "Episode: **414**/4000, Score: -120 (Δ-10.9)\n",
            "Average score: -120.1 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 5.6s (±109.06), ETA: 05:33:17 (00:59:47 to 04::18:13:01)\n",
            "Steps: 68. Time per step: 2.9e-02s. Reward per step: -1.77.\n",
            "It has been 413 episode(s) since the model was last saved, with a score of -108 (Δ-12.12).\n",
            "\n",
            "12:21:09 \n",
            "Episode: **415**/4000, Score: -104 (Δ 16.1)\n",
            "Average score: -120.0 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 5.6s (±108.81), ETA: 05:32:49 (00:59:46 to 04::17:55:59)\n",
            "Steps: 65. Time per step: 4.6e-02s. Reward per step: -1.61.\n",
            "It has been 414 episode(s) since the model was last saved, with a score of -108 (Δ-12.04).\n",
            "\n",
            "12:21:11 \n",
            "Episode: **416**/4000, Score: -118 (Δ-13.7)\n",
            "Average score: -120.0 (Δ 0.02)\n",
            "Episode time: 2.0s, average: 5.6s (±108.58), ETA: 05:32:13 (00:59:45 to 04::17:39:46)\n",
            "Steps: 67. Time per step: 3.0e-02s. Reward per step: -1.76.\n",
            "It has been 415 episode(s) since the model was last saved, with a score of -108 (Δ-12.02).\n",
            "\n",
            "12:21:14 \n",
            "Episode: **417**/4000, Score: -121 (Δ -3.1)\n",
            "Average score: -119.8 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 5.6s (±108.33), ETA: 05:31:45 (00:59:44 to 04::17:22:52)\n",
            "Steps: 67. Time per step: 4.5e-02s. Reward per step: -1.81.\n",
            "It has been 416 episode(s) since the model was last saved, with a score of -108 (Δ-11.89).\n",
            "\n",
            "12:22:14 \n",
            "Episode: **418**/4000, Score: -173 (Δ-51.4)\n",
            "Average score: -120.4 (Δ-0.55)\n",
            "Episode time: 60.0s, average: 5.7s (±115.15), ETA: 05:39:27 (00:59:43 to 05::00:15:46)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.11.\n",
            "It has been 417 episode(s) since the model was last saved, with a score of -108 (Δ-12.44).\n",
            "\n",
            "12:23:15 \n",
            "Episode: **419**/4000, Score: -172 (Δ  1.0)\n",
            "Average score: -120.7 (Δ-0.32)\n",
            "Episode time: 61.0s, average: 5.8s (±122.16), ETA: 05:47:14 (00:59:42 to 05::07:20:09)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.11.\n",
            "It has been 418 episode(s) since the model was last saved, with a score of -108 (Δ-12.77).\n",
            "\n",
            "12:24:15 \n",
            "Episode: **420**/4000, Score: -176 (Δ -4.4)\n",
            "Average score: -121.3 (Δ-0.58)\n",
            "Episode time: 60.0s, average: 5.9s (±128.84), ETA: 05:54:50 (00:59:41 to 05::14:04:34)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.11.\n",
            "It has been 419 episode(s) since the model was last saved, with a score of -108 (Δ-13.35).\n",
            "\n",
            "12:25:15 \n",
            "Episode: **421**/4000, Score: -164 (Δ 12.0)\n",
            "Average score: -121.8 (Δ-0.50)\n",
            "Episode time: 60.0s, average: 6.1s (±135.46), ETA: 06:02:24 (00:59:40 to 05::20:44:51)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 420 episode(s) since the model was last saved, with a score of -108 (Δ-13.85).\n",
            "\n",
            "12:26:15 \n",
            "Episode: **422**/4000, Score: -85 (Δ 78.5)\n",
            "Average score: -121.5 (Δ 0.27)\n",
            "Episode time: 60.0s, average: 6.2s (±142.01), ETA: 06:09:55 (00:59:39 to 06::03:21:03)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.05.\n",
            "It has been 421 episode(s) since the model was last saved, with a score of -108 (Δ-13.59).\n",
            "\n",
            "12:27:16 \n",
            "Episode: **423**/4000, Score: -140 (Δ-54.3)\n",
            "Average score: -121.8 (Δ-0.32)\n",
            "Episode time: 61.0s, average: 6.3s (±148.76), ETA: 06:17:32 (00:59:38 to 06::10:08:37)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.09.\n",
            "It has been 422 episode(s) since the model was last saved, with a score of -108 (Δ-13.90).\n",
            "\n",
            "12:28:17 \n",
            "Episode: **424**/4000, Score: -146 (Δ -6.6)\n",
            "Average score: -122.2 (Δ-0.40)\n",
            "Episode time: 61.0s, average: 6.5s (±155.44), ETA: 06:25:07 (00:59:37 to 06::16:52:02)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.09.\n",
            "It has been 423 episode(s) since the model was last saved, with a score of -108 (Δ-14.30).\n",
            "\n",
            "12:29:17 \n",
            "Episode: **425**/4000, Score: -140 (Δ  6.4)\n",
            "Average score: -122.5 (Δ-0.26)\n",
            "Episode time: 60.0s, average: 6.6s (±161.80), ETA: 06:32:31 (00:59:36 to 06::23:16:06)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.09.\n",
            "It has been 424 episode(s) since the model was last saved, with a score of -108 (Δ-14.56).\n",
            "\n",
            "12:30:19 \n",
            "Episode: **426**/4000, Score: -130 (Δ 10.4)\n",
            "Average score: -122.6 (Δ-0.06)\n",
            "Episode time: 62.0s, average: 6.7s (±168.62), ETA: 06:40:10 (00:59:35 to 07::06:06:54)\n",
            "Steps: 1600. Time per step: 3.9e-02s. Reward per step: -0.08.\n",
            "It has been 425 episode(s) since the model was last saved, with a score of -108 (Δ-14.62).\n",
            "\n",
            "12:31:20 \n",
            "Episode: **427**/4000, Score: -140 (Δ-10.0)\n",
            "Average score: -122.9 (Δ-0.29)\n",
            "Episode time: 61.0s, average: 6.8s (±175.11), ETA: 06:47:37 (00:59:34 to 07::12:38:08)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.09.\n",
            "It has been 426 episode(s) since the model was last saved, with a score of -108 (Δ-14.91).\n",
            "\n",
            "12:32:20 \n",
            "Episode: **428**/4000, Score: -133 (Δ  6.2)\n",
            "Average score: -123.0 (Δ-0.19)\n",
            "Episode time: 60.0s, average: 7.0s (±181.28), ETA: 06:54:54 (00:59:33 to 07::18:50:22)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.08.\n",
            "It has been 427 episode(s) since the model was last saved, with a score of -108 (Δ-15.10).\n",
            "\n",
            "12:33:20 \n",
            "Episode: **429**/4000, Score: -127 (Δ  6.0)\n",
            "Average score: -123.0 (Δ 0.01)\n",
            "Episode time: 60.0s, average: 7.1s (±187.40), ETA: 07:02:09 (00:59:32 to 08::00:58:49)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.08.\n",
            "It has been 428 episode(s) since the model was last saved, with a score of -108 (Δ-15.10).\n",
            "\n",
            "12:34:21 \n",
            "Episode: **430**/4000, Score: -104 (Δ 23.5)\n",
            "Average score: -122.8 (Δ 0.21)\n",
            "Episode time: 61.0s, average: 7.2s (±193.71), ETA: 07:09:29 (00:59:31 to 08::07:18:24)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.06.\n",
            "It has been 429 episode(s) since the model was last saved, with a score of -108 (Δ-14.88).\n",
            "\n",
            "12:34:23 \n",
            "Episode: **431**/4000, Score: -117 (Δ-13.6)\n",
            "Average score: -122.8 (Δ 0.01)\n",
            "Episode time: 2.0s, average: 7.2s (±193.32), ETA: 07:08:39 (00:59:30 to 08::06:51:21)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.89.\n",
            "It has been 430 episode(s) since the model was last saved, with a score of -108 (Δ-14.87).\n",
            "\n",
            "12:35:24 \n",
            "Episode: **432**/4000, Score: -93 (Δ 24.4)\n",
            "Average score: -122.6 (Δ 0.21)\n",
            "Episode time: 61.0s, average: 7.3s (±199.56), ETA: 07:15:56 (00:59:29 to 08::13:06:21)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.06.\n",
            "It has been 431 episode(s) since the model was last saved, with a score of -108 (Δ-14.66).\n",
            "\n",
            "12:36:25 \n",
            "Episode: **433**/4000, Score: -100 (Δ -7.2)\n",
            "Average score: -122.5 (Δ 0.12)\n",
            "Episode time: 61.0s, average: 7.5s (±205.74), ETA: 07:23:11 (00:59:28 to 08::19:17:34)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.06.\n",
            "It has been 432 episode(s) since the model was last saved, with a score of -108 (Δ-14.54).\n",
            "\n",
            "12:36:50 \n",
            "Episode: **434**/4000, Score: -155 (Δ-55.1)\n",
            "Average score: -122.8 (Δ-0.34)\n",
            "Episode time: 25.0s, average: 7.5s (±205.97), ETA: 07:25:28 (00:59:27 to 08::19:30:19)\n",
            "Steps: 661. Time per step: 3.8e-02s. Reward per step: -0.24.\n",
            "It has been 433 episode(s) since the model was last saved, with a score of -108 (Δ-14.88).\n",
            "\n",
            "12:37:51 \n",
            "Episode: **435**/4000, Score: -3 (Δ152.2)\n",
            "Average score: -121.7 (Δ 1.10)\n",
            "Episode time: 61.0s, average: 7.6s (±212.06), ETA: 07:32:39 (00:59:26 to 09::01:36:11)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.00.\n",
            "It has been 434 episode(s) since the model was last saved, with a score of -108 (Δ-13.78).\n",
            "\n",
            "12:38:17 \n",
            "Episode: **436**/4000, Score: -114 (Δ-110.6)\n",
            "Average score: -121.6 (Δ 0.13)\n",
            "Episode time: 26.0s, average: 7.7s (±212.35), ETA: 07:35:02 (00:59:25 to 09::01:52:05)\n",
            "Steps: 682. Time per step: 3.8e-02s. Reward per step: -0.17.\n",
            "It has been 435 episode(s) since the model was last saved, with a score of -108 (Δ-13.65).\n",
            "\n",
            "12:38:29 \n",
            "Episode: **437**/4000, Score: -113 (Δ  0.7)\n",
            "Average score: -121.6 (Δ-0.05)\n",
            "Episode time: 12.0s, average: 7.7s (±211.91), ETA: 07:35:29 (00:59:24 to 09::01:22:42)\n",
            "Steps: 293. Time per step: 4.1e-02s. Reward per step: -0.39.\n",
            "It has been 436 episode(s) since the model was last saved, with a score of -108 (Δ-13.70).\n",
            "\n",
            "12:38:31 \n",
            "Episode: **438**/4000, Score: -126 (Δ-12.4)\n",
            "Average score: -121.7 (Δ-0.10)\n",
            "Episode time: 2.0s, average: 7.7s (±211.50), ETA: 07:34:36 (00:59:23 to 09::00:53:53)\n",
            "Steps: 67. Time per step: 3.0e-02s. Reward per step: -1.87.\n",
            "It has been 437 episode(s) since the model was last saved, with a score of -108 (Δ-13.80).\n",
            "\n",
            "12:38:36 \n",
            "Episode: **439**/4000, Score: -115 (Δ 10.3)\n",
            "Average score: -121.8 (Δ-0.01)\n",
            "Episode time: 5.0s, average: 7.6s (±211.03), ETA: 07:34:06 (00:59:22 to 09::00:22:14)\n",
            "Steps: 118. Time per step: 4.2e-02s. Reward per step: -0.98.\n",
            "It has been 438 episode(s) since the model was last saved, with a score of -108 (Δ-13.82).\n",
            "\n",
            "12:39:04 \n",
            "Episode: **440**/4000, Score: -114 (Δ  1.7)\n",
            "Average score: -121.7 (Δ 0.02)\n",
            "Episode time: 28.0s, average: 7.7s (±211.49), ETA: 07:36:44 (00:59:21 to 09::00:48:36)\n",
            "Steps: 741. Time per step: 3.8e-02s. Reward per step: -0.15.\n",
            "It has been 439 episode(s) since the model was last saved, with a score of -108 (Δ-13.79).\n",
            "\n",
            "12:39:09 \n",
            "Episode: **441**/4000, Score: -143 (Δ-29.8)\n",
            "Average score: -121.9 (Δ-0.12)\n",
            "Episode time: 5.0s, average: 7.7s (±211.03), ETA: 07:36:14 (00:59:20 to 09::00:17:06)\n",
            "Steps: 134. Time per step: 3.7e-02s. Reward per step: -1.07.\n",
            "It has been 440 episode(s) since the model was last saved, with a score of -108 (Δ-13.92).\n",
            "\n",
            "12:39:14 \n",
            "Episode: **442**/4000, Score: -141 (Δ  2.6)\n",
            "Average score: -122.1 (Δ-0.29)\n",
            "Episode time: 5.0s, average: 7.7s (±210.56), ETA: 07:35:45 (00:59:19 to 08::23:45:45)\n",
            "Steps: 129. Time per step: 3.9e-02s. Reward per step: -1.09.\n",
            "It has been 441 episode(s) since the model was last saved, with a score of -108 (Δ-14.20).\n",
            "\n",
            "12:39:17 \n",
            "Episode: **443**/4000, Score: -121 (Δ 19.9)\n",
            "Average score: -122.2 (Δ-0.05)\n",
            "Episode time: 3.0s, average: 7.7s (±210.14), ETA: 07:34:59 (00:59:18 to 08::23:16:14)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.46.\n",
            "It has been 442 episode(s) since the model was last saved, with a score of -108 (Δ-14.25).\n",
            "\n",
            "12:39:26 \n",
            "Episode: **444**/4000, Score: -129 (Δ -7.8)\n",
            "Average score: -122.3 (Δ-0.10)\n",
            "Episode time: 9.0s, average: 7.7s (±209.67), ETA: 07:35:02 (00:59:17 to 08::22:44:57)\n",
            "Steps: 244. Time per step: 3.7e-02s. Reward per step: -0.53.\n",
            "It has been 443 episode(s) since the model was last saved, with a score of -108 (Δ-14.36).\n",
            "\n",
            "12:39:32 \n",
            "Episode: **445**/4000, Score: -113 (Δ 15.6)\n",
            "Average score: -122.1 (Δ 0.25)\n",
            "Episode time: 6.0s, average: 7.7s (±209.20), ETA: 07:34:41 (00:59:16 to 08::22:13:33)\n",
            "Steps: 136. Time per step: 4.4e-02s. Reward per step: -0.83.\n",
            "It has been 444 episode(s) since the model was last saved, with a score of -108 (Δ-14.11).\n",
            "\n",
            "12:40:33 \n",
            "Episode: **446**/4000, Score: -56 (Δ 57.3)\n",
            "Average score: -121.5 (Δ 0.56)\n",
            "Episode time: 61.0s, average: 7.8s (±215.10), ETA: 07:41:39 (00:59:15 to 09::04:06:11)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.03.\n",
            "It has been 445 episode(s) since the model was last saved, with a score of -108 (Δ-13.55).\n",
            "\n",
            "12:40:43 \n",
            "Episode: **447**/4000, Score: -110 (Δ-54.3)\n",
            "Average score: -121.2 (Δ 0.25)\n",
            "Episode time: 10.0s, average: 7.8s (±214.63), ETA: 07:41:48 (00:59:14 to 09::03:34:54)\n",
            "Steps: 253. Time per step: 4.0e-02s. Reward per step: -0.44.\n",
            "It has been 446 episode(s) since the model was last saved, with a score of -108 (Δ-13.30).\n",
            "\n",
            "12:40:46 \n",
            "Episode: **448**/4000, Score: -124 (Δ-13.6)\n",
            "Average score: -121.3 (Δ-0.11)\n",
            "Episode time: 3.0s, average: 7.8s (±214.20), ETA: 07:41:03 (00:59:13 to 09::03:05:14)\n",
            "Steps: 78. Time per step: 3.8e-02s. Reward per step: -1.59.\n",
            "It has been 447 episode(s) since the model was last saved, with a score of -108 (Δ-13.41).\n",
            "\n",
            "12:41:17 \n",
            "Episode: **449**/4000, Score: -120 (Δ  4.0)\n",
            "Average score: -121.4 (Δ-0.09)\n",
            "Episode time: 31.0s, average: 7.8s (±214.92), ETA: 07:43:59 (00:59:12 to 09::03:47:15)\n",
            "Steps: 808. Time per step: 3.8e-02s. Reward per step: -0.15.\n",
            "It has been 448 episode(s) since the model was last saved, with a score of -108 (Δ-13.50).\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}