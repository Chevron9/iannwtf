{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddpg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNr1Nb0mldNVRRtaUUuZV10",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chevron9/iannwtf/blob/master/ddpg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsSbPZt2wKxh",
        "outputId": "6a7ae822-d711-475b-b187-cd6490f2422b"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "import gym\n",
        "env = gym.make(\"BipedalWalker-v3\")\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33m  WARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX_s3Q4LwNWN",
        "outputId": "4bd12704-4a1f-4b98-d6a5-992ed8c51ce6"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os \n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYoGfSs2zgJz"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plots the scores of the course of training\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgD-CyT4zqCn"
      },
      "source": [
        "def timespan_format(timespan):\n",
        "    timespan = round(timespan)\n",
        "    h_full = timespan / (60*60)\n",
        "    h = timespan // (60*60)\n",
        "    m = (timespan % (60*60)) // 60\n",
        "    s = timespan % 60\n",
        "    time = f\"{h:02}:{m:02}:{s:02}\"\n",
        "    if h_full > 24:\n",
        "        d = timespan // (60*60*24)\n",
        "        h = (timespan % (60*60*24)) // (60*60)\n",
        "        time = f\"{d:02}::{h:02}:{m:02}:{s:02}\"\n",
        "    return time\n",
        "\n",
        "timespan_format(303601)\n",
        "test = False\n",
        "if test:\n",
        "    assert timespan_format(60) == \"00:01:00\"\n",
        "    assert timespan_format(60*60) == \"01:00:00\"\n",
        "    assert timespan_format(60*60+1) == \"01:00:01\"\n",
        "    assert timespan_format(60*60+60) == \"01:01:00\"\n",
        "    assert timespan_format(24*60*60+1) == \"01::00:00:01\"\n",
        "    assert timespan_format(303601) == \"03::12:20:01\" # 3 days 12 hours 20 minutes 1 second\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVmB-P2_wTm4"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "#all the networks of the model\n",
        "\n",
        "\n",
        "#critic network that takes the state and the action and puts out the value of the\n",
        "#the action in the given state\n",
        "class CriticNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512,\n",
        "            name='critic', chkpt_dir='tmp/'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        #Dimensions of the dense layers\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "\n",
        "\n",
        "        #says where the weights are saved\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_ddpg.h5')\n",
        "\n",
        "\n",
        "        #Optimization: using LeakyReLU as per https://arxiv.org/pdf/1709.06560.pdf\n",
        "\n",
        "        #dense layers with kernel and bias initializer(from an other implementation)\n",
        "        #and relu activation\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "        #denselayer with 1 neuron that gives the estimated q value of the\n",
        "        #state-action pair\n",
        "        f3 = 0.003\n",
        "        self.q = Dense(1, activation=None, kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3) ,\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f3, f3),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, action, training = True):\n",
        "        #feeds the network state and action pairs\n",
        "        action_value = self.dense_layer1(tf.concat([state, action], axis=1))\n",
        "\n",
        "        action_value = self.dense_layer2(action_value)\n",
        "\n",
        "        q = self.q(action_value)\n",
        "\n",
        "        #gives back an estimation of a q value\n",
        "        return q\n",
        "\n",
        "\n",
        "#critic network that takes the state and outputs a probability\n",
        "#distribution over all possible actions\n",
        "class ActorNetwork(keras.Model):\n",
        "    def __init__(self, dense1=512, dense2=512, n_actions=4, name='actor',\n",
        "            chkpt_dir='tmp/'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.dense1 = dense1\n",
        "        self.dense2 = dense2\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n",
        "                    self.model_name+'_ddpg.h5')\n",
        "\n",
        "        #first dense layer\n",
        "        f1 = 1. / np.sqrt(self.dense1)\n",
        "        self.dense_layer1 = Dense(self.dense1, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f1, f1),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f1, f1))\n",
        "\n",
        "\n",
        "        #second dense layer\n",
        "        f2 = 1. / np.sqrt(self.dense2)\n",
        "        self.dense_layer2 = Dense(self.dense2, activation= tf.keras.activations.relu,\n",
        "        kernel_initializer = tf.keras.initializers.RandomUniform(-f2, f2),\n",
        "        bias_initializer = tf.keras.initializers.RandomUniform(-f2, f2))\n",
        "\n",
        "\n",
        "        #output layer with tanh activation to get an output vector of length actionspace\n",
        "        #with values between -1 and 1 to fit to the action boundaries\n",
        "        f3 = 0.003\n",
        "        self.mu = Dense(self.n_actions, activation='tanh', kernel_initializer = tf.keras.initializers.RandomUniform(-f3, f3) , bias_initializer\n",
        "         = tf.keras.initializers.RandomUniform(-f3, f3))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, state, training = True):\n",
        "\n",
        "        actions = self.dense_layer1(state)\n",
        "        actions = self.dense_layer2(actions)\n",
        "\n",
        "        #gives back the actions the agent should take (deterministic policy)\n",
        "        actions = self.mu(actions)\n",
        "\n",
        "\n",
        "        return actions\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIiiKOnIwWGs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replaybuffer that stores all informations about the agents transitions\n",
        "# in np arrays\n",
        "class ReplayBuffer:\n",
        "    #initializes the memories with zeros with sizes depending on a big maximal size,\n",
        "    #the input_dimensions(output of the environment) or the numbers of possible\n",
        "    #actions\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.max_size = max_size\n",
        "        self.current_position = 0\n",
        "        self.state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.new_state_memory = np.zeros((self.max_size, *input_shape))\n",
        "        self.action_memory = np.zeros((self.max_size, n_actions))\n",
        "        self.reward_memory = np.zeros(self.max_size)\n",
        "        self.terminal_memory = np.zeros(self.max_size, dtype=np.bool)\n",
        "\n",
        "    #stores new transitions in memory\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.current_position % self.max_size\n",
        "\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.current_position+= 1\n",
        "\n",
        "    #gives back a random batch of of transition samples\n",
        "    def sample_buffer(self, batch_size):\n",
        "        #makes sure to have the correct current size of the memory\n",
        "        max_mem = min(self.current_position, self.max_size)\n",
        "\n",
        "        #selects a random batch of indexes in the memory size\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        #retrieves the batch from memory\n",
        "        states = self.state_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, dones"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yWD6XYVwZNH"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# the agent class where the all the important parameters and systems of the\n",
        "# model are managed\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, input_dims, alpha=0.001, beta=0.002, env=None,\n",
        "            gamma=0.99, n_actions=4, max_size=1000000, tau=0.001,\n",
        "            dense1=512, dense2=512, batch_size=64, noise=0.3, module_dir = \"\"):\n",
        "\n",
        "        #initializing network-parameters\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.n_actions = n_actions\n",
        "        self.noise = noise\n",
        "\n",
        "        #retrieves the maximum and minimum of the actionvalues\n",
        "        self.max_action = env.action_space.high[0]\n",
        "        self.min_action = env.action_space.low[0]\n",
        "\n",
        "        #initializes the Replaybuffer which stores what the agents does\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "\n",
        "        # initializing the Networks with given parameters\n",
        "        # target_actor and target_critic are just initialized as the actor and\n",
        "        # critic networks\n",
        "\n",
        "        chkpt_dir = module_dir+\"/tmp\"\n",
        "        self.actor = ActorNetwork(n_actions=n_actions, name='actor', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_actor = ActorNetwork(n_actions=n_actions, name='target_actor',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "        self.critic = CriticNetwork(name='critic',  dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "        self.target_critic = CriticNetwork(name='target_critic', dense1 = dense1, dense2 = dense2, chkpt_dir=chkpt_dir)\n",
        "\n",
        "\n",
        "        #compile the networks with learningrates\n",
        "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha))\n",
        "\n",
        "        self.critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "        self.target_critic.compile(optimizer=Adam(learning_rate=beta))\n",
        "\n",
        "        self.update_network_parameters(tau=1) #Hard copy, since this is the initialization\n",
        "\n",
        "    #updates the target networks\n",
        "    #soft copies the target and actor network dependent on tau\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_actor.weights\n",
        "        for i, weight in enumerate(self.actor.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_actor.set_weights(weights)\n",
        "\n",
        "        weights = []\n",
        "        targets = self.target_critic.weights\n",
        "        for i, weight in enumerate(self.critic.weights):\n",
        "            weights.append(weight * tau + targets[i]*(1-tau))\n",
        "        self.target_critic.set_weights(weights)\n",
        "\n",
        "    #stores the state, action, reward transition\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    #saves models in files\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.save_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.save_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    #loads models from files\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_weights(self.actor.checkpoint_file)\n",
        "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
        "        self.critic.load_weights(self.critic.checkpoint_file)\n",
        "        self.target_critic.load_weights(self.target_critic.checkpoint_file)\n",
        "\n",
        "    #choose_action with help of the actor network, adds noise if it for training\n",
        "    def choose_action(self, observation, evaluate=False):\n",
        "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
        "        actions = self.actor(state)\n",
        "\n",
        "        # inject eploration noise\n",
        "        if not evaluate:\n",
        "\n",
        "            actions += tf.random.normal(shape=[self.n_actions],\n",
        "                    mean=0.0, stddev=self.noise)\n",
        "\n",
        "        #makes sure that action boundaries are met\n",
        "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
        "\n",
        "        return actions[0]\n",
        "\n",
        "    #learn function of the networks\n",
        "    def learn(self):\n",
        "        #starts to learn when there are enough samples to fill a batch\n",
        "        if self.memory.current_position < self.batch_size:\n",
        "            return\n",
        "\n",
        "        #gets batch form memory\n",
        "        state, action, reward, new_state, done = \\\n",
        "                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        #convert np arrays to tensors to feed them to the networks\n",
        "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        states_ = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
        "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
        "\n",
        "        #update critic network\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            #target actor decides which action to take\n",
        "            target_actions = self.target_actor(states_)\n",
        "            #target critic evaluates the value of the actions in the given states\n",
        "            critic_value_ = tf.squeeze(self.target_critic(\n",
        "                                states_, target_actions), 1)\n",
        "\n",
        "            #critic network evaluate the actual states and actions the model took\n",
        "            critic_value = tf.squeeze(self.critic(states, actions), 1)\n",
        "\n",
        "            #target says what value of the action in a certain state should\n",
        "            #be like\n",
        "            target = rewards + self.gamma*critic_value_*(1-done)\n",
        "\n",
        "            #takes the MSE of the target and the actual critic value as the loss\n",
        "            critic_loss = keras.losses.MSE(target, critic_value)\n",
        "\n",
        "\n",
        "        #gets the gradients of the loss in respect to the parameters of the network\n",
        "        critic_network_gradient = tape.gradient(critic_loss,\n",
        "                                            self.critic.trainable_variables)\n",
        "\n",
        "        #aplies the gradients to the critic network\n",
        "        self.critic.optimizer.apply_gradients(zip(\n",
        "            critic_network_gradient, self.critic.trainable_variables))\n",
        "\n",
        "        #update the actor network\n",
        "        with tf.GradientTape() as tape:\n",
        "            #gets the policy of the actor in a state\n",
        "            action_policy = self.actor(states)\n",
        "\n",
        "            #loss of the actor is the negative value of the critic because we\n",
        "            #want to maximize the value but gradient decent minimizes\n",
        "            actor_loss = -self.critic(states, action_policy)\n",
        "\n",
        "            #the loss is a average of all the losses\n",
        "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
        "\n",
        "        #gradients of the loss in respect to the parameters of the actor network\n",
        "        actor_network_gradient = tape.gradient(actor_loss,\n",
        "                                    self.actor.trainable_variables)\n",
        "\n",
        "\n",
        "        #optimizing the network gradients\n",
        "        self.actor.optimizer.apply_gradients(zip(\n",
        "            actor_network_gradient, self.actor.trainable_variables))\n",
        "\n",
        "        self.update_network_parameters()\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppcQq5rdHQfg"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IbFPh3tnwdlA",
        "outputId": "38be4d36-8c95-47e4-ee93-64fc2bad5a3d"
      },
      "source": [
        "import sys\n",
        "print(f\"Python version {sys.version}\")\n",
        "print(sys.path)\n",
        "\n",
        "import os\n",
        "print(f\"Current working directory is {os.getcwd()}\")\n",
        "\n",
        "import gym\n",
        "import gym.envs.box2d\n",
        "import tensorflow as tf\n",
        "print(f\"TF version: {tf.version.VERSION}\")\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "# modules\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    #for the case you just want to load a previous model\n",
        "    load_checkpoint = False\n",
        "\n",
        "    #Housekeeping variables\n",
        "    last_score = 0\n",
        "    last_avg_score = 0\n",
        "    last_save = 0\n",
        "    avg_delta = []\n",
        "    avg_steps = []\n",
        "\n",
        "    t = t_start = time.localtime()\n",
        "    current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t)\n",
        "    print(f\"\\n----------------- Training started at {current_time}. -------------------\\ncheckpoint: {load_checkpoint}\")\n",
        "\n",
        "    module_dir = \"\"\n",
        "    figure_dir = module_dir+f'plots/'\n",
        "    figure_file = figure_dir+f'walker{current_time.replace(\":\",\"_\")}.png'\n",
        "\n",
        "    log_dir = module_dir+'logs/' + current_time.replace(\":\",\"_\")\n",
        "\n",
        "    #Tensorboard writer\n",
        "    writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "\n",
        "    #initialize the environment for the agent and initialize the agent\n",
        "\n",
        "    #tf.debugging.set_log_device_placement(True)\n",
        "    env = gym.make(\"BipedalWalker-v3\")\n",
        "    #env = gym.make('BipedalWalkerHardcore-v3')\n",
        "\n",
        "\n",
        "    n_actions = env.action_space.shape[0]\n",
        "\n",
        "    noise = 0.3\n",
        "    # NEW batch 128\n",
        "    agent = Agent(alpha=0.00005, beta=0.0005, input_dims=env.observation_space.shape, tau=0.001, env=env,\n",
        "                  batch_size=64, dense1=400, dense2=300, n_actions=n_actions, noise = noise, module_dir = module_dir)\n",
        "\n",
        "\n",
        "    episodes = 4000\n",
        "\n",
        "\n",
        "\n",
        "    #set bestscore to minimum\n",
        "    best_score = env.reward_range[0]\n",
        "    score_history = []\n",
        "\n",
        "\n",
        "    #initializes the model with one random sample batch if model are loaded\n",
        "    #you can't load an empty model for some reason\n",
        "    #these are all dummy variables etc until load_models overwrites them\n",
        "    if load_checkpoint:\n",
        "        n_steps = 0\n",
        "        while n_steps <= agent.batch_size:\n",
        "            observation = env.reset()\n",
        "            action = env.action_space.sample()\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            agent.remember(observation, action, reward, observation_, done)\n",
        "            n_steps += 1\n",
        "        agent.learn()\n",
        "        agent.load_models()\n",
        "\n",
        "    # ---------------------------------------\n",
        "    # main learning loop\n",
        "    # ---------------------------------------\n",
        "    try:\n",
        "        for i in range(episodes):\n",
        "            if i == 3:\n",
        "                tf.profiler.experimental.server.start(6009)\n",
        "                print(\"profiler started\")\n",
        "                # launch tensorboard with \"tensorboard --logdir logs\"\n",
        "                # capture profile\n",
        "            #if i == 13:\n",
        "            #    tf.profiler.experimental.stop\n",
        "            #    print(\"profiler stopped\")\n",
        "\n",
        "\n",
        "            current_episode = i\n",
        "            observation = env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "\n",
        "            #regulates the noise over the course of training as exponential\n",
        "            #decay to get smaller noise at the end, the noise is the\n",
        "            #standarddeviation of a normal distribution\n",
        "            #(numbers from trial and error)\n",
        "            agent.noise = noise * np.exp(-i/1500)\n",
        "\n",
        "            #while the environment is running the model chooses actions, saves states,\n",
        "            #rewards, actions and observations in the buffer and trains the networks\n",
        "            #on them\n",
        "            steps = 0\n",
        "            while not done:\n",
        "                action = agent.choose_action(observation)\n",
        "\n",
        "                observation_, reward, done, info = env.step(action)\n",
        "                steps += 1\n",
        "                score += reward\n",
        "                agent.remember(observation, action, reward, observation_, done)\n",
        "                agent.learn()\n",
        "\n",
        "                #saves previous observation\n",
        "                observation = observation_\n",
        "\n",
        "            score_history.append(score)\n",
        "            avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "            #saves the model if the average score is better than the best previous\n",
        "            if avg_score > best_score:\n",
        "                best_score = avg_score\n",
        "                agent.save_models()\n",
        "                last_save = current_episode\n",
        "\n",
        "            #calculating and giving some info on training progress\n",
        "            t_new = time.localtime()\n",
        "            current_time = time.strftime(\"%H:%M:%S\", t_new)\n",
        "            t_delta = time.mktime(t_new)-time.mktime(t)\n",
        "            t = t_new\n",
        "            avg_delta.append(t_delta)\n",
        "            avg_delta_mean = np.mean(avg_delta)\n",
        "            avg_delta_std = np.var(avg_delta)\n",
        "\n",
        "            ETA_avg = (episodes-i)*avg_delta_mean\n",
        "            ETA_min = (episodes-i)*max((avg_delta_mean-avg_delta_std),min(avg_delta))\n",
        "            ETA_max = (episodes-i)*(avg_delta_mean+avg_delta_std)\n",
        "\n",
        "            avg_steps.append(steps)\n",
        "            per_step = t_delta/steps\n",
        "            steps_per_score = score/steps\n",
        "\n",
        "            print(f\"{current_time} \\n\"\n",
        "            f'Episode: **{i+1}**/{episodes}, Score: {score:.0f} (Δ{score-last_score:5.1f})\\n'\n",
        "            f'Average score: {avg_score:.1f} (Δ{avg_score-last_avg_score:5.2f})\\n'\n",
        "            f'Episode time: {t_delta:.1f}s, average: {avg_delta_mean:.1f}s (±{avg_delta_std:4.2f}),',\n",
        "            f'ETA: {timespan_format(ETA_avg)} ({timespan_format(ETA_min)} to {timespan_format(ETA_max)})\\n'\n",
        "            f'Steps: {steps}. Time per step: {per_step:.1e}s. Reward per step: {steps_per_score:.2f}.\\n'\n",
        "            f'It has been {i - last_save} episode(s) since the model was last saved, with a score of {best_score:.0f} (Δ{avg_score-best_score:2.2f}).\\n')\n",
        "\n",
        "            last_score = score\n",
        "            last_avg_score = avg_score\n",
        "\n",
        "\n",
        "            with writer.as_default():\n",
        "                tf.summary.scalar('Average Score', avg_score, step=i)\n",
        "                tf.summary.scalar('ETA', ETA_avg, step=i)\n",
        "                tf.summary.scalar('Calculation time per step', per_step, step=i)\n",
        "                tf.summary.scalar('Calculation time per episode', t_delta, step=i)\n",
        "                tf.summary.scalar('Steps', steps, step=i)\n",
        "                if ((i+1) % 50) == 0: #writer.flush and learning plot has a large performance impact, so only do it every 50 episodes\n",
        "                    writer.flush()\n",
        "                    x = [j+1 for j in range(current_episode+1)]\n",
        "                    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        episodes = current_episode\n",
        "        print(\"Manually shutting down training.\")\n",
        "\n",
        "    #plots the whole score history\n",
        "    x = [i+1 for i in range(episodes)]\n",
        "    plot_learning_curve(x, score_history, figure_file)\n",
        "\n",
        "t2 = time.localtime()\n",
        "current_time = time.strftime(\"%Y-%m-%d-%H:%M:%S\", t2)\n",
        "t_delta = time.mktime(t2)-time.mktime(t_start)\n",
        "print(f\"\\n----------------- Training ended at {current_time}. Duration was {t_delta/60:.2f} minutes.-----------------\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version 3.7.10 (default, Feb 20 2021, 21:17:23) \n",
            "[GCC 7.5.0]\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "Current working directory is /content\n",
            "TF version: 2.4.1\n",
            "\n",
            "----------------- Training started at 2021-04-01-11:18:35. -------------------\n",
            "checkpoint: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "... saving models ...\n",
            "11:18:38 \n",
            "Episode: **1**/4000, Score: -95 (Δ-94.6)\n",
            "Average score: -94.6 (Δ-94.55)\n",
            "Episode time: 3.0s, average: 3.0s (±0.00), ETA: 03:20:00 (03:20:00 to 03:20:00)\n",
            "Steps: 102. Time per step: 2.9e-02s. Reward per step: -0.93.\n",
            "It has been 0 episode(s) since the model was last saved, with a score of -95 (Δ0.00).\n",
            "\n",
            "11:18:40 \n",
            "Episode: **2**/4000, Score: -111 (Δ-16.4)\n",
            "Average score: -102.8 (Δ-8.22)\n",
            "Episode time: 2.0s, average: 2.5s (±0.25), ETA: 02:46:38 (02:29:58 to 03:03:17)\n",
            "Steps: 65. Time per step: 3.1e-02s. Reward per step: -1.71.\n",
            "It has been 1 episode(s) since the model was last saved, with a score of -95 (Δ-8.22).\n",
            "\n",
            "11:18:48 \n",
            "Episode: **3**/4000, Score: -134 (Δ-23.0)\n",
            "Average score: -113.2 (Δ-10.39)\n",
            "Episode time: 8.0s, average: 4.3s (±6.89), ETA: 04:48:45 (02:13:16 to 12:27:46)\n",
            "Steps: 211. Time per step: 3.8e-02s. Reward per step: -0.63.\n",
            "It has been 2 episode(s) since the model was last saved, with a score of -95 (Δ-18.62).\n",
            "\n",
            "profiler started\n",
            "11:19:00 \n",
            "Episode: **4**/4000, Score: -150 (Δ-16.2)\n",
            "Average score: -122.4 (Δ-9.25)\n",
            "Episode time: 12.0s, average: 6.2s (±16.19), ETA: 06:56:21 (02:13:14 to 01::00:54:43)\n",
            "Steps: 313. Time per step: 3.8e-02s. Reward per step: -0.48.\n",
            "It has been 3 episode(s) since the model was last saved, with a score of -95 (Δ-27.87).\n",
            "\n",
            "11:19:02 \n",
            "Episode: **5**/4000, Score: -101 (Δ 49.5)\n",
            "Average score: -118.1 (Δ 4.36)\n",
            "Episode time: 2.0s, average: 5.4s (±15.84), ETA: 05:59:38 (02:13:12 to 23:34:35)\n",
            "Steps: 69. Time per step: 2.9e-02s. Reward per step: -1.46.\n",
            "It has been 4 episode(s) since the model was last saved, with a score of -95 (Δ-23.52).\n",
            "\n",
            "11:19:06 \n",
            "Episode: **6**/4000, Score: -103 (Δ -2.6)\n",
            "Average score: -115.6 (Δ 2.46)\n",
            "Episode time: 4.0s, average: 5.2s (±13.47), ETA: 05:44:01 (02:13:10 to 20:41:02)\n",
            "Steps: 91. Time per step: 4.4e-02s. Reward per step: -1.13.\n",
            "It has been 5 episode(s) since the model was last saved, with a score of -95 (Δ-21.05).\n",
            "\n",
            "11:19:10 \n",
            "Episode: **7**/4000, Score: -105 (Δ -2.0)\n",
            "Average score: -114.1 (Δ 1.48)\n",
            "Episode time: 4.0s, average: 5.0s (±11.71), ETA: 05:32:50 (02:13:08 to 18:32:37)\n",
            "Steps: 107. Time per step: 3.7e-02s. Reward per step: -0.98.\n",
            "It has been 6 episode(s) since the model was last saved, with a score of -95 (Δ-19.58).\n",
            "\n",
            "11:19:13 \n",
            "Episode: **8**/4000, Score: -102 (Δ  3.3)\n",
            "Average score: -112.6 (Δ 1.52)\n",
            "Episode time: 3.0s, average: 4.8s (±10.69), ETA: 05:16:07 (02:13:06 to 17:07:22)\n",
            "Steps: 96. Time per step: 3.1e-02s. Reward per step: -1.06.\n",
            "It has been 7 episode(s) since the model was last saved, with a score of -95 (Δ-18.05).\n",
            "\n",
            "11:19:18 \n",
            "Episode: **9**/4000, Score: -102 (Δ -0.1)\n",
            "Average score: -111.4 (Δ 1.17)\n",
            "Episode time: 5.0s, average: 4.8s (±9.51), ETA: 05:17:53 (02:13:04 to 15:50:22)\n",
            "Steps: 126. Time per step: 4.0e-02s. Reward per step: -0.81.\n",
            "It has been 8 episode(s) since the model was last saved, with a score of -95 (Δ-16.88).\n",
            "\n",
            "11:20:18 \n",
            "Episode: **10**/4000, Score: -126 (Δ-23.8)\n",
            "Average score: -112.9 (Δ-1.44)\n",
            "Episode time: 60.0s, average: 10.3s (±283.01), ETA: 11:25:07 (02:13:02 to 13::13:10:00)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.08.\n",
            "It has been 9 episode(s) since the model was last saved, with a score of -95 (Δ-18.32).\n",
            "\n",
            "11:20:21 \n",
            "Episode: **11**/4000, Score: -111 (Δ 14.6)\n",
            "Average score: -112.7 (Δ 0.15)\n",
            "Episode time: 3.0s, average: 9.6s (±261.69), ETA: 10:40:49 (02:13:00 to 12::12:42:56)\n",
            "Steps: 90. Time per step: 3.3e-02s. Reward per step: -1.24.\n",
            "It has been 10 episode(s) since the model was last saved, with a score of -95 (Δ-18.16).\n",
            "\n",
            "11:20:25 \n",
            "Episode: **12**/4000, Score: -112 (Δ -1.3)\n",
            "Average score: -112.7 (Δ 0.02)\n",
            "Episode time: 4.0s, average: 9.2s (±242.31), ETA: 10:09:26 (02:12:58 to 11::14:38:43)\n",
            "Steps: 90. Time per step: 4.4e-02s. Reward per step: -1.25.\n",
            "It has been 11 episode(s) since the model was last saved, with a score of -95 (Δ-18.14).\n",
            "\n",
            "11:20:27 \n",
            "Episode: **13**/4000, Score: -114 (Δ -1.0)\n",
            "Average score: -112.8 (Δ-0.06)\n",
            "Episode time: 2.0s, average: 8.6s (±227.31), ETA: 09:32:38 (02:12:56 to 10::21:21:25)\n",
            "Steps: 63. Time per step: 3.2e-02s. Reward per step: -1.80.\n",
            "It has been 12 episode(s) since the model was last saved, with a score of -95 (Δ-18.20).\n",
            "\n",
            "11:20:32 \n",
            "Episode: **14**/4000, Score: -114 (Δ -0.6)\n",
            "Average score: -112.9 (Δ-0.10)\n",
            "Episode time: 5.0s, average: 8.4s (±211.94), ETA: 09:15:20 (02:12:54 to 10::03:59:00)\n",
            "Steps: 120. Time per step: 4.2e-02s. Reward per step: -0.95.\n",
            "It has been 13 episode(s) since the model was last saved, with a score of -95 (Δ-18.30).\n",
            "\n",
            "11:21:32 \n",
            "Episode: **15**/4000, Score: -159 (Δ-44.7)\n",
            "Average score: -115.9 (Δ-3.07)\n",
            "Episode time: 60.0s, average: 11.8s (±363.76), ETA: 13:03:55 (02:12:52 to 17::07:49:42)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 14 episode(s) since the model was last saved, with a score of -95 (Δ-21.37).\n",
            "\n",
            "11:22:31 \n",
            "Episode: **16**/4000, Score: -155 (Δ  4.1)\n",
            "Average score: -118.3 (Δ-2.43)\n",
            "Episode time: 59.0s, average: 14.8s (±471.56), ETA: 16:19:39 (02:12:50 to 22::10:19:15)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 15 episode(s) since the model was last saved, with a score of -95 (Δ-23.79).\n",
            "\n",
            "11:23:32 \n",
            "Episode: **17**/4000, Score: -142 (Δ 12.3)\n",
            "Average score: -119.8 (Δ-1.42)\n",
            "Episode time: 61.0s, average: 17.5s (±562.25), ETA: 19:20:03 (02:12:48 to 26::17:33:23)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.09.\n",
            "It has been 16 episode(s) since the model was last saved, with a score of -95 (Δ-25.21).\n",
            "\n",
            "11:24:31 \n",
            "Episode: **18**/4000, Score: -142 (Δ  0.7)\n",
            "Average score: -121.0 (Δ-1.22)\n",
            "Episode time: 59.0s, average: 19.8s (±621.51), ETA: 21:52:55 (02:12:46 to 29::13:30:34)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.09.\n",
            "It has been 17 episode(s) since the model was last saved, with a score of -95 (Δ-26.43).\n",
            "\n",
            "11:25:31 \n",
            "Episode: **19**/4000, Score: -149 (Δ -7.5)\n",
            "Average score: -122.5 (Δ-1.49)\n",
            "Episode time: 60.0s, average: 21.9s (±669.46), ETA: 01::00:13:05 (02:12:44 to 31::20:43:05)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.09.\n",
            "It has been 18 episode(s) since the model was last saved, with a score of -95 (Δ-27.92).\n",
            "\n",
            "11:26:31 \n",
            "Episode: **20**/4000, Score: -156 (Δ -7.0)\n",
            "Average score: -124.2 (Δ-1.69)\n",
            "Episode time: 60.0s, average: 23.8s (±704.96), ETA: 01::02:19:08 (02:12:42 to 33::13:53:14)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 19 episode(s) since the model was last saved, with a score of -95 (Δ-29.61).\n",
            "\n",
            "11:27:31 \n",
            "Episode: **21**/4000, Score: -160 (Δ -4.0)\n",
            "Average score: -125.9 (Δ-1.72)\n",
            "Episode time: 60.0s, average: 25.5s (±730.82), ETA: 01::04:13:05 (02:12:40 to 34::20:10:52)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 20 episode(s) since the model was last saved, with a score of -95 (Δ-31.32).\n",
            "\n",
            "11:28:32 \n",
            "Episode: **22**/4000, Score: -148 (Δ 12.7)\n",
            "Average score: -126.9 (Δ-0.98)\n",
            "Episode time: 61.0s, average: 27.1s (±752.21), ETA: 01::05:59:36 (02:12:38 to 35::21:23:34)\n",
            "Steps: 1600. Time per step: 3.8e-02s. Reward per step: -0.09.\n",
            "It has been 21 episode(s) since the model was last saved, with a score of -95 (Δ-32.31).\n",
            "\n",
            "11:28:38 \n",
            "Episode: **23**/4000, Score: -117 (Δ 30.1)\n",
            "Average score: -126.4 (Δ 0.41)\n",
            "Episode time: 6.0s, average: 26.2s (±738.08), ETA: 01::04:58:13 (02:12:36 to 35::04:33:08)\n",
            "Steps: 154. Time per step: 3.9e-02s. Reward per step: -0.76.\n",
            "It has been 22 episode(s) since the model was last saved, with a score of -95 (Δ-31.90).\n",
            "\n",
            "11:28:55 \n",
            "Episode: **24**/4000, Score: -150 (Δ-32.4)\n",
            "Average score: -127.4 (Δ-0.97)\n",
            "Episode time: 17.0s, average: 25.8s (±710.72), ETA: 01::04:32:19 (02:12:34 to 33::21:41:21)\n",
            "Steps: 460. Time per step: 3.7e-02s. Reward per step: -0.33.\n",
            "It has been 23 episode(s) since the model was last saved, with a score of -95 (Δ-32.87).\n",
            "\n",
            "11:28:56 \n",
            "Episode: **25**/4000, Score: -112 (Δ 38.0)\n",
            "Average score: -126.8 (Δ 0.62)\n",
            "Episode time: 1.0s, average: 24.8s (±705.97), ETA: 01::03:26:04 (01:06:16 to 33::15:08:38)\n",
            "Steps: 32. Time per step: 3.1e-02s. Reward per step: -3.49.\n",
            "It has been 24 episode(s) since the model was last saved, with a score of -95 (Δ-32.25).\n",
            "\n",
            "11:28:57 \n",
            "Episode: **26**/4000, Score: -112 (Δ -0.5)\n",
            "Average score: -126.2 (Δ 0.56)\n",
            "Episode time: 1.0s, average: 23.9s (±699.84), ETA: 01::02:24:54 (01:06:15 to 33::07:09:19)\n",
            "Steps: 32. Time per step: 3.1e-02s. Reward per step: -3.51.\n",
            "It has been 25 episode(s) since the model was last saved, with a score of -95 (Δ-31.69).\n",
            "\n",
            "11:28:58 \n",
            "Episode: **27**/4000, Score: -112 (Δ  0.3)\n",
            "Average score: -125.7 (Δ 0.53)\n",
            "Episode time: 1.0s, average: 23.1s (±692.66), ETA: 01::01:28:16 (01:06:14 to 32::22:05:32)\n",
            "Steps: 32. Time per step: 3.1e-02s. Reward per step: -3.50.\n",
            "It has been 26 episode(s) since the model was last saved, with a score of -95 (Δ-31.16).\n",
            "\n",
            "11:29:00 \n",
            "Episode: **28**/4000, Score: -112 (Δ -0.0)\n",
            "Average score: -125.2 (Δ 0.49)\n",
            "Episode time: 2.0s, average: 22.3s (±683.22), ETA: 01::00:38:03 (01:06:13 to 32::10:38:29)\n",
            "Steps: 32. Time per step: 6.2e-02s. Reward per step: -3.50.\n",
            "It has been 27 episode(s) since the model was last saved, with a score of -95 (Δ-30.67).\n",
            "\n",
            "11:29:01 \n",
            "Episode: **29**/4000, Score: -114 (Δ -2.0)\n",
            "Average score: -124.8 (Δ 0.39)\n",
            "Episode time: 1.0s, average: 21.6s (±674.79), ETA: 23:49:00 (01:06:12 to 32::00:20:23)\n",
            "Steps: 34. Time per step: 2.9e-02s. Reward per step: -3.35.\n",
            "It has been 28 episode(s) since the model was last saved, with a score of -95 (Δ-30.28).\n",
            "\n",
            "11:29:02 \n",
            "Episode: **30**/4000, Score: -112 (Δ  2.0)\n",
            "Average score: -124.4 (Δ 0.43)\n",
            "Episode time: 1.0s, average: 20.9s (±665.96), ETA: 23:03:14 (01:06:11 to 31::13:38:28)\n",
            "Steps: 32. Time per step: 3.1e-02s. Reward per step: -3.50.\n",
            "It has been 29 episode(s) since the model was last saved, with a score of -95 (Δ-29.86).\n",
            "\n",
            "11:29:03 \n",
            "Episode: **31**/4000, Score: -113 (Δ -1.4)\n",
            "Average score: -124.1 (Δ 0.36)\n",
            "Episode time: 1.0s, average: 20.3s (±656.84), ETA: 22:20:25 (01:06:10 to 31::02:41:06)\n",
            "Steps: 33. Time per step: 3.0e-02s. Reward per step: -3.44.\n",
            "It has been 30 episode(s) since the model was last saved, with a score of -95 (Δ-29.50).\n",
            "\n",
            "11:29:05 \n",
            "Episode: **32**/4000, Score: -112 (Δ  1.0)\n",
            "Average score: -123.7 (Δ 0.37)\n",
            "Episode time: 2.0s, average: 19.7s (±646.40), ETA: 21:42:20 (01:06:09 to 30::14:21:51)\n",
            "Steps: 33. Time per step: 6.1e-02s. Reward per step: -3.40.\n",
            "It has been 31 episode(s) since the model was last saved, with a score of -95 (Δ-29.14).\n",
            "\n",
            "11:29:06 \n",
            "Episode: **33**/4000, Score: -114 (Δ -1.5)\n",
            "Average score: -123.4 (Δ 0.30)\n",
            "Episode time: 1.0s, average: 19.1s (±637.08), ETA: 21:04:33 (01:06:08 to 30::03:16:31)\n",
            "Steps: 33. Time per step: 3.0e-02s. Reward per step: -3.45.\n",
            "It has been 32 episode(s) since the model was last saved, with a score of -95 (Δ-28.84).\n",
            "\n",
            "11:29:07 \n",
            "Episode: **34**/4000, Score: -111 (Δ  2.5)\n",
            "Average score: -123.0 (Δ 0.36)\n",
            "Episode time: 1.0s, average: 18.6s (±627.71), ETA: 20:29:00 (01:06:07 to 29::16:11:16)\n",
            "Steps: 37. Time per step: 2.7e-02s. Reward per step: -3.01.\n",
            "It has been 33 episode(s) since the model was last saved, with a score of -95 (Δ-28.48).\n",
            "\n",
            "11:30:07 \n",
            "Episode: **35**/4000, Score: -158 (Δ-46.9)\n",
            "Average score: -124.0 (Δ-1.00)\n",
            "Episode time: 60.0s, average: 19.8s (±657.38), ETA: 21:46:53 (01:06:06 to 31::01:59:28)\n",
            "Steps: 1600. Time per step: 3.7e-02s. Reward per step: -0.10.\n",
            "It has been 34 episode(s) since the model was last saved, with a score of -95 (Δ-29.49).\n",
            "\n",
            "11:30:10 \n",
            "Episode: **36**/4000, Score: -118 (Δ 39.9)\n",
            "Average score: -123.9 (Δ 0.16)\n",
            "Episode time: 3.0s, average: 19.3s (±646.71), ETA: 21:15:47 (01:06:05 to 30::13:32:40)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.48.\n",
            "It has been 35 episode(s) since the model was last saved, with a score of -95 (Δ-29.33).\n",
            "\n",
            "11:30:13 \n",
            "Episode: **37**/4000, Score: -118 (Δ  0.3)\n",
            "Average score: -123.7 (Δ 0.16)\n",
            "Episode time: 3.0s, average: 18.9s (±636.22), ETA: 20:46:20 (01:06:04 to 30::01:19:36)\n",
            "Steps: 79. Time per step: 3.8e-02s. Reward per step: -1.49.\n",
            "It has been 36 episode(s) since the model was last saved, with a score of -95 (Δ-29.17).\n",
            "\n",
            "11:30:16 \n",
            "Episode: **38**/4000, Score: -117 (Δ  0.7)\n",
            "Average score: -123.5 (Δ 0.17)\n",
            "Episode time: 3.0s, average: 18.4s (±625.93), ETA: 20:18:27 (01:06:03 to 29::13:21:13)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.65.\n",
            "It has been 37 episode(s) since the model was last saved, with a score of -95 (Δ-29.00).\n",
            "\n",
            "11:30:19 \n",
            "Episode: **39**/4000, Score: -119 (Δ -1.4)\n",
            "Average score: -123.4 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 18.1s (±615.84), ETA: 19:51:59 (01:06:02 to 29::01:38:11)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.46.\n",
            "It has been 38 episode(s) since the model was last saved, with a score of -95 (Δ-28.87).\n",
            "\n",
            "11:30:22 \n",
            "Episode: **40**/4000, Score: -118 (Δ  0.6)\n",
            "Average score: -123.3 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 17.7s (±605.97), ETA: 19:26:51 (01:06:01 to 28::14:10:55)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.48.\n",
            "It has been 39 episode(s) since the model was last saved, with a score of -95 (Δ-28.74).\n",
            "\n",
            "11:30:25 \n",
            "Episode: **41**/4000, Score: -117 (Δ  0.6)\n",
            "Average score: -123.1 (Δ 0.14)\n",
            "Episode time: 3.0s, average: 17.3s (±596.31), ETA: 19:02:56 (01:06:00 to 28::02:59:39)\n",
            "Steps: 72. Time per step: 4.2e-02s. Reward per step: -1.63.\n",
            "It has been 40 episode(s) since the model was last saved, with a score of -95 (Δ-28.59).\n",
            "\n",
            "11:30:27 \n",
            "Episode: **42**/4000, Score: -117 (Δ  0.1)\n",
            "Average score: -123.0 (Δ 0.14)\n",
            "Episode time: 2.0s, average: 17.0s (±587.57), ETA: 18:38:34 (01:05:59 to 27::16:48:21)\n",
            "Steps: 69. Time per step: 2.9e-02s. Reward per step: -1.70.\n",
            "It has been 41 episode(s) since the model was last saved, with a score of -95 (Δ-28.46).\n",
            "\n",
            "11:30:30 \n",
            "Episode: **43**/4000, Score: -117 (Δ  0.2)\n",
            "Average score: -122.9 (Δ 0.14)\n",
            "Episode time: 3.0s, average: 16.6s (±578.33), ETA: 18:16:53 (01:05:58 to 27::06:07:10)\n",
            "Steps: 67. Time per step: 4.5e-02s. Reward per step: -1.75.\n",
            "It has been 42 episode(s) since the model was last saved, with a score of -95 (Δ-28.32).\n",
            "\n",
            "11:30:33 \n",
            "Episode: **44**/4000, Score: -119 (Δ -1.6)\n",
            "Average score: -122.8 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 16.3s (±569.31), ETA: 17:56:11 (01:05:57 to 26::19:42:02)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.47.\n",
            "It has been 43 episode(s) since the model was last saved, with a score of -95 (Δ-28.22).\n",
            "\n",
            "11:30:36 \n",
            "Episode: **45**/4000, Score: -117 (Δ  1.7)\n",
            "Average score: -122.6 (Δ 0.13)\n",
            "Episode time: 3.0s, average: 16.0s (±560.51), ETA: 17:36:24 (01:05:56 to 26::09:32:44)\n",
            "Steps: 72. Time per step: 4.2e-02s. Reward per step: -1.63.\n",
            "It has been 44 episode(s) since the model was last saved, with a score of -95 (Δ-28.10).\n",
            "\n",
            "11:30:39 \n",
            "Episode: **46**/4000, Score: -118 (Δ -1.1)\n",
            "Average score: -122.5 (Δ 0.10)\n",
            "Episode time: 3.0s, average: 15.7s (±551.93), ETA: 17:17:28 (01:05:55 to 25::23:38:59)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.48.\n",
            "It has been 45 episode(s) since the model was last saved, with a score of -95 (Δ-28.00).\n",
            "\n",
            "11:30:42 \n",
            "Episode: **47**/4000, Score: -119 (Δ -0.4)\n",
            "Average score: -122.5 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 15.5s (±543.57), ETA: 16:59:21 (01:05:54 to 25::14:00:29)\n",
            "Steps: 81. Time per step: 3.7e-02s. Reward per step: -1.46.\n",
            "It has been 46 episode(s) since the model was last saved, with a score of -95 (Δ-27.91).\n",
            "\n",
            "11:30:45 \n",
            "Episode: **48**/4000, Score: -119 (Δ -0.3)\n",
            "Average score: -122.4 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 15.2s (±535.41), ETA: 16:41:59 (01:05:53 to 25::04:36:54)\n",
            "Steps: 83. Time per step: 3.6e-02s. Reward per step: -1.43.\n",
            "It has been 47 episode(s) since the model was last saved, with a score of -95 (Δ-27.84).\n",
            "\n",
            "11:30:48 \n",
            "Episode: **49**/4000, Score: -118 (Δ  1.0)\n",
            "Average score: -122.3 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 15.0s (±527.47), ETA: 16:25:19 (01:05:52 to 24::19:27:51)\n",
            "Steps: 73. Time per step: 4.1e-02s. Reward per step: -1.61.\n",
            "It has been 48 episode(s) since the model was last saved, with a score of -95 (Δ-27.74).\n",
            "\n",
            "11:30:51 \n",
            "Episode: **50**/4000, Score: -118 (Δ -0.4)\n",
            "Average score: -122.2 (Δ 0.08)\n",
            "Episode time: 3.0s, average: 14.7s (±519.72), ETA: 16:09:19 (01:05:51 to 24::10:32:59)\n",
            "Steps: 80. Time per step: 3.7e-02s. Reward per step: -1.48.\n",
            "It has been 49 episode(s) since the model was last saved, with a score of -95 (Δ-27.66).\n",
            "\n",
            "11:30:54 \n",
            "Episode: **51**/4000, Score: -118 (Δ  0.4)\n",
            "Average score: -122.1 (Δ 0.09)\n",
            "Episode time: 3.0s, average: 14.5s (±512.17), ETA: 15:53:56 (01:05:50 to 24::01:51:54)\n",
            "Steps: 79. Time per step: 3.8e-02s. Reward per step: -1.49.\n",
            "It has been 50 episode(s) since the model was last saved, with a score of -95 (Δ-27.57).\n",
            "\n",
            "11:30:56 \n",
            "Episode: **52**/4000, Score: -116 (Δ  2.3)\n",
            "Average score: -122.0 (Δ 0.13)\n",
            "Episode time: 2.0s, average: 14.2s (±505.26), ETA: 15:37:53 (01:05:49 to 23::17:52:42)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -2.14.\n",
            "It has been 51 episode(s) since the model was last saved, with a score of -95 (Δ-27.45).\n",
            "\n",
            "11:30:58 \n",
            "Episode: **53**/4000, Score: -114 (Δ  1.6)\n",
            "Average score: -121.8 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 14.0s (±498.51), ETA: 15:22:26 (01:05:48 to 23::10:04:20)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.33.\n",
            "It has been 52 episode(s) since the model was last saved, with a score of -95 (Δ-27.29).\n",
            "\n",
            "11:31:00 \n",
            "Episode: **54**/4000, Score: -114 (Δ -0.1)\n",
            "Average score: -121.7 (Δ 0.14)\n",
            "Episode time: 2.0s, average: 13.8s (±491.90), ETA: 15:07:34 (01:05:47 to 23::02:26:35)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.38.\n",
            "It has been 53 episode(s) since the model was last saved, with a score of -95 (Δ-27.15).\n",
            "\n",
            "11:31:12 \n",
            "Episode: **55**/4000, Score: -120 (Δ -5.5)\n",
            "Average score: -121.7 (Δ 0.04)\n",
            "Episode time: 12.0s, average: 13.8s (±483.02), ETA: 15:05:11 (01:05:46 to 22::16:31:36)\n",
            "Steps: 321. Time per step: 3.7e-02s. Reward per step: -0.37.\n",
            "It has been 54 episode(s) since the model was last saved, with a score of -95 (Δ-27.11).\n",
            "\n",
            "11:31:53 \n",
            "Episode: **56**/4000, Score: -192 (Δ-72.7)\n",
            "Average score: -122.9 (Δ-1.26)\n",
            "Episode time: 41.0s, average: 14.2s (±487.40), ETA: 15:36:56 (01:05:45 to 22::21:43:36)\n",
            "Steps: 1102. Time per step: 3.7e-02s. Reward per step: -0.17.\n",
            "It has been 55 episode(s) since the model was last saved, with a score of -95 (Δ-28.38).\n",
            "\n",
            "11:31:55 \n",
            "Episode: **57**/4000, Score: -102 (Δ 90.0)\n",
            "Average score: -122.6 (Δ 0.36)\n",
            "Episode time: 2.0s, average: 14.0s (±481.44), ETA: 15:22:34 (01:05:44 to 22::14:49:03)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.13.\n",
            "It has been 56 episode(s) since the model was last saved, with a score of -95 (Δ-28.02).\n",
            "\n",
            "11:31:57 \n",
            "Episode: **58**/4000, Score: -101 (Δ  0.9)\n",
            "Average score: -122.2 (Δ 0.36)\n",
            "Episode time: 2.0s, average: 13.8s (±475.59), ETA: 15:08:42 (01:05:43 to 22::08:02:57)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.03.\n",
            "It has been 57 episode(s) since the model was last saved, with a score of -95 (Δ-27.65).\n",
            "\n",
            "11:31:59 \n",
            "Episode: **59**/4000, Score: -100 (Δ  1.1)\n",
            "Average score: -121.8 (Δ 0.37)\n",
            "Episode time: 2.0s, average: 13.6s (±469.86), ETA: 14:55:18 (01:05:42 to 22::01:25:10)\n",
            "Steps: 59. Time per step: 3.4e-02s. Reward per step: -1.70.\n",
            "It has been 58 episode(s) since the model was last saved, with a score of -95 (Δ-27.28).\n",
            "\n",
            "11:32:01 \n",
            "Episode: **60**/4000, Score: -100 (Δ  0.3)\n",
            "Average score: -121.5 (Δ 0.36)\n",
            "Episode time: 2.0s, average: 13.4s (±464.25), ETA: 14:42:21 (01:05:41 to 21::18:55:33)\n",
            "Steps: 57. Time per step: 3.5e-02s. Reward per step: -1.75.\n",
            "It has been 59 episode(s) since the model was last saved, with a score of -95 (Δ-26.91).\n",
            "\n",
            "11:32:04 \n",
            "Episode: **61**/4000, Score: -99 (Δ  1.0)\n",
            "Average score: -121.1 (Δ 0.37)\n",
            "Episode time: 3.0s, average: 13.3s (±458.39), ETA: 14:30:53 (01:05:40 to 21::12:11:51)\n",
            "Steps: 60. Time per step: 5.0e-02s. Reward per step: -1.65.\n",
            "It has been 60 episode(s) since the model was last saved, with a score of -95 (Δ-26.55).\n",
            "\n",
            "11:32:06 \n",
            "Episode: **62**/4000, Score: -101 (Δ -1.6)\n",
            "Average score: -120.8 (Δ 0.33)\n",
            "Episode time: 2.0s, average: 13.1s (±453.01), ETA: 14:18:45 (01:05:39 to 21::05:58:50)\n",
            "Steps: 57. Time per step: 3.5e-02s. Reward per step: -1.76.\n",
            "It has been 61 episode(s) since the model was last saved, with a score of -95 (Δ-26.22).\n",
            "\n",
            "11:32:08 \n",
            "Episode: **63**/4000, Score: -102 (Δ -1.0)\n",
            "Average score: -120.5 (Δ 0.30)\n",
            "Episode time: 2.0s, average: 12.9s (±447.74), ETA: 14:06:59 (01:05:38 to 20::23:53:27)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -1.95.\n",
            "It has been 62 episode(s) since the model was last saved, with a score of -95 (Δ-25.91).\n",
            "\n",
            "11:32:10 \n",
            "Episode: **64**/4000, Score: -101 (Δ  1.1)\n",
            "Average score: -120.1 (Δ 0.31)\n",
            "Episode time: 2.0s, average: 12.7s (±442.57), ETA: 13:55:35 (01:05:37 to 20::17:55:34)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.73.\n",
            "It has been 63 episode(s) since the model was last saved, with a score of -95 (Δ-25.60).\n",
            "\n",
            "11:32:12 \n",
            "Episode: **65**/4000, Score: -101 (Δ -0.3)\n",
            "Average score: -119.9 (Δ 0.30)\n",
            "Episode time: 2.0s, average: 12.6s (±437.51), ETA: 13:44:32 (01:05:36 to 20::12:04:59)\n",
            "Steps: 64. Time per step: 3.1e-02s. Reward per step: -1.58.\n",
            "It has been 64 episode(s) since the model was last saved, with a score of -95 (Δ-25.30).\n",
            "\n",
            "11:32:15 \n",
            "Episode: **66**/4000, Score: -99 (Δ  2.0)\n",
            "Average score: -119.5 (Δ 0.32)\n",
            "Episode time: 3.0s, average: 12.4s (±432.24), ETA: 13:34:49 (01:05:35 to 20::06:02:51)\n",
            "Steps: 64. Time per step: 4.7e-02s. Reward per step: -1.54.\n",
            "It has been 65 episode(s) since the model was last saved, with a score of -95 (Δ-24.98).\n",
            "\n",
            "11:32:17 \n",
            "Episode: **67**/4000, Score: -108 (Δ -9.2)\n",
            "Average score: -119.4 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 12.3s (±427.39), ETA: 13:24:25 (01:05:34 to 20::00:26:59)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -1.96.\n",
            "It has been 66 episode(s) since the model was last saved, with a score of -95 (Δ-24.81).\n",
            "\n",
            "11:32:19 \n",
            "Episode: **68**/4000, Score: -106 (Δ  1.8)\n",
            "Average score: -119.2 (Δ 0.19)\n",
            "Episode time: 2.0s, average: 12.1s (±422.63), ETA: 13:14:19 (01:05:33 to 19::18:57:55)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.71.\n",
            "It has been 67 episode(s) since the model was last saved, with a score of -95 (Δ-24.62).\n",
            "\n",
            "11:32:22 \n",
            "Episode: **69**/4000, Score: -105 (Δ  0.9)\n",
            "Average score: -119.0 (Δ 0.20)\n",
            "Episode time: 3.0s, average: 12.0s (±417.70), ETA: 13:05:27 (01:05:32 to 19::13:18:25)\n",
            "Steps: 71. Time per step: 4.2e-02s. Reward per step: -1.48.\n",
            "It has been 68 episode(s) since the model was last saved, with a score of -95 (Δ-24.42).\n",
            "\n",
            "11:32:24 \n",
            "Episode: **70**/4000, Score: -100 (Δ  5.6)\n",
            "Average score: -118.7 (Δ 0.27)\n",
            "Episode time: 2.0s, average: 11.8s (±413.13), ETA: 12:55:54 (01:05:31 to 19::08:02:58)\n",
            "Steps: 59. Time per step: 3.4e-02s. Reward per step: -1.69.\n",
            "It has been 69 episode(s) since the model was last saved, with a score of -95 (Δ-24.14).\n",
            "\n",
            "11:32:27 \n",
            "Episode: **71**/4000, Score: -105 (Δ -5.1)\n",
            "Average score: -118.5 (Δ 0.20)\n",
            "Episode time: 3.0s, average: 11.7s (±408.40), ETA: 12:47:33 (01:05:30 to 19::02:37:43)\n",
            "Steps: 66. Time per step: 4.5e-02s. Reward per step: -1.59.\n",
            "It has been 70 episode(s) since the model was last saved, with a score of -95 (Δ-23.95).\n",
            "\n",
            "11:32:29 \n",
            "Episode: **72**/4000, Score: -99 (Δ  5.6)\n",
            "Average score: -118.2 (Δ 0.27)\n",
            "Episode time: 2.0s, average: 11.6s (±404.02), ETA: 12:38:31 (01:05:29 to 18::21:35:09)\n",
            "Steps: 63. Time per step: 3.2e-02s. Reward per step: -1.58.\n",
            "It has been 71 episode(s) since the model was last saved, with a score of -95 (Δ-23.68).\n",
            "\n",
            "11:32:34 \n",
            "Episode: **73**/4000, Score: -106 (Δ -6.6)\n",
            "Average score: -118.1 (Δ 0.17)\n",
            "Episode time: 5.0s, average: 11.5s (±399.07), ETA: 12:32:25 (01:05:28 to 18::15:58:19)\n",
            "Steps: 142. Time per step: 3.5e-02s. Reward per step: -0.75.\n",
            "It has been 72 episode(s) since the model was last saved, with a score of -95 (Δ-23.51).\n",
            "\n",
            "11:32:38 \n",
            "Episode: **74**/4000, Score: -109 (Δ -3.2)\n",
            "Average score: -117.9 (Δ 0.12)\n",
            "Episode time: 4.0s, average: 11.4s (±394.43), ETA: 12:25:36 (01:05:27 to 18::10:40:53)\n",
            "Steps: 101. Time per step: 4.0e-02s. Reward per step: -1.08.\n",
            "It has been 73 episode(s) since the model was last saved, with a score of -95 (Δ-23.39).\n",
            "\n",
            "11:32:41 \n",
            "Episode: **75**/4000, Score: -106 (Δ  2.8)\n",
            "Average score: -117.8 (Δ 0.16)\n",
            "Episode time: 3.0s, average: 11.3s (±390.09), ETA: 12:18:05 (01:05:26 to 18::05:43:18)\n",
            "Steps: 87. Time per step: 3.4e-02s. Reward per step: -1.22.\n",
            "It has been 74 episode(s) since the model was last saved, with a score of -95 (Δ-23.23).\n",
            "\n",
            "11:32:43 \n",
            "Episode: **76**/4000, Score: -100 (Δ  6.4)\n",
            "Average score: -117.5 (Δ 0.24)\n",
            "Episode time: 2.0s, average: 11.2s (±386.08), ETA: 12:09:55 (01:05:25 to 18::01:06:00)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -1.92.\n",
            "It has been 75 episode(s) since the model was last saved, with a score of -95 (Δ-23.00).\n",
            "\n",
            "11:32:45 \n",
            "Episode: **77**/4000, Score: -104 (Δ -4.6)\n",
            "Average score: -117.4 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 11.0s (±382.14), ETA: 12:01:57 (01:05:24 to 17::20:33:59)\n",
            "Steps: 43. Time per step: 4.7e-02s. Reward per step: -2.43.\n",
            "It has been 76 episode(s) since the model was last saved, with a score of -95 (Δ-22.83).\n",
            "\n",
            "11:32:47 \n",
            "Episode: **78**/4000, Score: -108 (Δ -3.3)\n",
            "Average score: -117.3 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 10.9s (±378.28), ETA: 11:54:11 (01:05:23 to 17::16:07:09)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.86.\n",
            "It has been 77 episode(s) since the model was last saved, with a score of -95 (Δ-22.70).\n",
            "\n",
            "11:32:50 \n",
            "Episode: **79**/4000, Score: -100 (Δ  8.1)\n",
            "Average score: -117.0 (Δ 0.22)\n",
            "Episode time: 3.0s, average: 10.8s (±374.27), ETA: 11:47:27 (01:05:22 to 17::11:32:23)\n",
            "Steps: 62. Time per step: 4.8e-02s. Reward per step: -1.61.\n",
            "It has been 78 episode(s) since the model was last saved, with a score of -95 (Δ-22.48).\n",
            "\n",
            "11:32:51 \n",
            "Episode: **80**/4000, Score: -102 (Δ -2.0)\n",
            "Average score: -116.8 (Δ 0.19)\n",
            "Episode time: 1.0s, average: 10.7s (±370.78), ETA: 11:39:15 (01:05:21 to 17::07:30:03)\n",
            "Steps: 50. Time per step: 2.0e-02s. Reward per step: -2.03.\n",
            "It has been 79 episode(s) since the model was last saved, with a score of -95 (Δ-22.29).\n",
            "\n",
            "11:32:53 \n",
            "Episode: **81**/4000, Score: -102 (Δ -0.4)\n",
            "Average score: -116.7 (Δ 0.18)\n",
            "Episode time: 2.0s, average: 10.6s (±367.13), ETA: 11:32:03 (01:05:20 to 17::03:17:54)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.93.\n",
            "It has been 80 episode(s) since the model was last saved, with a score of -95 (Δ-22.11).\n",
            "\n",
            "11:32:55 \n",
            "Episode: **82**/4000, Score: -104 (Δ -1.6)\n",
            "Average score: -116.5 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 10.5s (±363.54), ETA: 11:25:02 (01:05:19 to 16::23:10:25)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.16.\n",
            "It has been 81 episode(s) since the model was last saved, with a score of -95 (Δ-21.95).\n",
            "\n",
            "11:32:57 \n",
            "Episode: **83**/4000, Score: -99 (Δ  4.7)\n",
            "Average score: -116.3 (Δ 0.21)\n",
            "Episode time: 2.0s, average: 10.4s (±360.02), ETA: 11:18:11 (01:05:18 to 16::19:07:29)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.71.\n",
            "It has been 82 episode(s) since the model was last saved, with a score of -95 (Δ-21.74).\n",
            "\n",
            "11:32:59 \n",
            "Episode: **84**/4000, Score: -101 (Δ -1.6)\n",
            "Average score: -116.1 (Δ 0.19)\n",
            "Episode time: 2.0s, average: 10.3s (±356.56), ETA: 11:11:29 (01:05:17 to 16::15:08:59)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -1.83.\n",
            "It has been 83 episode(s) since the model was last saved, with a score of -95 (Δ-21.55).\n",
            "\n",
            "11:33:01 \n",
            "Episode: **85**/4000, Score: -104 (Δ -3.1)\n",
            "Average score: -116.0 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 10.2s (±353.16), ETA: 11:04:57 (01:05:16 to 16::11:14:50)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.16.\n",
            "It has been 84 episode(s) since the model was last saved, with a score of -95 (Δ-21.40).\n",
            "\n",
            "11:33:03 \n",
            "Episode: **86**/4000, Score: -100 (Δ  3.2)\n",
            "Average score: -115.8 (Δ 0.18)\n",
            "Episode time: 2.0s, average: 10.1s (±349.83), ETA: 10:58:34 (01:05:15 to 16::07:24:53)\n",
            "Steps: 57. Time per step: 3.5e-02s. Reward per step: -1.76.\n",
            "It has been 85 episode(s) since the model was last saved, with a score of -95 (Δ-21.22).\n",
            "\n",
            "11:33:05 \n",
            "Episode: **87**/4000, Score: -103 (Δ -2.5)\n",
            "Average score: -115.6 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 10.0s (±346.55), ETA: 10:52:20 (01:05:14 to 16::03:39:03)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.24.\n",
            "It has been 86 episode(s) since the model was last saved, with a score of -95 (Δ-21.07).\n",
            "\n",
            "11:33:07 \n",
            "Episode: **88**/4000, Score: -102 (Δ  0.8)\n",
            "Average score: -115.5 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 9.9s (±343.33), ETA: 10:46:14 (01:05:13 to 15::23:57:15)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.89.\n",
            "It has been 87 episode(s) since the model was last saved, with a score of -95 (Δ-20.92).\n",
            "\n",
            "11:33:09 \n",
            "Episode: **89**/4000, Score: -102 (Δ -0.3)\n",
            "Average score: -115.3 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 9.8s (±340.17), ETA: 10:40:17 (01:05:12 to 15::20:19:21)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -1.97.\n",
            "It has been 88 episode(s) since the model was last saved, with a score of -95 (Δ-20.77).\n",
            "\n",
            "11:33:11 \n",
            "Episode: **90**/4000, Score: -101 (Δ  1.7)\n",
            "Average score: -115.2 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 9.7s (±337.06), ETA: 10:34:27 (01:05:11 to 15::16:45:17)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.19.\n",
            "It has been 89 episode(s) since the model was last saved, with a score of -95 (Δ-20.61).\n",
            "\n",
            "11:33:13 \n",
            "Episode: **91**/4000, Score: -101 (Δ -0.2)\n",
            "Average score: -115.0 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 9.6s (±334.01), ETA: 10:28:45 (01:05:10 to 15::13:14:57)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -1.80.\n",
            "It has been 90 episode(s) since the model was last saved, with a score of -95 (Δ-20.45).\n",
            "\n",
            "11:33:15 \n",
            "Episode: **92**/4000, Score: -100 (Δ  0.4)\n",
            "Average score: -114.8 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 9.6s (±331.01), ETA: 10:23:10 (01:05:09 to 15::09:48:15)\n",
            "Steps: 59. Time per step: 3.4e-02s. Reward per step: -1.70.\n",
            "It has been 91 episode(s) since the model was last saved, with a score of -95 (Δ-20.29).\n",
            "\n",
            "11:33:18 \n",
            "Episode: **93**/4000, Score: -101 (Δ -0.7)\n",
            "Average score: -114.7 (Δ 0.15)\n",
            "Episode time: 3.0s, average: 9.5s (±327.91), ETA: 10:18:25 (01:05:08 to 15::06:16:01)\n",
            "Steps: 57. Time per step: 5.3e-02s. Reward per step: -1.77.\n",
            "It has been 92 episode(s) since the model was last saved, with a score of -95 (Δ-20.15).\n",
            "\n",
            "11:33:20 \n",
            "Episode: **94**/4000, Score: -100 (Δ  1.4)\n",
            "Average score: -114.5 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 9.4s (±325.01), ETA: 10:13:04 (01:05:07 to 15::02:56:33)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.04.\n",
            "It has been 93 episode(s) since the model was last saved, with a score of -95 (Δ-19.99).\n",
            "\n",
            "11:33:22 \n",
            "Episode: **95**/4000, Score: -99 (Δ  1.0)\n",
            "Average score: -114.4 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 9.3s (±322.16), ETA: 10:07:50 (01:05:06 to 14::23:40:28)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.86.\n",
            "It has been 94 episode(s) since the model was last saved, with a score of -95 (Δ-19.82).\n",
            "\n",
            "11:33:24 \n",
            "Episode: **96**/4000, Score: -98 (Δ  0.5)\n",
            "Average score: -114.2 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 9.3s (±319.36), ETA: 10:02:42 (01:05:05 to 14::20:27:40)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.82.\n",
            "It has been 95 episode(s) since the model was last saved, with a score of -95 (Δ-19.65).\n",
            "\n",
            "11:33:26 \n",
            "Episode: **97**/4000, Score: -100 (Δ -1.6)\n",
            "Average score: -114.1 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 9.2s (±316.60), ETA: 09:57:40 (01:05:04 to 14::17:18:05)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.08.\n",
            "It has been 96 episode(s) since the model was last saved, with a score of -95 (Δ-19.50).\n",
            "\n",
            "11:33:28 \n",
            "Episode: **98**/4000, Score: -99 (Δ  0.8)\n",
            "Average score: -113.9 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 9.1s (±313.90), ETA: 09:52:45 (01:05:03 to 14::14:11:39)\n",
            "Steps: 62. Time per step: 3.2e-02s. Reward per step: -1.60.\n",
            "It has been 97 episode(s) since the model was last saved, with a score of -95 (Δ-19.35).\n",
            "\n",
            "11:33:30 \n",
            "Episode: **99**/4000, Score: -100 (Δ -1.0)\n",
            "Average score: -113.8 (Δ 0.14)\n",
            "Episode time: 2.0s, average: 9.0s (±311.23), ETA: 09:47:56 (01:05:02 to 14::11:08:18)\n",
            "Steps: 60. Time per step: 3.3e-02s. Reward per step: -1.67.\n",
            "It has been 98 episode(s) since the model was last saved, with a score of -95 (Δ-19.21).\n",
            "\n",
            "11:33:32 \n",
            "Episode: **100**/4000, Score: -100 (Δ -0.3)\n",
            "Average score: -113.6 (Δ 0.14)\n",
            "Episode time: 2.0s, average: 9.0s (±308.61), ETA: 09:43:12 (01:05:01 to 14::08:07:56)\n",
            "Steps: 59. Time per step: 3.4e-02s. Reward per step: -1.70.\n",
            "It has been 99 episode(s) since the model was last saved, with a score of -95 (Δ-19.07).\n",
            "\n",
            "11:33:34 \n",
            "Episode: **101**/4000, Score: -102 (Δ -1.4)\n",
            "Average score: -113.7 (Δ-0.07)\n",
            "Episode time: 2.0s, average: 8.9s (±306.03), ETA: 09:38:34 (01:05:00 to 14::05:10:30)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.07.\n",
            "It has been 100 episode(s) since the model was last saved, with a score of -95 (Δ-19.14).\n",
            "\n",
            "11:33:36 \n",
            "Episode: **102**/4000, Score: -102 (Δ -0.7)\n",
            "Average score: -113.6 (Δ 0.09)\n",
            "Episode time: 2.0s, average: 8.8s (±303.49), ETA: 09:34:01 (01:04:59 to 14::02:15:56)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -2.01.\n",
            "It has been 101 episode(s) since the model was last saved, with a score of -95 (Δ-19.06).\n",
            "\n",
            "11:33:38 \n",
            "Episode: **103**/4000, Score: -101 (Δ  1.2)\n",
            "Average score: -113.3 (Δ 0.33)\n",
            "Episode time: 2.0s, average: 8.8s (±300.99), ETA: 09:29:34 (01:04:58 to 13::23:24:09)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.91.\n",
            "It has been 102 episode(s) since the model was last saved, with a score of -95 (Δ-18.73).\n",
            "\n",
            "11:33:41 \n",
            "Episode: **104**/4000, Score: -99 (Δ  2.2)\n",
            "Average score: -112.8 (Δ 0.51)\n",
            "Episode time: 3.0s, average: 8.7s (±298.42), ETA: 09:25:49 (01:04:57 to 13::20:27:59)\n",
            "Steps: 62. Time per step: 4.8e-02s. Reward per step: -1.59.\n",
            "It has been 103 episode(s) since the model was last saved, with a score of -95 (Δ-18.21).\n",
            "\n",
            "11:33:43 \n",
            "Episode: **105**/4000, Score: -98 (Δ  1.0)\n",
            "Average score: -112.7 (Δ 0.03)\n",
            "Episode time: 2.0s, average: 8.6s (±296.00), ETA: 09:21:31 (01:04:56 to 13::17:41:46)\n",
            "Steps: 70. Time per step: 2.9e-02s. Reward per step: -1.40.\n",
            "It has been 104 episode(s) since the model was last saved, with a score of -95 (Δ-18.19).\n",
            "\n",
            "11:33:45 \n",
            "Episode: **106**/4000, Score: -101 (Δ -3.7)\n",
            "Average score: -112.7 (Δ 0.02)\n",
            "Episode time: 2.0s, average: 8.6s (±293.62), ETA: 09:17:18 (01:04:55 to 13::14:58:09)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.88.\n",
            "It has been 105 episode(s) since the model was last saved, with a score of -95 (Δ-18.17).\n",
            "\n",
            "11:33:47 \n",
            "Episode: **107**/4000, Score: -101 (Δ  0.5)\n",
            "Average score: -112.7 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 8.5s (±291.28), ETA: 09:13:10 (01:04:54 to 13::12:17:05)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -1.80.\n",
            "It has been 106 episode(s) since the model was last saved, with a score of -95 (Δ-18.13).\n",
            "\n",
            "11:33:50 \n",
            "Episode: **108**/4000, Score: -102 (Δ -1.0)\n",
            "Average score: -112.7 (Δ-0.00)\n",
            "Episode time: 3.0s, average: 8.5s (±288.86), ETA: 09:09:42 (01:04:53 to 13::09:31:56)\n",
            "Steps: 56. Time per step: 5.4e-02s. Reward per step: -1.82.\n",
            "It has been 107 episode(s) since the model was last saved, with a score of -95 (Δ-18.13).\n",
            "\n",
            "11:33:52 \n",
            "Episode: **109**/4000, Score: -101 (Δ  1.5)\n",
            "Average score: -112.7 (Δ 0.02)\n",
            "Episode time: 2.0s, average: 8.4s (±286.59), ETA: 09:05:43 (01:04:52 to 13::06:55:55)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.86.\n",
            "It has been 108 episode(s) since the model was last saved, with a score of -95 (Δ-18.11).\n",
            "\n",
            "11:33:54 \n",
            "Episode: **110**/4000, Score: -101 (Δ -0.5)\n",
            "Average score: -112.4 (Δ 0.25)\n",
            "Episode time: 2.0s, average: 8.4s (±284.36), ETA: 09:01:48 (01:04:51 to 13::04:22:17)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -1.94.\n",
            "It has been 109 episode(s) since the model was last saved, with a score of -95 (Δ-17.86).\n",
            "\n",
            "11:33:56 \n",
            "Episode: **111**/4000, Score: -101 (Δ  0.3)\n",
            "Average score: -112.3 (Δ 0.10)\n",
            "Episode time: 2.0s, average: 8.3s (±282.15), ETA: 08:57:56 (01:04:50 to 13::01:50:59)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -1.83.\n",
            "It has been 110 episode(s) since the model was last saved, with a score of -95 (Δ-17.76).\n",
            "\n",
            "11:33:58 \n",
            "Episode: **112**/4000, Score: -100 (Δ  0.3)\n",
            "Average score: -112.2 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 8.2s (±279.99), ETA: 08:54:10 (01:04:49 to 12::23:21:57)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.89.\n",
            "It has been 111 episode(s) since the model was last saved, with a score of -95 (Δ-17.64).\n",
            "\n",
            "11:34:00 \n",
            "Episode: **113**/4000, Score: -101 (Δ -0.9)\n",
            "Average score: -112.1 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 8.2s (±277.85), ETA: 08:50:27 (01:04:48 to 12::20:55:09)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.11.\n",
            "It has been 112 episode(s) since the model was last saved, with a score of -95 (Δ-17.52).\n",
            "\n",
            "11:34:02 \n",
            "Episode: **114**/4000, Score: -102 (Δ -0.3)\n",
            "Average score: -111.9 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 8.1s (±275.75), ETA: 08:46:47 (01:04:47 to 12::18:30:32)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.88.\n",
            "It has been 113 episode(s) since the model was last saved, with a score of -95 (Δ-17.39).\n",
            "\n",
            "11:34:04 \n",
            "Episode: **115**/4000, Score: -101 (Δ  0.7)\n",
            "Average score: -111.4 (Δ 0.58)\n",
            "Episode time: 2.0s, average: 8.1s (±273.67), ETA: 08:43:12 (01:04:46 to 12::16:08:02)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -1.94.\n",
            "It has been 114 episode(s) since the model was last saved, with a score of -95 (Δ-16.81).\n",
            "\n",
            "11:34:06 \n",
            "Episode: **116**/4000, Score: -102 (Δ -1.0)\n",
            "Average score: -110.8 (Δ 0.53)\n",
            "Episode time: 2.0s, average: 8.0s (±271.63), ETA: 08:39:40 (01:04:45 to 12::13:47:38)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.89.\n",
            "It has been 115 episode(s) since the model was last saved, with a score of -95 (Δ-16.29).\n",
            "\n",
            "11:34:08 \n",
            "Episode: **117**/4000, Score: -101 (Δ  0.9)\n",
            "Average score: -110.4 (Δ 0.41)\n",
            "Episode time: 2.0s, average: 8.0s (±269.61), ETA: 08:36:12 (01:04:44 to 12::11:29:16)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -1.80.\n",
            "It has been 116 episode(s) since the model was last saved, with a score of -95 (Δ-15.87).\n",
            "\n",
            "11:34:10 \n",
            "Episode: **118**/4000, Score: -101 (Δ  0.3)\n",
            "Average score: -110.0 (Δ 0.41)\n",
            "Episode time: 2.0s, average: 7.9s (±267.63), ETA: 08:32:48 (01:04:43 to 12::09:12:54)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.90.\n",
            "It has been 117 episode(s) since the model was last saved, with a score of -95 (Δ-15.46).\n",
            "\n",
            "11:34:12 \n",
            "Episode: **119**/4000, Score: -103 (Δ -2.2)\n",
            "Average score: -109.5 (Δ 0.46)\n",
            "Episode time: 2.0s, average: 7.9s (±265.67), ETA: 08:29:27 (01:04:42 to 12::06:58:30)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.14.\n",
            "It has been 118 episode(s) since the model was last saved, with a score of -95 (Δ-15.00).\n",
            "\n",
            "11:34:14 \n",
            "Episode: **120**/4000, Score: -101 (Δ  1.7)\n",
            "Average score: -109.0 (Δ 0.55)\n",
            "Episode time: 2.0s, average: 7.8s (±263.74), ETA: 08:26:09 (01:04:41 to 12::04:46:01)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.91.\n",
            "It has been 119 episode(s) since the model was last saved, with a score of -95 (Δ-14.45).\n",
            "\n",
            "11:34:16 \n",
            "Episode: **121**/4000, Score: -101 (Δ  0.2)\n",
            "Average score: -108.4 (Δ 0.59)\n",
            "Episode time: 2.0s, average: 7.8s (±261.84), ETA: 08:22:54 (01:04:40 to 12::02:35:24)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.87.\n",
            "It has been 120 episode(s) since the model was last saved, with a score of -95 (Δ-13.86).\n",
            "\n",
            "11:34:18 \n",
            "Episode: **122**/4000, Score: -101 (Δ -0.2)\n",
            "Average score: -107.9 (Δ 0.46)\n",
            "Episode time: 2.0s, average: 7.7s (±259.97), ETA: 08:19:43 (01:04:39 to 12::00:26:38)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -1.99.\n",
            "It has been 121 episode(s) since the model was last saved, with a score of -95 (Δ-13.39).\n",
            "\n",
            "11:34:20 \n",
            "Episode: **123**/4000, Score: -100 (Δ  1.0)\n",
            "Average score: -107.8 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 7.7s (±258.12), ETA: 08:16:34 (01:04:38 to 11::22:19:40)\n",
            "Steps: 55. Time per step: 3.6e-02s. Reward per step: -1.82.\n",
            "It has been 122 episode(s) since the model was last saved, with a score of -95 (Δ-13.22).\n",
            "\n",
            "11:34:22 \n",
            "Episode: **124**/4000, Score: -101 (Δ -0.9)\n",
            "Average score: -107.3 (Δ 0.49)\n",
            "Episode time: 2.0s, average: 7.6s (±256.30), ETA: 08:13:29 (01:04:37 to 11::20:14:28)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -1.95.\n",
            "It has been 123 episode(s) since the model was last saved, with a score of -95 (Δ-12.74).\n",
            "\n",
            "11:34:23 \n",
            "Episode: **125**/4000, Score: -103 (Δ -1.5)\n",
            "Average score: -107.2 (Δ 0.09)\n",
            "Episode time: 1.0s, average: 7.6s (±254.59), ETA: 08:09:56 (01:04:36 to 11::18:16:46)\n",
            "Steps: 47. Time per step: 2.1e-02s. Reward per step: -2.19.\n",
            "It has been 124 episode(s) since the model was last saved, with a score of -95 (Δ-12.64).\n",
            "\n",
            "11:34:25 \n",
            "Episode: **126**/4000, Score: -101 (Δ  1.8)\n",
            "Average score: -107.1 (Δ 0.11)\n",
            "Episode time: 2.0s, average: 7.5s (±252.82), ETA: 08:06:56 (01:04:35 to 11::16:14:53)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.90.\n",
            "It has been 125 episode(s) since the model was last saved, with a score of -95 (Δ-12.53).\n",
            "\n",
            "11:34:27 \n",
            "Episode: **127**/4000, Score: -102 (Δ -0.9)\n",
            "Average score: -107.0 (Δ 0.10)\n",
            "Episode time: 2.0s, average: 7.5s (±251.07), ETA: 08:04:00 (01:04:34 to 11::14:14:41)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -2.00.\n",
            "It has been 126 episode(s) since the model was last saved, with a score of -95 (Δ-12.43).\n",
            "\n",
            "11:34:29 \n",
            "Episode: **128**/4000, Score: -103 (Δ -0.7)\n",
            "Average score: -106.9 (Δ 0.09)\n",
            "Episode time: 2.0s, average: 7.5s (±249.34), ETA: 08:01:06 (01:04:33 to 11::12:16:06)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.05.\n",
            "It has been 127 episode(s) since the model was last saved, with a score of -95 (Δ-12.33).\n",
            "\n",
            "11:34:31 \n",
            "Episode: **129**/4000, Score: -102 (Δ  0.7)\n",
            "Average score: -106.8 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 7.4s (±247.64), ETA: 07:58:15 (01:04:32 to 11::10:19:07)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.04.\n",
            "It has been 128 episode(s) since the model was last saved, with a score of -95 (Δ-12.21).\n",
            "\n",
            "11:34:33 \n",
            "Episode: **130**/4000, Score: -102 (Δ  0.3)\n",
            "Average score: -106.7 (Δ 0.11)\n",
            "Episode time: 2.0s, average: 7.4s (±245.96), ETA: 07:55:26 (01:04:31 to 11::08:23:42)\n",
            "Steps: 54. Time per step: 3.7e-02s. Reward per step: -1.88.\n",
            "It has been 129 episode(s) since the model was last saved, with a score of -95 (Δ-12.11).\n",
            "\n",
            "11:34:35 \n",
            "Episode: **131**/4000, Score: -101 (Δ  0.7)\n",
            "Average score: -106.5 (Δ 0.13)\n",
            "Episode time: 2.0s, average: 7.3s (±244.30), ETA: 07:52:40 (01:04:30 to 11::06:29:49)\n",
            "Steps: 58. Time per step: 3.4e-02s. Reward per step: -1.74.\n",
            "It has been 130 episode(s) since the model was last saved, with a score of -95 (Δ-11.98).\n",
            "\n",
            "11:34:37 \n",
            "Episode: **132**/4000, Score: -101 (Δ  0.1)\n",
            "Average score: -106.4 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 7.3s (±242.66), ETA: 07:49:57 (01:04:29 to 11::04:37:27)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.90.\n",
            "It has been 131 episode(s) since the model was last saved, with a score of -95 (Δ-11.86).\n",
            "\n",
            "11:34:39 \n",
            "Episode: **133**/4000, Score: -101 (Δ -0.5)\n",
            "Average score: -106.3 (Δ 0.13)\n",
            "Episode time: 2.0s, average: 7.2s (±241.04), ETA: 07:47:16 (01:04:28 to 11::02:46:33)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.91.\n",
            "It has been 132 episode(s) since the model was last saved, with a score of -95 (Δ-11.74).\n",
            "\n",
            "11:34:42 \n",
            "Episode: **134**/4000, Score: -100 (Δ  0.6)\n",
            "Average score: -106.2 (Δ 0.11)\n",
            "Episode time: 3.0s, average: 7.2s (±239.38), ETA: 07:45:06 (01:04:27 to 11::00:53:03)\n",
            "Steps: 56. Time per step: 5.4e-02s. Reward per step: -1.79.\n",
            "It has been 133 episode(s) since the model was last saved, with a score of -95 (Δ-11.63).\n",
            "\n",
            "11:34:43 \n",
            "Episode: **135**/4000, Score: -103 (Δ -2.1)\n",
            "Average score: -105.6 (Δ 0.56)\n",
            "Episode time: 1.0s, average: 7.2s (±237.89), ETA: 07:42:01 (01:04:26 to 10::23:10:01)\n",
            "Steps: 48. Time per step: 2.1e-02s. Reward per step: -2.14.\n",
            "It has been 134 episode(s) since the model was last saved, with a score of -95 (Δ-11.07).\n",
            "\n",
            "11:34:45 \n",
            "Episode: **136**/4000, Score: -103 (Δ -0.6)\n",
            "Average score: -105.5 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 7.1s (±236.34), ETA: 07:39:27 (01:04:25 to 10::21:23:23)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.06.\n",
            "It has been 135 episode(s) since the model was last saved, with a score of -95 (Δ-10.92).\n",
            "\n",
            "11:34:47 \n",
            "Episode: **137**/4000, Score: -102 (Δ  0.9)\n",
            "Average score: -105.3 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 7.1s (±234.80), ETA: 07:36:55 (01:04:24 to 10::19:38:07)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.05.\n",
            "It has been 136 episode(s) since the model was last saved, with a score of -95 (Δ-10.77).\n",
            "\n",
            "11:34:49 \n",
            "Episode: **138**/4000, Score: -102 (Δ  0.6)\n",
            "Average score: -105.2 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 7.1s (±233.29), ETA: 07:34:25 (01:04:23 to 10::17:54:11)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.03.\n",
            "It has been 137 episode(s) since the model was last saved, with a score of -95 (Δ-10.61).\n",
            "\n",
            "11:34:51 \n",
            "Episode: **139**/4000, Score: -101 (Δ  0.5)\n",
            "Average score: -105.0 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 7.0s (±231.79), ETA: 07:31:57 (01:04:22 to 10::16:11:34)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.02.\n",
            "It has been 138 episode(s) since the model was last saved, with a score of -95 (Δ-10.44).\n",
            "\n",
            "11:34:53 \n",
            "Episode: **140**/4000, Score: -102 (Δ -0.8)\n",
            "Average score: -104.8 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 7.0s (±230.31), ETA: 07:29:32 (01:04:21 to 10::14:30:15)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.04.\n",
            "It has been 139 episode(s) since the model was last saved, with a score of -95 (Δ-10.28).\n",
            "\n",
            "11:34:55 \n",
            "Episode: **141**/4000, Score: -103 (Δ -0.9)\n",
            "Average score: -104.7 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 7.0s (±228.86), ETA: 07:27:08 (01:04:20 to 10::12:50:11)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.14.\n",
            "It has been 140 episode(s) since the model was last saved, with a score of -95 (Δ-10.13).\n",
            "\n",
            "11:34:57 \n",
            "Episode: **142**/4000, Score: -101 (Δ  2.0)\n",
            "Average score: -104.5 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 6.9s (±227.42), ETA: 07:24:47 (01:04:19 to 10::11:11:23)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -1.80.\n",
            "It has been 141 episode(s) since the model was last saved, with a score of -95 (Δ-9.96).\n",
            "\n",
            "11:34:59 \n",
            "Episode: **143**/4000, Score: -102 (Δ -1.1)\n",
            "Average score: -104.4 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 6.9s (±225.99), ETA: 07:22:27 (01:04:18 to 10::09:33:48)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.08.\n",
            "It has been 142 episode(s) since the model was last saved, with a score of -95 (Δ-9.81).\n",
            "\n",
            "11:35:01 \n",
            "Episode: **144**/4000, Score: -101 (Δ  1.1)\n",
            "Average score: -104.2 (Δ 0.18)\n",
            "Episode time: 2.0s, average: 6.8s (±224.59), ETA: 07:20:10 (01:04:17 to 10::07:57:25)\n",
            "Steps: 56. Time per step: 3.6e-02s. Reward per step: -1.80.\n",
            "It has been 143 episode(s) since the model was last saved, with a score of -95 (Δ-9.63).\n",
            "\n",
            "11:35:03 \n",
            "Episode: **145**/4000, Score: -102 (Δ -0.8)\n",
            "Average score: -104.0 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 6.8s (±223.20), ETA: 07:17:54 (01:04:16 to 10::06:22:12)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -1.99.\n",
            "It has been 144 episode(s) since the model was last saved, with a score of -95 (Δ-9.48).\n",
            "\n",
            "11:35:05 \n",
            "Episode: **146**/4000, Score: -102 (Δ -0.5)\n",
            "Average score: -103.9 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 6.8s (±221.83), ETA: 07:15:40 (01:04:15 to 10::04:48:10)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.22.\n",
            "It has been 145 episode(s) since the model was last saved, with a score of -95 (Δ-9.32).\n",
            "\n",
            "11:35:06 \n",
            "Episode: **147**/4000, Score: -102 (Δ  0.4)\n",
            "Average score: -103.7 (Δ 0.17)\n",
            "Episode time: 1.0s, average: 6.7s (±220.55), ETA: 07:13:02 (01:04:14 to 10::03:19:24)\n",
            "Steps: 51. Time per step: 2.0e-02s. Reward per step: -1.99.\n",
            "It has been 146 episode(s) since the model was last saved, with a score of -95 (Δ-9.15).\n",
            "\n",
            "11:35:08 \n",
            "Episode: **148**/4000, Score: -102 (Δ -0.2)\n",
            "Average score: -103.5 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 6.7s (±219.21), ETA: 07:10:52 (01:04:13 to 10::01:47:33)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.08.\n",
            "It has been 147 episode(s) since the model was last saved, with a score of -95 (Δ-8.98).\n",
            "\n",
            "11:35:10 \n",
            "Episode: **149**/4000, Score: -102 (Δ -0.1)\n",
            "Average score: -103.4 (Δ 0.16)\n",
            "Episode time: 2.0s, average: 6.7s (±217.88), ETA: 07:08:43 (01:04:12 to 10::00:16:48)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.17.\n",
            "It has been 148 episode(s) since the model was last saved, with a score of -95 (Δ-8.82).\n",
            "\n",
            "11:35:12 \n",
            "Episode: **150**/4000, Score: -101 (Δ  0.9)\n",
            "Average score: -103.2 (Δ 0.17)\n",
            "Episode time: 2.0s, average: 6.6s (±216.58), ETA: 07:06:36 (01:04:11 to 09::22:47:07)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.02.\n",
            "It has been 149 episode(s) since the model was last saved, with a score of -95 (Δ-8.65).\n",
            "\n",
            "11:35:14 \n",
            "Episode: **151**/4000, Score: -103 (Δ -1.5)\n",
            "Average score: -103.1 (Δ 0.15)\n",
            "Episode time: 2.0s, average: 6.6s (±215.28), ETA: 07:04:31 (01:04:10 to 09::21:18:30)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.18.\n",
            "It has been 150 episode(s) since the model was last saved, with a score of -95 (Δ-8.50).\n",
            "\n",
            "11:35:16 \n",
            "Episode: **152**/4000, Score: -103 (Δ  0.1)\n",
            "Average score: -102.9 (Δ 0.13)\n",
            "Episode time: 2.0s, average: 6.6s (±214.01), ETA: 07:02:28 (01:04:09 to 09::19:50:56)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.14.\n",
            "It has been 151 episode(s) since the model was last saved, with a score of -95 (Δ-8.37).\n",
            "\n",
            "11:35:18 \n",
            "Episode: **153**/4000, Score: -102 (Δ  0.9)\n",
            "Average score: -102.8 (Δ 0.12)\n",
            "Episode time: 2.0s, average: 6.6s (±212.74), ETA: 07:00:26 (01:04:08 to 09::18:24:23)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.92.\n",
            "It has been 152 episode(s) since the model was last saved, with a score of -95 (Δ-8.25).\n",
            "\n",
            "11:35:20 \n",
            "Episode: **154**/4000, Score: -102 (Δ  0.1)\n",
            "Average score: -102.7 (Δ 0.13)\n",
            "Episode time: 2.0s, average: 6.5s (±211.50), ETA: 06:58:25 (01:04:07 to 09::16:58:51)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.03.\n",
            "It has been 153 episode(s) since the model was last saved, with a score of -95 (Δ-8.12).\n",
            "\n",
            "11:35:22 \n",
            "Episode: **155**/4000, Score: -102 (Δ -0.6)\n",
            "Average score: -102.5 (Δ 0.18)\n",
            "Episode time: 2.0s, average: 6.5s (±210.26), ETA: 06:56:27 (01:04:06 to 09::15:34:18)\n",
            "Steps: 52. Time per step: 3.8e-02s. Reward per step: -1.96.\n",
            "It has been 154 episode(s) since the model was last saved, with a score of -95 (Δ-7.95).\n",
            "\n",
            "11:35:24 \n",
            "Episode: **156**/4000, Score: -102 (Δ  0.5)\n",
            "Average score: -101.6 (Δ 0.91)\n",
            "Episode time: 2.0s, average: 6.5s (±209.04), ETA: 06:54:29 (01:04:05 to 09::14:10:43)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.03.\n",
            "It has been 155 episode(s) since the model was last saved, with a score of -95 (Δ-7.04).\n",
            "\n",
            "11:35:26 \n",
            "Episode: **157**/4000, Score: -101 (Δ  0.3)\n",
            "Average score: -101.6 (Δ 0.01)\n",
            "Episode time: 2.0s, average: 6.4s (±207.84), ETA: 06:52:33 (01:04:04 to 09::12:48:05)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.03.\n",
            "It has been 156 episode(s) since the model was last saved, with a score of -95 (Δ-7.03).\n",
            "\n",
            "11:35:27 \n",
            "Episode: **158**/4000, Score: -102 (Δ -0.3)\n",
            "Average score: -101.6 (Δ-0.00)\n",
            "Episode time: 1.0s, average: 6.4s (±206.71), ETA: 06:50:15 (01:04:03 to 09::11:29:59)\n",
            "Steps: 50. Time per step: 2.0e-02s. Reward per step: -2.03.\n",
            "It has been 157 episode(s) since the model was last saved, with a score of -95 (Δ-7.03).\n",
            "\n",
            "11:35:29 \n",
            "Episode: **159**/4000, Score: -101 (Δ  0.1)\n",
            "Average score: -101.6 (Δ-0.01)\n",
            "Episode time: 2.0s, average: 6.4s (±205.53), ETA: 06:48:22 (01:04:02 to 09::10:09:10)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -1.99.\n",
            "It has been 158 episode(s) since the model was last saved, with a score of -95 (Δ-7.04).\n",
            "\n",
            "11:35:31 \n",
            "Episode: **160**/4000, Score: -103 (Δ -1.6)\n",
            "Average score: -101.6 (Δ-0.03)\n",
            "Episode time: 2.0s, average: 6.3s (±204.36), ETA: 06:46:30 (01:04:01 to 09::08:49:16)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.19.\n",
            "It has been 159 episode(s) since the model was last saved, with a score of -95 (Δ-7.07).\n",
            "\n",
            "11:35:33 \n",
            "Episode: **161**/4000, Score: -102 (Δ  1.4)\n",
            "Average score: -101.6 (Δ-0.03)\n",
            "Episode time: 2.0s, average: 6.3s (±203.21), ETA: 06:44:40 (01:04:00 to 09::07:30:16)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.03.\n",
            "It has been 160 episode(s) since the model was last saved, with a score of -95 (Δ-7.10).\n",
            "\n",
            "11:35:35 \n",
            "Episode: **162**/4000, Score: -102 (Δ -0.4)\n",
            "Average score: -101.7 (Δ-0.02)\n",
            "Episode time: 2.0s, average: 6.3s (±202.07), ETA: 06:42:51 (01:03:59 to 09::06:12:09)\n",
            "Steps: 50. Time per step: 4.0e-02s. Reward per step: -2.04.\n",
            "It has been 161 episode(s) since the model was last saved, with a score of -95 (Δ-7.11).\n",
            "\n",
            "11:35:37 \n",
            "Episode: **163**/4000, Score: -102 (Δ  0.2)\n",
            "Average score: -101.7 (Δ-0.00)\n",
            "Episode time: 2.0s, average: 6.3s (±200.95), ETA: 06:41:04 (01:03:58 to 09::04:54:53)\n",
            "Steps: 46. Time per step: 4.3e-02s. Reward per step: -2.22.\n",
            "It has been 162 episode(s) since the model was last saved, with a score of -95 (Δ-7.12).\n",
            "\n",
            "11:35:39 \n",
            "Episode: **164**/4000, Score: -102 (Δ  0.2)\n",
            "Average score: -101.7 (Δ-0.01)\n",
            "Episode time: 2.0s, average: 6.2s (±199.83), ETA: 06:39:18 (01:03:57 to 09::03:38:28)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.08.\n",
            "It has been 163 episode(s) since the model was last saved, with a score of -95 (Δ-7.13).\n",
            "\n",
            "11:35:40 \n",
            "Episode: **165**/4000, Score: -102 (Δ -0.6)\n",
            "Average score: -101.7 (Δ-0.01)\n",
            "Episode time: 1.0s, average: 6.2s (±198.79), ETA: 06:37:10 (01:03:56 to 09::02:26:10)\n",
            "Steps: 48. Time per step: 2.1e-02s. Reward per step: -2.13.\n",
            "It has been 164 episode(s) since the model was last saved, with a score of -95 (Δ-7.14).\n",
            "\n",
            "11:35:42 \n",
            "Episode: **166**/4000, Score: -102 (Δ  0.6)\n",
            "Average score: -101.7 (Δ-0.03)\n",
            "Episode time: 2.0s, average: 6.2s (±197.69), ETA: 06:35:26 (01:03:55 to 09::01:11:23)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.08.\n",
            "It has been 165 episode(s) since the model was last saved, with a score of -95 (Δ-7.17).\n",
            "\n",
            "11:35:44 \n",
            "Episode: **167**/4000, Score: -102 (Δ  0.2)\n",
            "Average score: -101.7 (Δ 0.06)\n",
            "Episode time: 2.0s, average: 6.2s (±196.61), ETA: 06:33:44 (01:03:54 to 08::23:57:24)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -1.99.\n",
            "It has been 166 episode(s) since the model was last saved, with a score of -95 (Δ-7.11).\n",
            "\n",
            "11:35:46 \n",
            "Episode: **168**/4000, Score: -103 (Δ -1.1)\n",
            "Average score: -101.6 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 6.1s (±195.55), ETA: 06:32:03 (01:03:53 to 08::22:44:13)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.14.\n",
            "It has been 167 episode(s) since the model was last saved, with a score of -95 (Δ-7.07).\n",
            "\n",
            "11:35:48 \n",
            "Episode: **169**/4000, Score: -101 (Δ  1.6)\n",
            "Average score: -101.6 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 6.1s (±194.49), ETA: 06:30:23 (01:03:52 to 08::21:31:50)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.06.\n",
            "It has been 168 episode(s) since the model was last saved, with a score of -95 (Δ-7.03).\n",
            "\n",
            "11:35:50 \n",
            "Episode: **170**/4000, Score: -101 (Δ -0.1)\n",
            "Average score: -101.6 (Δ-0.01)\n",
            "Episode time: 2.0s, average: 6.1s (±193.45), ETA: 06:28:44 (01:03:51 to 08::20:20:12)\n",
            "Steps: 51. Time per step: 3.9e-02s. Reward per step: -1.98.\n",
            "It has been 169 episode(s) since the model was last saved, with a score of -95 (Δ-7.04).\n",
            "\n",
            "11:35:52 \n",
            "Episode: **171**/4000, Score: -101 (Δ -0.2)\n",
            "Average score: -101.6 (Δ 0.03)\n",
            "Episode time: 2.0s, average: 6.1s (±192.41), ETA: 06:27:06 (01:03:50 to 08::19:09:21)\n",
            "Steps: 49. Time per step: 4.1e-02s. Reward per step: -2.07.\n",
            "It has been 170 episode(s) since the model was last saved, with a score of -95 (Δ-7.01).\n",
            "\n",
            "11:35:53 \n",
            "Episode: **172**/4000, Score: -103 (Δ -1.8)\n",
            "Average score: -101.6 (Δ-0.04)\n",
            "Episode time: 1.0s, average: 6.0s (±191.44), ETA: 06:25:08 (01:03:49 to 08::18:02:14)\n",
            "Steps: 45. Time per step: 2.2e-02s. Reward per step: -2.29.\n",
            "It has been 171 episode(s) since the model was last saved, with a score of -95 (Δ-7.05).\n",
            "\n",
            "11:35:55 \n",
            "Episode: **173**/4000, Score: -102 (Δ  1.2)\n",
            "Average score: -101.6 (Δ 0.04)\n",
            "Episode time: 2.0s, average: 6.0s (±190.43), ETA: 06:23:32 (01:03:48 to 08::16:52:49)\n",
            "Steps: 47. Time per step: 4.3e-02s. Reward per step: -2.17.\n",
            "It has been 172 episode(s) since the model was last saved, with a score of -95 (Δ-7.01).\n",
            "\n",
            "11:35:57 \n",
            "Episode: **174**/4000, Score: -102 (Δ  0.3)\n",
            "Average score: -101.5 (Δ 0.07)\n",
            "Episode time: 2.0s, average: 6.0s (±189.43), ETA: 06:21:58 (01:03:47 to 08::15:44:08)\n",
            "Steps: 48. Time per step: 4.2e-02s. Reward per step: -2.12.\n",
            "It has been 173 episode(s) since the model was last saved, with a score of -95 (Δ-6.93).\n",
            "\n",
            "11:35:59 \n",
            "Episode: **175**/4000, Score: -101 (Δ  0.6)\n",
            "Average score: -101.4 (Δ 0.05)\n",
            "Episode time: 2.0s, average: 6.0s (±188.43), ETA: 06:20:25 (01:03:46 to 08::14:36:10)\n",
            "Steps: 53. Time per step: 3.8e-02s. Reward per step: -1.90.\n",
            "It has been 174 episode(s) since the model was last saved, with a score of -95 (Δ-6.88).\n",
            "\n",
            "Manually shutting down training.\n",
            "\n",
            "----------------- Training ended at 2021-04-01-11:36:00. Duration was 17.42 minutes.-----------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}