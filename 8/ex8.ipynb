{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import PIL\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, split into test and training data\n",
    "train_ds, test_ds = tfds.load('fashion_mnist', split= ['train', 'test'], data_dir=\"data\\\\\", as_supervised=False, shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length train:  60000\n",
      "length train:  10000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "0",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2222e64ee9c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m       \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0moverview_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-2222e64ee9c2>\u001b[0m in \u001b[0;36moverview_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"min \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def overview_data():\n",
    "  train_ds_numpy = tfds.as_numpy(train_ds)\n",
    "  test_ds_numpy = tfds.as_numpy(test_ds)\n",
    "\n",
    "  #Look at shapes images and labels of dataset\n",
    "  print(\"length train: \",len(train_ds))\n",
    "  print(\"length train: \",len(test_ds))\n",
    "\n",
    "  x = 0\n",
    "  for i in train_ds_numpy:\n",
    "    x += 1\n",
    "    # print(i)\n",
    "    print(\"shape \",format(i[0].shape))\n",
    "    print(\"max \",np.amax(i[0]))\n",
    "    print(\"min \",np.amin(i[0]))\n",
    "    print(format(i[1]))\n",
    "    if x == 3:\n",
    "      break\n",
    "\n",
    "  x = 0\n",
    "  for i in test_ds_numpy:\n",
    "    x += 1\n",
    "    print(\"shape \",format(i[0].shape))\n",
    "    print(\"max \",np.amax(i[0]))\n",
    "    print(\"min \",np.amin(i[0]))\n",
    "    print(format(i[1]))\n",
    "    if x == 3:\n",
    "      break\n",
    "\n",
    "# overview_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "0",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1fb125a338fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mtile_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabelling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-1fb125a338fc>\u001b[0m in \u001b[0;36mtile_plot\u001b[1;34m(train, imgs, greyscale, scale, labelling)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         label = {\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def tile_plot(train,imgs=55,greyscale=True,scale=False,labelling=True):\n",
    "    # This function plots the images in a tiled fashion, for better visualization.\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    columns = 4\n",
    "    rows = 10\n",
    "    fig, ax = plt.subplots(1,imgs)\n",
    "    for i,j in enumerate(tfds.as_numpy(train)):\n",
    "        img= j[0]\n",
    "        \n",
    "        label = {\n",
    "        0 : 'T-shirt/top',\n",
    "        1 : 'Trouser',\n",
    "        2 : 'Pullover',\n",
    "        3 : 'Dress',\n",
    "        4 : 'Coat',\n",
    "        5 : 'Sandal',\n",
    "        6 : 'Shirt',\n",
    "        7 : 'Sneaker',\n",
    "        8 : 'Bag',\n",
    "        9 : 'Ankle boot'\n",
    "        }\n",
    "        # if label is still just an integer\n",
    "        if isinstance(j[1], np.int64):\n",
    "            if labelling:\n",
    "                lbl = label[j[1]]\n",
    "        else:\n",
    "            indice = np.where(j[1] == 1)\n",
    "            indice = indice[0]\n",
    "            if labelling:\n",
    "                lbl = label[int(indice)]\n",
    "        subax = fig.add_subplot(rows+1,columns+1,i+1)   \n",
    "        subax.axis(\"off\")\n",
    "        if labelling:\n",
    "            subax.set_title(lbl)\n",
    "\n",
    "        if greyscale: #Dealing with grayscale images requires a different approach, since imshow doesn't handle them as adroitly as RGB\n",
    "            if scale: # This is to visualize our normalized images. Since this doesn't conform to the standard scale, we have to define our own\n",
    "                plt.imshow(np.squeeze(img),vmin=-2, vmax= 2.5,cmap=\"gray\")\n",
    "            else:\n",
    "                plt.imshow(tf.cast(np.squeeze(img),tf.uint8),cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(tf.cast(img,tf.uint8)) #cast to uint because otherwise there's an error from matplotlib\n",
    "        ax[i].axis(\"off\")\n",
    "        if i == (imgs-1): \n",
    "            # this is really awkward, but sadly prefetch_datasets are quite particular about indexing.\n",
    "            # converting them to numpy first might be smarter in the future\n",
    "            break\n",
    "    plt.subplots_adjust(top=3)\n",
    "    plt.show()\n",
    "\n",
    "# tile_plot(train_ds,labelling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build input pipeline\n",
    "train_ds = train_ds.map(lambda image: (tf.cast(image/255, tf.float32)))\n",
    "test_ds = test_ds.map(lambda image: (tf.cast(image/255, tf.float32)))\n",
    "\n",
    "prefetch_size = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# batching, shuffle und prefetching\n",
    "train_ds = train_ds.batch(128).shuffle(buffer_size=64).prefetch(prefetch_size)\n",
    "test_ds = test_ds.batch(128).shuffle(buffer_size=64).prefetch(prefetch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(total_epochs, train_loss, test_loss, accuracies, timing):\n",
    "    \"\"\"Helper function to plot the models performance inline during and after training\"\"\"\n",
    "    #clear_output(wait=True) # Clear the previous graph\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "    # Estimation for remaining time\n",
    "    epoch = len(train_loss) - 1\n",
    "    remaining_time = (timing[1] - timing[0]) * (total_epochs - epoch)\n",
    "    fig.suptitle(f\"Epoch {epoch} / {total_epochs} - Remaining Training Time: {time.strftime('%M:%S', time.gmtime(remaining_time))} min\", fontsize=16)\n",
    "\n",
    "    ax[0].plot(train_loss)\n",
    "    ax[0].plot(test_loss)\n",
    "    ax[0].legend([\"training\", \"test\"])\n",
    "    ax[0].set(xlabel=\"Training Steps\", ylabel=\"Loss\")\n",
    "    ax[1].plot(accuracies)\n",
    "    ax[1].set(xlabel=\"Training Steps\", ylabel=\"Accuracy\", title=f\"max accuracy: {max(accuracies)*100:.2f}%\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_layer(Layer):\n",
    "    \"\"\"This class allows us to easily define Convolutional layers\"\"\"\n",
    "    \n",
    "    def __init__(self,filters,strides=1,batch_norm=True,kernel_size=3):\n",
    "        super(CNN_layer, self).__init__()\n",
    "\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filters = filters,\n",
    "                                         kernel_size = 3,\n",
    "                                         strides = strides,\n",
    "                                         activation = None,\n",
    "                                         padding = 'same',\n",
    "                                         kernel_initializer = tf.keras.initializers.glorot_normal,\n",
    "                                         bias_initializer = 'zeros',\n",
    "                                         kernel_regularizer = None) # tf.keras.regularizers.L2(0.01)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()                                         \n",
    "        self.activ_1 = tf.keras.activations.relu\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,x,training):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_norm_1(x,training=training)\n",
    "        x = self.activ_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose_layer(Layer):\n",
    "    \"\"\"Transpose layers upscale featuremaps they are given.\"\"\"\n",
    "    \n",
    "    def __init__(self,filters,stride=2,batch_norm=True,kernel_size=3):\n",
    "        super(Transpose_layer, self).__init__()\n",
    "\n",
    "        self.conv_1 = tf.keras.layers.Conv2DTranspose(filters = filters,\n",
    "                                         kernel_size = kernel_size,\n",
    "                                         strides = stride,\n",
    "                                         activation = None,\n",
    "                                         padding = 'same',\n",
    "                                         kernel_initializer = tf.keras.initializers.glorot_normal,\n",
    "                                         bias_initializer = 'zeros',\n",
    "                                         kernel_regularizer = None)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()                                         \n",
    "        self.activ_1 = tf.keras.activations.relu\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,x,training):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_norm_1(x,training=training)\n",
    "        x = self.activ_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_layer(Layer):\n",
    "    \"\"\"Classic dense layer, incorporating batch norm\"\"\"\n",
    "    \n",
    "    def __init__(self,filters,batch_norm=True):\n",
    "        super(Dense_layer, self).__init__()\n",
    "\n",
    "        self.l1 = tf.keras.layers.Dense(units=filters, activation=None)\n",
    "        self.l2 = tf.keras.layers.BatchNormalization()                                         \n",
    "        self.l3 = tf.keras.activations.relu\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,x,training):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x,training=training)\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    \n",
    "    def __init__(self): #todo: consider upping filter sizes!\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden = [\n",
    "            CNN_layer(32,strides=2), # 14\n",
    "            CNN_layer(32,strides=2), # 7x7x8\n",
    "            tf.keras.layers.Flatten(),\n",
    "            Dense_layer(7*7*16),\n",
    "            \n",
    "        ]\n",
    "        self.conv1 = tf.keras.layers.Dense(units=20, activation=tf.keras.activations.relu) #1x1x10\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,x,training):\n",
    "        for i in self.hidden:\n",
    "            x = i(x,training=training)\n",
    "        #for i in x:\n",
    "        #        print(i.shape)\n",
    "        #        break\n",
    "        x = self.conv1(x)\n",
    "        #print(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden = [\n",
    "            Dense_layer(7*7*16),\n",
    "            tf.keras.layers.Reshape((7,7,16)),\n",
    "\n",
    "            Transpose_layer(32), # 4\n",
    "            Transpose_layer(32), # 8\n",
    "        ]\n",
    "\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filters = 1,\n",
    "                                         kernel_size = 3,\n",
    "                                         strides = 1,\n",
    "                                         activation = tf.keras.activations.sigmoid,\n",
    "                                         padding = 'same',\n",
    "                                         kernel_initializer = tf.keras.initializers.glorot_normal,\n",
    "                                         bias_initializer = 'zeros',\n",
    "                                         kernel_regularizer = None)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,x,training):\n",
    "        for i in self.hidden:\n",
    "            x = i(x,training=training)\n",
    "\n",
    "        #x = tf.image.resize(x, [28,28], method=tf.image.ResizeMethod.BILINEAR, preserve_aspect_ratio=False,\n",
    "        #                    antialias=False, name=None) # https://github.com/tensorflow/tensorflow/issues/6720\n",
    "\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        #for i in x:\n",
    "        #        print(i.shape)\n",
    "        #        break\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.h1 = Dense_layer(7*7*64), #784\n",
    "        self.h2 = Transpose_layer(32,kernel_size=4)\n",
    "        self.h3 = Transpose_layer(32,kernel_size=4)\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filters = 1,\n",
    "                                         kernel_size = 3,\n",
    "                                         strides = 1,\n",
    "                                         activation = tf.keras.activations.sigmoid,\n",
    "                                         padding = 'same',\n",
    "                                         kernel_initializer = tf.keras.initializers.glorot_normal,\n",
    "                                         bias_initializer = 'zeros',\n",
    "                                         kernel_regularizer = None)\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self,x,training):\n",
    "        x = self.h1(x,training=training)\n",
    "        x = tf.reshape(x, shape = [-1,7,7,64])\n",
    "        x = self.h2(x,training=training)\n",
    "        x = self.h3(x,training=training)\n",
    "        x = self.conv_1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.h1 = CNN_layer(32,strides=2), # 14\n",
    "        self.h2 = CNN_layer(32,strides=2), # 7x7x8\n",
    "        # self.h3 = tf.keras.layers.Flatten(),\n",
    "        self.h4 = Dense_layer(7*7*64),\n",
    "        self.output = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.sigmoid) #1x1x10\n",
    "\n",
    "\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self,x,training):\n",
    "        x = self.h1(x,training=training)\n",
    "        x = self.h2(x,training=training)\n",
    "        # x = self.h3(x,training=training)\n",
    "        x = self.h4(x,training=training)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientTape.gradient(target=only_respective_model_weights)\n",
    "\n",
    "def train_step(model, data, target, loss_function, optimizer):\n",
    "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    fake = generator(data, training = True)\n",
    "    # add label \n",
    "    prediction = model(fake, training = True)\n",
    "    loss = loss_function(target, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  \n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "  return loss \n",
    "\n",
    "\n",
    "def test_step(model, test_data, loss_function):\n",
    "  # test over complete test data\n",
    "\n",
    "  test_accuracy_aggregator = []\n",
    "  test_loss_aggregator = []\n",
    "\n",
    "  for (data, target) in test_data:\n",
    "    target = data\n",
    "    prediction = model(data, training = False)\n",
    "    \n",
    "    sample_test_loss = loss_function(target, prediction)\n",
    "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "\n",
    "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "  test_loss = np.mean(test_loss_aggregator)\n",
    "  test_accuracy = np.mean(test_accuracy_aggregator)\n",
    "\n",
    "  return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights if we wanna use old data.\n",
    "# model = Auto_encoder()\n",
    "# model.load_weights(\"weights/autoencoder_w\")\n",
    "# print(\"Using loaded weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space = np.random.normal(loc=0.0, scale=1.0, size = (100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "### Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = tf.optimizers.schedules.ExponentialDecay(0.001, 5000, 0.97, staircase=True) #polynomial?\n",
    "running_average_factor = 0.95\n",
    "\n",
    "# Initialize the model.\n",
    "model = Auto_encoder()\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "\n",
    "latent_space = [numpy.random.normal(loc=0.0, scale=1.0, size = (1,100))\n",
    "\n",
    "\n",
    "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
    "# loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Initialize the optimizer: Adam with default parameters. Check out 'tf.keras.optimizers'\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "#\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(f\"Training started at {current_time}.\")\n",
    "\n",
    "#testing once before we begin\n",
    "test_loss, test_accuracy = test_step(model, test_ds, loss_func)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "#check how model performs on train data once before we begin\n",
    "train_loss, _ = test_step(model, train_ds, loss_func)\n",
    "train_losses.append(train_loss)\n",
    "print(model.summary())\n",
    "\n",
    "# We train for num_epochs epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.perf_counter()\n",
    "    print('Epoch: __ ' + str(epoch))\n",
    "\n",
    "    #training (and checking in with training)\n",
    "    running_average = 0\n",
    "    for (data,target) in train_ds:\n",
    "        #train_loss = train_step(model, data, target, loss_func, optimizer)\n",
    "        train_loss = train_step(model, data, data, loss_func, optimizer)\n",
    "        running_average = running_average_factor * running_average  + (1 - running_average_factor) * train_loss\n",
    "    train_losses.append(running_average)\n",
    "\n",
    "    #testing\n",
    "    test_loss, test_accuracy = test_step(model, test_ds, loss_func)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    plot_performance(num_epochs, train_losses, test_losses, test_accuracies, (start, end))\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy, step=epoch)\n",
    "\n",
    "t2 = time.localtime()\n",
    "current_time2 = time.strftime(\"%H:%M:%S\", t)\n",
    "t_delta = time.mktime(t2)-time.mktime(t)\n",
    "print(f\"Training ended at {current_time}. Duration was {t_delta/60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# images = model(data,training=False).numpy()\n",
    "# tile_plot(images,labelling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # generate fake fashion from latent space\n",
    "    # discriminate between fake batch and real batch\n",
    "    # discriminator loss\n",
    "    # generator loss is how good they were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f34621b6821d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tensorboard'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tensorboard'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'--logdir logs/gradient_tape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Winpy\\python-3.7.2.amd64\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2325\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2326\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2327\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2328\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Winpy\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[1;34m(line)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Winpy\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\notebook.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(args_string)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mparsed_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mstart_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Winpy\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\manager.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(arguments, timeout)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[0mend_time_seconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_time_seconds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend_time_seconds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll_interval_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[0msubprocess_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(\"weights/autoencoder_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = model(data,training=False).numpy()\n",
    "\n",
    "x_test = list(map(lambda x: x[0], test_ds))\n",
    "x_test = tfds.as_numpy(test_ds)\n",
    "n = 15\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "  #display original\n",
    "  ax = plt.subplot(2, n, i + 1)\n",
    "  plt.imshow(np.squeeze(data[i]))\n",
    "  plt.title(\"original\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  # display reconstruction\n",
    "  ax = plt.subplot(2, n, i + 1 + n)\n",
    "  #ax = plt.subplot(2, n, i + 1)\n",
    "  # print(images[i].shape)\n",
    "  plt.imshow(np.squeeze(images[i]))\n",
    "  plt.title(\"construct\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}