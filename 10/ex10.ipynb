{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import PIL\n",
    "import tensorflow_probability as tfp\n",
    "import string\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml=YAML()\n",
    "yaml.default_flow_style=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset, split into test and training data\n",
    "train_ds = tfds.load('tiny_shakespeare', split= 'train', data_dir=\"data\\\\\",batch_size=-1)\n",
    "train_ds_numpy = tfds.as_numpy(train_ds)\n",
    "train_ds = train_ds_numpy[\"text\"][0].lower()\n",
    "train_ds = str(train_ds)\n",
    "train_ds = train_ds.replace(\"\\\\n\", \" \")\n",
    "\n",
    "#print(train_ds)\n",
    "train_ds = train_ds.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "train_ds = train_ds.split()\n",
    "#print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12073\n",
      "12073\n"
     ]
    }
   ],
   "source": [
    "def assign_id(data):\n",
    "    num = 1\n",
    "    global id_dict\n",
    "    id_dict = {}\n",
    "    global word_dict\n",
    "    word_dict = {}\n",
    "    for i in data:\n",
    "        if i not in word_dict.keys():\n",
    "            word_dict[i] = num\n",
    "            id_dict[f\"{num}\"] = i\n",
    "            num+=1\n",
    "    return\n",
    "\n",
    "assign_id(train_ds)\n",
    "print(len(word_dict))\n",
    "print(len(id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_id(word):\n",
    "    return word_dict[word]\n",
    "\n",
    "def from_id(id):\n",
    "    return id_dict[str(id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5436"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "to_id(\"castle\")\n",
    "#print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'me'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from_id(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_context(data,index,width):\n",
    "    if width%2 != 0:\n",
    "        print(\"Width muss gerade sein!\")\n",
    "        return\n",
    "    contexts = {tuple(data[index]) : [None]*(width+1)}\n",
    "    #print(\"context is \",contexts)\n",
    "    width=int((width/2)*-1)\n",
    "    for i in range(0,len(contexts[tuple(data[index])])):\n",
    "        #print(f\"width = {int(width)}\")\n",
    "        #print(data[index+width])\n",
    "        #if index+width >= len(contexts[tuple(data[index])]):\n",
    "            \n",
    "        contexts[tuple(data[index])][i] = data[index+width]\n",
    "        width+=1\n",
    "    \n",
    "    for i in range(0,len(contexts[tuple(data[index])])-1):\n",
    "        if contexts[tuple(data[index])][i] == tuple(data[index]):\n",
    "            #print(contexts[train_ds[index]])\n",
    "            contexts[tuple(data[index])].pop(i)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "def create_dataset(corpus,width=4):\n",
    "    for i in range(0,len(corpus)-width):\n",
    "        ctx = create_context(corpus,i,width)\n",
    "        #print(ctx.items())\n",
    "        data = []\n",
    "        for key in ctx.keys():\n",
    "            for i in ctx[key]:\n",
    "                data.append((key,i))\n",
    "            #print(data)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    return dataset\n",
    "\n",
    "def onehotcorpus():\n",
    "    onehot_corpus = [None]*len(word_dict)\n",
    "    for i in range(0,len(word_dict)):\n",
    "        vector = [0]*len(word_dict)\n",
    "        vector[i] = 1\n",
    "        onehot_corpus[i] = vector\n",
    "    return onehot_corpus\n",
    "\n",
    "oc = onehotcorpus()\n",
    "#print(oc)\n",
    "for i in oc:\n",
    "    #print(i)\n",
    "    break\n",
    "data = create_dataset(oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<TensorSliceDataset shapes: (2, 12073), types: tf.int32>\n",
      "tf.Tensor(\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], shape=(2, 12073), dtype=int32)\n",
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(12073,), dtype=int32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(Layer):\n",
    "  def __init__(self,):\n",
    "    # no biases!\n",
    "    super(LSTM, self).__init__()\n",
    "    self.hidden_layer_1 = tf.keras.layers.Dense(units=300,\n",
    "                                               activation=None\n",
    "                                               )\n",
    "    self.hidden_layer_2 = tf.keras.layers.Dense(units=12073,\n",
    "                                               activation=tf.keras.activations.sigmoid\n",
    "                                               )\n",
    "    self.add_weight\n",
    "\n",
    "\n",
    "  def build(self, input_shape)\n",
    "\n",
    "\n",
    "  def call(self,input):\n",
    "    tf.nn.embedding_lookup()\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build input pipeline\n",
    "prefetch_size = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# batching, shuffle und prefetching\n",
    "train_ds = data.batch(64).shuffle(buffer_size=64).prefetch(prefetch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(total_epochs, train_loss, test_loss, accuracies, timing):\n",
    "    \"\"\"Helper function to plot the models performance inline during and after training\"\"\"\n",
    "    #clear_output(wait=True) # Clear the previous graph\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "    # Estimation for remaining time\n",
    "    epoch = len(train_loss) - 1\n",
    "    remaining_time = (timing[1] - timing[0]) * (total_epochs - epoch)\n",
    "    fig.suptitle(f\"Epoch {epoch} / {total_epochs} - Remaining Training Time: {time.strftime('%M:%S', time.gmtime(remaining_time))} min\", fontsize=16)\n",
    "\n",
    "    ax[0].plot(train_loss)\n",
    "    ax[0].plot(test_loss)\n",
    "    ax[0].legend([\"training\", \"test\"])\n",
    "    ax[0].set(xlabel=\"Training Steps\", ylabel=\"Loss\")\n",
    "    ax[1].plot(accuracies)\n",
    "    ax[1].set(xlabel=\"Training Steps\", ylabel=\"Accuracy\", title=f\"max accuracy: {max(accuracies)*100:.2f}%\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, sample, context):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(data)\n",
    "        loss = loss_function(target, context)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "    return loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data, target, loss_function, optimizer):\n",
    "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    prediction = model(data, training = True)\n",
    "    loss = loss_function(target, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  \n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "  return loss \n",
    "\n",
    "\n",
    "def test_step(model, test_data, loss_function):\n",
    "  # test over complete test data\n",
    "\n",
    "  test_accuracy_aggregator = []\n",
    "  test_loss_aggregator = []\n",
    "\n",
    "  for (data, target) in test_data:\n",
    "    target = data\n",
    "    prediction = model(data, training = False)\n",
    "    \n",
    "    sample_test_loss = loss_function(target, prediction)\n",
    "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "\n",
    "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "  test_loss = np.mean(test_loss_aggregator)\n",
    "  test_accuracy = np.mean(test_accuracy_aggregator)\n",
    "\n",
    "  return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "### Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = tf.optimizers.schedules.ExponentialDecay(0.001, 5000, 0.97, staircase=True) #polynomial?\n",
    "running_average_factor = 0.95\n",
    "\n",
    "# Initialize the model.\n",
    "model = Auto_encoder()\n",
    "\n",
    "\n",
    "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
    "# loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_func = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Initialize the optimizer: Adam with default parameters. Check out 'tf.keras.optimizers'\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "#\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(f\"Training started at {current_time}.\")\n",
    "\n",
    "#testing once before we begin\n",
    "test_loss, test_accuracy = test_step(model, test_ds, loss_func)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "#check how model performs on train data once before we begin\n",
    "train_loss, _ = test_step(model, train_ds, loss_func)\n",
    "train_losses.append(train_loss)\n",
    "print(model.summary())\n",
    "\n",
    "# We train for num_epochs epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.perf_counter()\n",
    "    print('Epoch: __ ' + str(epoch))\n",
    "\n",
    "    #training (and checking in with training)\n",
    "    running_average = 0\n",
    "    for (data,target) in train_ds:\n",
    "        #train_loss = train_step(model, data, target, loss_func, optimizer)\n",
    "        train_loss = train_step(model, data, data, loss_func, optimizer)\n",
    "        running_average = running_average_factor * running_average  + (1 - running_average_factor) * train_loss\n",
    "    train_losses.append(running_average)\n",
    "\n",
    "    #testing\n",
    "    test_loss, test_accuracy = test_step(model, test_ds, loss_func)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    plot_performance(num_epochs, train_losses, test_losses, test_accuracies, (start, end))\n",
    "\n",
    "t2 = time.localtime()\n",
    "current_time2 = time.strftime(\"%H:%M:%S\", t)\n",
    "t_delta = time.mktime(t2)-time.mktime(t)\n",
    "print(f\"Training ended at {current_time}. Duration was {t_delta/60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# images = model(data,training=False).numpy()\n",
    "# tile_plot(images,labelling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}